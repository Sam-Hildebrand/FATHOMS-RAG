{
  "model": "gpt-oss:20b_Docling with easyOCR",
  "api_model": "gpt-oss:20b",
  "scores": {
    "Text-Only": 0.6559139784946236,
    "Tables": 0.23076923076923078,
    "Images": 0.07142857142857142,
    "Multimodal": 0.35333333333333333,
    "Cross-Document Multimodal": 0.55,
    "Text-Only Hallucination Rate": 0.25806451612903225,
    "Tables Hallucination Rate": 0.38461538461538464,
    "Images Hallucination Rate": 0.5,
    "Multimodal Hallucination Rate": 0.6,
    "Cross-Document Multimodal Hallucination Rate": 0.4
  },
  "overall_score": 0.3722890228051518,
  "total_hallucination_rate": 0.42853598014888333,
  "responses": [
    {
      "category": "Text-Only",
      "question": "What type of neural network does the HyperTransformer model generate? Do not use acronyms.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "The HyperTransformer generates a convolutional neural network.",
      "ground_truth": [
        [
          "convolution"
        ],
        [
          "cnn"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What modality other than text does Multimodal-CoT incorporate?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Multimodal\u2011CoT incorporates **vision (image) modality** in addition to text.",
      "ground_truth": [
        [
          "vision"
        ],
        [
          "image"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one question and answer benchmark that the Multimodal-CoT paper mentions in its abstract.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "ScienceQA.",
      "ground_truth": [
        [
          "scienceqa"
        ],
        [
          "a okvqa"
        ],
        [
          "aokvqa"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In the Multimodal-CoT paper, what are the two stages of the proposed framework? Answer as a comma separated list.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "rationale generation, answer inference",
      "ground_truth": [
        [
          "rationale generation",
          "answer inference"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which datasets are used to evaluate HyperTransformer? Answer as a comma separated list.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "miniImageNet, tieredImageNet, CIFAR\u2011FS, FC100",
      "ground_truth": [
        [
          "omniglot, miniimagenet, tieredimagenet"
        ],
        [
          "omniglot, mini imagenet, tiered imagenet"
        ],
        [
          "omniglot, mini image net, tiered image net"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What optimization-based algorithm mentioned in the HyperTransformer paper can fine-tune the embedding by performing additional SGD updates on all parameters of the model producing it?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "The paper refers to **MAML (Model\u2011Agnostic Meta\u2011Learning)** as the optimization\u2011based algorithm that fine\u2011tunes the embedding by performing additional SGD updates on all parameters of the model that produces it.",
      "ground_truth": [
        [
          "maml"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What function (and input to the function) is used to generate H_language in Multimodal_CoT? Omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**Answer**\n\nIn Multimodal\u2011CoT, the language representation \\(H_{\\text{language}}\\) is produced by feeding the **textual input** (the question, its context, and the answer options) into the model\u2019s **language\u2011encoder function**.  \nFormally:  \n\n\\[\nH_{\\text{language}} \\;=\\; f_{\\text{lang}}\\bigl(\\text{Q},\\,\\text{C},\\,\\text{M}\\bigr)\n\\]\n\nwhere \\(f_{\\text{lang}}\\) is the encoder (e.g., the T5 encoder) and the inputs are the concatenated tokens of the question (Q), context (C), and multiple\u2011choice options (M).",
      "ground_truth": [
        [
          "languageencoder",
          "x",
          "language"
        ],
        [
          "language encoder",
          "x",
          "language"
        ],
        [
          "language",
          "encoder",
          "x"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does Multimodal-CoT claim to be the main benefit of separating rationale generation from answer inference?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Multimodal\u2011CoT says that by **splitting the task into two stages\u2014first generating a multimodal\u2011aware rationale and then using that rationale to infer the final answer\u2014it lets the answer\u2011inference step benefit from a higher\u2011quality, multimodal\u2011grounded reasoning chain.** This separation is claimed to reduce hallucinated reasoning and accelerate convergence, giving the model a more reliable and efficient path to the correct answer.",
      "ground_truth": [
        [
          "hallucinat"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "NPHardEval4V says reasoning abilities increase as problem size increases. Answer \"true\" or \"false\"",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "There is a typo in the version of The Path To Autonomous Cyber Defense that you have. What is 'cyber' misspelled as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "",
      "ground_truth": [
        [
          "cuber"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the Oak Ridge Center for AI Security Research abbreviated as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The Oak Ridge Center for AI Security Research is abbreviated as **ORCA**.",
      "ground_truth": [
        [
          "caiser"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does DocLLM model that traditional LLMs don't?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM explicitly models the **spatial layout of a document**\u2014the positions, sizes, and relative arrangement of text tokens (i.e., the bounding\u2011box coordinates obtained from OCR).  \nTraditional LLMs treat input as a plain text sequence and do not incorporate any visual or layout information; DocLLM adds a separate \u201cspatial\u201d modality and uses a disentangled attention mechanism to capture the cross\u2011alignment between text semantics and the document\u2019s spatial structure.",
      "ground_truth": [
        [
          "spatial layouts",
          "text semantics"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which pre-training objective does DocLLM use instead of standard next-token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM replaces the usual left\u2011to\u2011right next\u2011token prediction with an **infilling pre\u2011training objective**.  \nIn this setup the model is trained to fill in masked text blocks (spans) by conditioning on both the preceding and succeeding tokens, rather than predicting the next token in a linear sequence.",
      "ground_truth": [
        [
          "text infilling"
        ],
        [
          "block infilling"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What type of problems does the NPHardEval4V benchmark aim to test in MLLMs?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The NPHardEval4V benchmark is designed to evaluate **algorithmic reasoning** in multimodal large language models by presenting them with a range of computational\u2011complexity problems. Specifically, it tests three classes of problems:\n\n1. **P (polynomial\u2011time) problems** \u2013 e.g., shortest\u2011path, edit\u2011distance, sorted\u2011array search.  \n2. **NP\u2011complete problems** \u2013 e.g., knapsack, traveling\u2011salesman decision, graph\u2011coloring decision.  \n3. **NP\u2011hard problems** \u2013 e.g., graph\u2011coloring optimization, traveling\u2011salesman optimization, meeting\u2011scheduling.\n\nThus, the benchmark focuses on reasoning tasks that span the P, NP\u2011complete, and NP\u2011hard complexity classes.",
      "ground_truth": [
        [
          "reasoning"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name the two NP computational complexity classes used in NPHardEval4V.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The two NP\u2011based complexity classes used in NPHardEval4V are **NP\u2011complete** and **NP\u2011hard**.",
      "ground_truth": [
        [
          "np hard",
          "np complete"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What are attackers called in the strategic game modeled in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "In the strategic game described in the paper, the attackers are referred to as the **\u201cred team.\u201d**",
      "ground_truth": [
        [
          "red team"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Who does the blue team represent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The blue team represents the defenders \u2013 the organization\u2019s security or SOC personnel who protect and defend the network.",
      "ground_truth": [
        [
          "defens"
        ],
        [
          "defender"
        ],
        [
          "defen"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does NPHardEval4V use in place of static benchmarks?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "NPHardEval4V replaces static benchmarks with a **dynamic benchmark that is updated regularly (monthly)**, so that the evaluation data changes over time and prevents models from over\u2011fitting to a fixed test set.",
      "ground_truth": [
        [
          "dynamic",
          "benchmark"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the attention mechanism in DocLLM that allows for selective focus called?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The selective\u2011focus attention mechanism used in DocLLM is called **disentangled spatial attention**.",
      "ground_truth": [
        [
          "disentangle",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-source LLM was adapted to build DocLLM-7B?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM\u20117B was built on top of the open\u2011source **Llama\u202f2\u202f\u2013\u202f7\u202fB** language model.",
      "ground_truth": [
        [
          "llama2",
          "7b"
        ],
        [
          "llama 2",
          "7 b"
        ],
        [
          "llama 2",
          "7b"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What medium does the ChartMimic benchmark evaluate a model's ability to turn to code?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "ChartMimic evaluates a model\u2019s ability to convert **visual charts (figures)** into executable code.",
      "ground_truth": [
        [
          "chart"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many total (figure, instruction, code) triplets were annotated for ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The benchmark contains **4,800** manually annotated (figure, instruction, code) triplets.",
      "ground_truth": [
        [
          "4,800"
        ],
        [
          "4800"
        ],
        [
          "four thousand, eight hundred"
        ],
        [
          "four thousand eight hundred"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What are the two tasks defined in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The ChartMimic benchmark defines **two tasks**:\n\n1. **Direct Mimic** \u2013 The model must generate code that reproduces the given chart exactly, assessing visual comprehension and reasoning.  \n2. **Customized Mimic** \u2013 The model must generate code for a new chart that incorporates data specified in the instruction while preserving the original chart\u2019s aesthetic and design, evaluating the integration of visual and textual information.",
      "ground_truth": [
        [
          "direct",
          "custom"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-weights model achieved the highest GPT-4o score on the Direct Mimic and Customized Mimic tasks?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The top\u2011performing open\u2011weight model on both the Direct Mimic and Customized Mimic tasks is **InternVL\u20112\u202fLlama\u20113\u202f\u2013\u202f76\u202fB**.",
      "ground_truth": [
        [
          "internvl2",
          "llama3",
          "76"
        ]
      ],
      "correctness_score": 0.3333333333333333,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Name one of the four low-level evaluation metrics used in ChartMimic.",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "**Exact Match** (the percentage of generated code that exactly matches the ground\u2011truth code).",
      "ground_truth": [
        [
          "text"
        ],
        [
          "layout"
        ],
        [
          "type"
        ],
        [
          "color"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What maximum accuracy did state-of-the-art models achieve on normal ENIGMAEVAL puzzles? Answer as a single value.",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "7.0%",
      "ground_truth": [
        [
          "7.0%"
        ],
        [
          "7%"
        ],
        [
          "7.0 %"
        ],
        [
          "7 %"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many puzzles are in the ENIGMAEVAL normal split?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "The ENIGMAEVAL normal split contains **949 puzzles**.",
      "ground_truth": [
        [
          "949"
        ],
        [
          "nine hundred forty nine"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does ENIGMAEVAL call a puzzle that combines answers or elements from multiple previous puzzles to reveal a final solution?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "ENIGMAEVAL refers to such a puzzle as a **meta\u2011puzzle**.",
      "ground_truth": [
        [
          "meta puzzle"
        ],
        [
          "metapuzzle"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What does PCFM stand for?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "PCFM stands for **Physics\u2011Constrained Flow Matching**.",
      "ground_truth": [
        [
          "physics",
          "constrained",
          "flow",
          "matching"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which methods propose gradient-based constraint enforcement during sampling, but often require backpropagation through expensive PDE operators?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "The paper points out that **DiffusionPDE** (reference\u202f[10]) and **D\u2011Flow** (reference\u202f[35]) are the two methods that use gradient\u2011based constraint enforcement during the sampling process.  Both of these approaches typically need to back\u2011propagate through the underlying PDE operators, which can be computationally expensive and may still fail to satisfy the constraints exactly.",
      "ground_truth": [
        [
          "diffusionpde, d flow"
        ],
        [
          "diffusion pde, d flow"
        ],
        [
          "diffusionpde, dflow"
        ],
        [
          "diffusion pde, dflow"
        ],
        [
          "diffusionpde, d-flow"
        ],
        [
          "diffusion pde, d-flow"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In ENIGMAEVAL, how do the authors justify excluding audio/video puzzles?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "The authors exclude puzzles that rely on audio, video, or other interactive media because **current frontier models do not have robust capabilities for processing those modalities**. In the \u201cPuzzle sourcing\u201d section they state:\n\n> \u201cAudio/Video and Interactive Elements. At the time of writing only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities, as well as those requiring interaction with a web application.\u201d\n\nThus, the justification is that the benchmark is meant to evaluate reasoning, not multimodal perception, and the limited ability of existing models to handle audio/video or interactive content would confound the assessment.",
      "ground_truth": [
        [
          "parsing limitations"
        ],
        [
          "scope"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "In Table 11 of the Multimodal-CoT Paper, what size of Multimodal-CoT was used?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Table\u202f11 reports the experiments that were run with the **770\u202fmillion\u2011parameter** Multimodal\u2011CoT model (i.e., a T5\u2011770M backbone).",
      "ground_truth": [
        [
          "738M"
        ],
        [
          "738"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What template does DocLLM use for CLS Internal Classification?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM uses the **`CLS: <text>`** template for its internal classification (CLS) task. In this format the prompt begins with the literal token `CLS:` followed by the document or text that needs to be classified.",
      "ground_truth": [
        [
          "{document} What type of document is this?"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In DocLLM, how many total documents were used in pre-training?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The excerpt provided does not contain a specific figure for the number of documents used during DocLLM\u2019s pre\u2011training phase. In other words, the document does not state how many total documents were used for pre\u2011training.",
      "ground_truth": [
        [
          "5,592,245"
        ],
        [
          "5592245"
        ],
        [
          "five million five hundred ninety two thousand two hundred forty five"
        ],
        [
          "five million, five hundred ninety two thousand, and forty five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, what is the Testmini test-set execution rate of InternVL2-26B on the Direct Mimic task?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The Testmini test\u2011set execution rate for **InternVL2\u201126B** on the Direct\u202fMimic task is **71\u202f%**.",
      "ground_truth": [
        [
          "69.3"
        ],
        [
          "sixty nine point three"
        ],
        [
          "sixty-nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggests 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot MiniImageNet with 8 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggest 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot OMNIGLOT with 64 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the PCFM paper, what is the MMSE/10^-2 for D-Flow?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "I don't know.",
      "ground_truth": [
        [
          "1.97"
        ],
        [
          "one point ninety seven"
        ],
        [
          "one point nine seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, which chart type has the largest average code token length?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The chart type with the largest average code token length in ChartMimic is **PIP** (with an average of 1,083.9 tokens).",
      "ground_truth": [
        [
          "plot in plot"
        ],
        [
          "pip"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "What accuracy did Multimodal-CoT get on the MMMU benchmark?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The provided excerpt does not contain any reported accuracy for the MMMU benchmark.",
      "ground_truth": [
        [
          "28.7"
        ],
        [
          "twenty eight point seven"
        ],
        [
          "twenty-eight point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the NPHardEval4V paper, what is LLaVa's instruction following performance?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "I\u2019m sorry, but the provided excerpt does not contain the specific instruction\u2011following score for LLaVa. The paper\u2019s tables (e.g., Table\u202f2) that list the individual model performances are not included in the text you supplied, so I can\u2019t locate the value from the given content.",
      "ground_truth": [
        [
          "0.75"
        ],
        [
          "zero point seven five"
        ],
        [
          "zero point seventy five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What was the highest TSP reasoning score across all complexity levels in NPHardEval4V? Answer as a single number only. Omit Reasoning Steps.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "0",
      "ground_truth": [
        [
          "0"
        ],
        [
          "zero"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "How many samples were used in testing while instruct-tuning for VQA in DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The provided excerpt of the paper does not contain any explicit mention of the number of test samples that were used during the instruction\u2011tuning phase for the Visual Question\u2011Answering (VQA) task in DocLLM.",
      "ground_truth": [
        [
          "24,347"
        ],
        [
          "24347"
        ],
        [
          "twenty four thousand three hundred forty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What feature shape is used for ViT in the Multimodal-CoT paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper uses a **768\u2011dimensional feature vector** for the Vision\u2011Transformer (ViT).",
      "ground_truth": [
        [
          "145",
          "1024"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what is the input to the f1 block?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the HyperTransformer architecture the **f1 block** is the first processing stage that receives the *support\u2011set representations*.  \nSpecifically, the input to the f1 block is the set of embeddings produced by the feature extractor for each support image (i.e., the support\u2011set embeddings). These embeddings are then forwarded to the Transformer\u2011based weight generator.",
      "ground_truth": [
        [
          "cnn layer 1"
        ],
        [
          "convolutional neural network layer 1"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what type of samples contain Class Embeddings?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the HyperTransformer paper, **the support (i.e., labeled) samples contain the class embeddings**. These labeled support examples are used to generate the task\u2011specific CNN weights, and the embeddings of their classes are the class embeddings that the Transformer operates on.",
      "ground_truth": [
        [
          "labeled samples"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper's Figure 1, what magnetic pole is shown on the far right?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "I don't know.",
      "ground_truth": [
        [
          "north"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In figures in the Multimodal-CoT paper, what step creates the Rationale? Please omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The Rationale is produced in the **first stage** of the two\u2011stage framework \u2013 the **Rationale Generation** step.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper, what is used to extract vision components?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "",
      "ground_truth": [
        [
          "vision transformer",
          "vit"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer model diagram, what do placeholder tokens represent?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the diagram of the HyperTransformer, the \u201cplaceholder\u201d tokens are simply markers that indicate where the actual support\u2011set and query\u2011set samples will be fed into the Transformer.  They are not learned parameters or data themselves; they serve as positional placeholders that separate the support examples from the query examples (and, in the semi\u2011supervised extension, from the unlabeled examples).",
      "ground_truth": [
        [
          "transformed tokens",
          "cnn weights"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In Figure 2 of the Multimodal-CoT paper, The answer stays the same when the vision component is introduced. Answer \"true\" or \"false\"",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "true",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what visibly distinguishes only generating logits from generating full CNN weights?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the HyperTransformer paper the two design choices are:\n\n| Choice | What the Transformer produces | What stays fixed | Typical effect |\n|--------|------------------------------|------------------|----------------|\n| **Logits\u2011only generation** | Weights for the final classification (logits) layer only | All earlier convolutional layers (the feature extractor / embedding) are shared and fixed across tasks | Works well for larger CNNs; the task\u2011specific signal is injected only at the very end, keeping the model small and fast. |\n| **Full\u2011CNN weight generation** | Weights for **every** layer of the CNN \u2013 the logits layer **and all intermediate feature\u2011extractor layers** | None \u2013 the whole network is task\u2011specific | Gives a noticeable accuracy boost for very small CNNs (where the fixed embedding is too weak). For larger models the benefit disappears once the network is already expressive enough. |\n\nThus, the visible distinction is that logits\u2011only generation keeps the backbone fixed and only modulates the final layer, whereas full\u2011CNN generation rewrites the entire network\u2019s parameters for each task. The former is simpler and sufficient for larger models; the latter is needed only when the target CNN is small enough that a task\u2011specific embedding is advantageous.",
      "ground_truth": [
        [
          "varied convolution patterns"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What are the 4 key elements of DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "**The four key elements of DocLLM are:**\n\n1. **Input representation** \u2013 Documents are fed to the model as a sequence of text tokens together with their bounding\u2011box coordinates (obtained from OCR).  \n2. **Disentangled spatial attention** \u2013 The transformer\u2019s self\u2011attention is extended to compute cross\u2011modal dependencies between the textual semantics and the spatial layout, treating the two modalities separately.  \n3. **Infilling pre\u2011training objective** \u2013 During pre\u2011training the model learns to mask and autoregressively predict whole text blocks (infilling), rather than just next\u2011token prediction, to handle irregular and heterogeneous layouts.  \n4. **Instruction\u2011tuning on a curated dataset** \u2013 The pre\u2011trained model is fine\u2011tuned on a newly collated instruction dataset that covers key visual\u2011document intelligence tasks (e.g., extraction, VQA, classification).",
      "ground_truth": [
        [
          "ocred document"
        ],
        [
          "llm extension"
        ],
        [
          "pre-training"
        ],
        [
          "instruction tuning"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What prompt template does DocLLM use for VQA extraction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM uses a simple, two\u2011line prompt for the Visual Question\u2011Answering (VQA) extraction task.  \nThe template is:\n\n```\nQuestion: {question}\nAnswer:\n```\n\nThe model receives the question prefixed with \u201cQuestion:\u201d and is expected to generate the answer immediately after the \u201cAnswer:\u201d line. This prompt is applied to every VQA instance in the instruction\u2011tuning dataset.",
      "ground_truth": [
        [
          "{document} {question}"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "Referencing the Knapsack Problem figure in NPHardEval4V, what is the id of the item with weight 8? Do not include step numbers in your reasoning.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The document excerpt provided does not contain the specific item identifiers or weights for the Knapsack Problem figure, so the id of the item with weight\u202f8 cannot be determined from the text above.",
      "ground_truth": [
        [
          "4"
        ],
        [
          "four"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What is the Knapsack Capacity given in the example in NPHardEval4V?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The provided excerpt does not contain the specific value of the knapsack capacity used in the example of the Knapsack Problem (KSP) in the NPHardEval4V benchmark.",
      "ground_truth": [
        [
          "40"
        ],
        [
          "forty"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "At approximately how many training steps does the Detector observation start to outperform the CybORG observation in the Path To Autonomous Cyber Defense paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "In the paper\u2019s comparison of the two observation spaces, the **Detector\u2011based observation begins to beat the original CybORG observation after roughly\u202f200\u202f000 training steps**.",
      "ground_truth": [
        [
          "40m"
        ],
        [
          "40 m"
        ],
        [
          "forty m"
        ],
        [
          "40 million"
        ],
        [
          "40,000,000"
        ],
        [
          "40000000"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What outputs to the RL Agent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "**Outputs (actions) that the RL agent can issue in the paper**\n\nThe paper describes the set of *actions* that an autonomous cyber\u2011defense agent is allowed to take in the training environments (e.g., CybORG).  These actions are the \u201coutputs\u201d of the RL policy and are grouped into the two main defensive functions that the agent can perform:\n\n| Function | Example actions the agent can output |\n|----------|--------------------------------------|\n| **Detection / reconnaissance** | \u2022 Deploy a honeypot host<br>\u2022 Deploy a honey\u2011network (decoy subnet)<br>\u2022 Trigger a network\u2011level scan or probe to gather host information |\n| **Response / remediation** | \u2022 Add or modify a firewall rule (e.g., block an IP or port)<br>\u2022 Restore a compromised host to its original, clean state (re\u2011image or revert to snapshot)<br>\u2022 Remove or quarantine a malicious process or user account |\n\nThese actions are the discrete outputs that the RL agent selects at each decision step.  The paper emphasizes that a realistic, small action space (e.g., focusing on a single life\u2011cycle stage such as \u201cdetect\u201d or \u201crespond\u201d) makes it easier to train, test, and deploy such agents.",
      "ground_truth": [
        [
          "observation converter"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What portion of Hallucinations go unresolved in the Multimodal-CoT Paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper does **not** give a numeric figure for the fraction of hallucinations that remain unresolved.  It only states that the Multimodal\u2011CoT framework \u201cmitigates hallucination\u201d (and improves convergence), but no explicit percentage or proportion of hallucinations that still persist is reported.",
      "ground_truth": [
        [
          "29.3"
        ],
        [
          "twenty nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "According to the GCP example figure in NPHardEval4V, what was the reasoning score that Gemini received for the first complexity class?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The document you provided does not contain the numerical value of Gemini\u2019s reasoning score for the NP\u2011Hard (first) complexity class. The only reference to Gemini\u2019s performance is in the description of Figure\u202f1, which mentions overall performance categories (recognition, instruction following, reasoning) across the three complexity classes, but it does not list the specific scores. Therefore, based on the content above, the reasoning score for Gemini on the first (NP\u2011Hard) class is not specified.",
      "ground_truth": [
        [
          "0.37"
        ],
        [
          "zero point three seven"
        ],
        [
          "zero point thirty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the HyperTransformer paper, performance increases as channel size increases.  Answer \"true\" or \"false\".",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Approximately how many questions have only 1 image in Raw PDF Format in EnigmaEval?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "Based on the statistics reported in the paper, only a minority of the 1,184 puzzles contain a single visual element in the original PDF.  Roughly **300\u202f\u2013\u202f350** puzzles (\u2248\u202f25\u201330\u202f% of the set) have exactly one image in the raw PDF format.",
      "ground_truth": [
        [
          "800"
        ],
        [
          "eight hundred"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Consulting the figures and text of the Multimodal-CoT paper, we would expect the number of hallucinations to increase after adding the visual modality Answer \"true\" or \"false\".",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "False.",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "The activation embeddings condition the generation of intermediate CNN layers in HyperTransformer. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "true",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What mechanism shown in the Multimodal-CoT paper ensures that generated rationales attend to the visual input?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper guarantees that the rationale generation step actually \u201csees\u201d the image by using a **two\u2011stage framework**.  \nIn the first stage the model is fed the multimodal input (the question text, any accompanying text, and the image features) and is asked to produce a rationale. Because the image features are part of the input to this stage, the generated rationale is forced to attend to the visual content.  \nThe second stage then takes that rationale (now enriched with visual information) and produces the final answer. Thus, the two\u2011stage design is the mechanism that ensures the rationales attend to the visual input.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What phenomenon depicted in the Multimodal-CoT paper and noted in the text explains the performance gap in Table 3?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The gap shown in Table\u202f3 is attributed to the **hallucination of intermediate reasoning chains**.  \nThe paper notes that small language models (\u2264\u202f1\u202fB parameters) often generate ill\u2011logical or fabricated rationales that mislead the subsequent answer\u2011inference step. This hallucination phenomenon explains why the one\u2011stage CoT setting underperforms compared with the two\u2011stage Multimodal\u2011CoT approach, producing the performance gap observed in Table\u202f3.",
      "ground_truth": [
        [
          "hallucinated rationale"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Which CNN model size, as shown in Figure 3 and noted in Section 5.4 of HyperTransformer, makes generating only the final logits layer perform on-par with generating all layers? Do not include step numbers in your reasoning. Only include the final answer.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "The larger CNN model \u2013 the 8\u2011layer, 128\u2011channel network (\u2248\u202f1.5\u202fM parameters) \u2013 is the size at which generating only the final logits layer yields performance on par with generating all layers.",
      "ground_truth": [
        [
          "8"
        ],
        [
          "eight"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What are the placeholders in Figure 2 of HyperTransformer replaced with?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In Figure\u202f2 the generic placeholders are simply standing in for the two data collections that define a few\u2011shot episode.  \nThe placeholder that represents the **support set** (the few labeled examples that describe the task) is replaced with the actual support set, and the placeholder that represents the **query set** (the unseen examples that the generated model must classify) is replaced with the actual query set.",
      "ground_truth": [
        [
          "weight"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In Multimodal-CoT, what correction rate with vision features does Figure 3 report for hallucinated rationales?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Figure\u202f3 reports that, when vision features are incorporated, the model can correct **about 70\u202f%** of the hallucinated rationales.",
      "ground_truth": [
        [
          "60.7"
        ],
        [
          "sixty point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What mechanism in Multimodal-CoT handles vision-absent inputs?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "In Multimodal\u2011CoT, when an input instance does not contain an image, the model simply inserts a special token **`<no_image>`** into the vision\u2011modality stream. This token tells the model that no visual information is available, allowing it to proceed with reasoning using only the textual components.",
      "ground_truth": [
        [
          "blank features"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What layout-aware mechanism in DocLLM, guides token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM uses an **infilling pre\u2011training objective** that is aware of the document layout.  \nDuring pre\u2011training, contiguous text blocks (identified by their bounding\u2011box positions) are randomly masked and replaced with special tokens (e.g., `[M]` for the masked span and `[S]` for its start). The model is then trained to autoregressively predict the tokens inside the masked span, conditioned on both the preceding and succeeding text. This layout\u2011aware infilling mechanism guides the token\u2011prediction process by explicitly leveraging the spatial arrangement of the text.",
      "ground_truth": [
        [
          "spatially constrained decoding"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 7 directly connected to vertex 1? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "no",
      "ground_truth": [
        [
          "no"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 2 directly connected to vertex 5? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "no",
      "ground_truth": [
        [
          "yes"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Which parameter adjustment, depicted in Autonomous Cyber Defense, changes agent performance?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The performance of the autonomous agent is affected by the **detection\u2011probability settings** for the red\u2011team actions. Adjusting how likely the blue agent is to observe (detect) each attacker action changes the agent\u2019s effectiveness, as shown in the study\u2019s Figure\u202f1.",
      "ground_truth": [
        [
          "detection probability"
        ],
        [
          "detection probabilities"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the DocLLM, which mechanism integrates spatial layout into the transformer block?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM incorporates spatial layout through a **disentangled spatial\u2011attention mechanism** that extends the transformer\u2019s self\u2011attention layer. This mechanism treats the bounding\u2011box coordinates of each text token as a separate modality and computes cross\u2011modal attention scores between the text and spatial representations, thereby embedding layout information directly into each transformer block.",
      "ground_truth": [
        [
          "disentangled",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What is the weight of the single path that is the answer to the SSP in Figure 3 of NPHardEval4V? Submit your answer as a single integer.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The document does not provide the weight of the single path for the SSP in Figure\u202f3.",
      "ground_truth": [
        [
          "5"
        ],
        [
          "five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to DocLLM's prompt templates and its training architecture, which task would be most sensitive to missing spatial layout?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "**Key\u2011Information Extraction (form\u2011understanding)**  \n\nDocLLM\u2019s instruction\u2011tuning prompts for KIE explicitly ask the model to \u201cextract the relevant fields from the document\u201d and the model is trained with a *disentangled spatial\u2011attention* mechanism that learns to align text tokens with their bounding\u2011box coordinates.  \nIf the spatial layout (bounding\u2011box information) is omitted, the model loses the only signal that tells it where a field starts, ends, or how it relates to other fields. Because KIE depends on these positional cues to map tokens to the correct semantic slots, it is the most vulnerable to missing spatial layout.  \n\nIn contrast, tasks such as natural\u2011language inference or pure document classification rely almost entirely on textual content, and visual\u2011question\u2011answering can sometimes be answered from text alone, so they are comparatively less sensitive to the absence of layout information.",
      "ground_truth": [
        [
          "kie"
        ],
        [
          "key information extraction"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Which model is the best at accurately recreating color in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The document does not provide any specific comparison or metric that indicates which model is best at accurately reproducing the color schemes of charts in ChartMimic. While GPT\u20114o is noted as the overall best performer on the benchmark, the text does not mention a dedicated color\u2011recreation evaluation or identify a model that outperforms others in that particular aspect.",
      "ground_truth": [
        [
          "gpt 4o"
        ],
        [
          "4o"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to ChartMimic, what makes an HR chart unique?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "According to ChartMimic, an **HR chart** is unique because it visualizes **hierarchical data using a radial layout**.  In other words, it combines a hierarchical structure with a circular (radial) representation, allowing complex relationships to be displayed in a compact, radial form.",
      "ground_truth": [
        [
          "layer"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What optimization method is applied during PCFM sampling as shown in Algorithm 1 and Figure 4?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "During the PCFM sampling procedure (see Algorithm\u202f1 and the illustration in Figure\u202f4) the intermediate flow states are projected onto the constraint manifold by solving the nonlinear constraint equations with a **Newton\u2011Raphson (Newton\u2019s method)** solver. This root\u2011finding routine is applied at each sampling step to enforce the hard constraints exactly while keeping the sampling trajectory aligned with the pretrained flow.",
      "ground_truth": [
        [
          "gauss newton projection",
          "newton gauss projection"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What are the 18 Regular chart categories in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "**18 Regular chart categories in ChartMimic**\n\n| # | Chart category |\n|---|----------------|\n| 1 | Bar |\n| 2 | Line |\n| 3 | Error\u2011Bar |\n| 4 | Heatmap |\n| 5 | Box |\n| 6 | Scatter |\n| 7 | Histogram |\n| 8 | Radar |\n| 9 | 3\u2011D |\n|10 | Pie |\n|11 | Error\u2011Point |\n|12 | Violin |\n|13 | Area |\n|14 | Contour |\n|15 | Density |\n|16 | HR (Heat\u2011map\u2011Radar?) \u2013 the \u201cHR\u201d type listed in the table |\n|17 | Multi\u2011Diff (the \u201cMuiltidiff\u201d type) |\n|18 | PIP (the \u201cPIP\u201d type) |\n\nThese 18 categories are the *regular* chart types that ChartMimic covers; the remaining four types (Graph, Quiver, Treemap, Combination) are treated as the benchmark\u2019s *advanced* chart categories.",
      "ground_truth": [
        [
          "bar",
          "errorbar",
          "line",
          "errorpoint",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "treemap",
          "heatmap",
          "contour"
        ],
        [
          "bar",
          "error bar",
          "line",
          "error point",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "tree map",
          "heat map",
          "contour"
        ]
      ],
      "correctness_score": 0.8333333333333334,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Does accuracy go up or down on hard ENIGMAEVAL puzzles, why?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "**Accuracy drops on the hard subset.**  \nIn the experiments reported in the paper, the best\u2011performing multimodal LLMs score about **7\u202f%** on the *normal* puzzles, but their accuracy falls to **0\u202f%** on the *hard* puzzles.  \n\nThe drop is attributed to the nature of the hard puzzles: they typically demand five or more non\u2011trivial, multi\u2011step deductions, often with only minimal verification and intermediate clues that are only thematically hinted. This requires a higher level of creative, lateral reasoning and knowledge synthesis than the normal puzzles, and current LLMs are not yet able to perform that depth of reasoning reliably.",
      "ground_truth": [
        [
          "down",
          "ocr"
        ],
        [
          "down",
          "parsing"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the ChartMimic paper, what is substituted in Figure 10 for CB_29?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The provided excerpt of the ChartMimic paper does not contain any mention of Figure\u202f10 or the placeholder \u201cCB_29,\u201d so the document does not specify what is substituted for CB_29.",
      "ground_truth": [
        [
          "dataset"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "From both papers (Multimodal-CoT and HyperTransformer), what do models experience as Transformer capacity continues to grow across model size and modality?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**What the two papers report about the effect of scaling the Transformer**\n\n| Paper | Observation about scaling the Transformer (size\u202f+\u202fmodality) | What the models \u201cexperience\u201d |\n|-------|-----------------------------------------------------------|------------------------------|\n| **HyperTransformer** | \u2022 Adding more Transformer capacity (more layers, larger hidden size) gives a *larger performance boost* for **small** generated CNNs.  <br>\u2022 For **larger** CNNs the extra capacity is largely unnecessary \u2013 the benefit plateaus and the method no longer outperforms other approaches.  <br>\u2022 When the Transformer is large enough it can even generate *all* CNN weights (not just the logits) and improve accuracy for very small models, but above a size threshold only generating the logits is sufficient. | Models see a *diminishing\u2011returns* pattern: the biggest gains come when the Transformer is large enough to encode the task but the target CNN is still small; once the target CNN is large, extra Transformer capacity yields little extra benefit. |\n| **Multimodal\u2011CoT** | \u2022 Scaling the Transformer (larger language\u2011model size) together with adding a **vision** modality improves the quality of the generated chain\u2011of\u2011thought.  <br>\u2022 Larger, multimodal Transformers produce **fewer hallucinations**, **better convergence** during fine\u2011tuning, and **higher overall accuracy** on multimodal reasoning benchmarks (ScienceQA, A\u2011OKVQA). | Models experience *improved reasoning fidelity* (less hallucination), *faster convergence*, and *higher accuracy* as the Transformer grows in size and incorporates multimodal inputs. |\n\n**Bottom line:**  \nAcross both works, increasing Transformer capacity helps the model learn more complex task\u2011specific behavior, but the magnitude of the benefit depends on the size of the target model (HyperTransformer) or on the inclusion of additional modalities (Multimodal\u2011CoT). The larger the Transformer, the better it can encode task nuances, reduce hallucinations, and converge more quickly, though for very large target CNNs the extra capacity may become redundant.",
      "ground_truth": [
        [
          "diminish",
          "return"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which embedding design, as illustrated in Figure 1 and described in the text of both HyperTransformer and Multimodal-CoT, drives improved few-shot or reasoning performance?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**Answer**\n\nThe common embedding design that is highlighted in Figure\u202f1 of both papers and is credited with the performance gains is a **shared, task\u2011agnostic feature embedding that is fed into a Transformer\u2011based module**.  \n\n* In **HyperTransformer** the support set is first mapped to a shared, task\u2011agnostic CNN embedding.  A Transformer then consumes this embedding and **generates the weights of a task\u2011specific CNN (often just the final classification layer)**.  This \u201cembedding\u2011plus\u2011Transformer\u201d design lets the model encode the structure of the task in the Transformer while keeping the inference CNN small, which yields the observed few\u2011shot improvements.\n\n* In **Multimodal\u2011CoT** the same idea is applied to multimodal inputs: the text and image are projected into a shared multimodal embedding, which is then processed by a Transformer\u2011based encoder.  The resulting embedding is used in a two\u2011stage pipeline (rationale generation \u2192 answer inference), and the richer multimodal embedding is what drives the improved reasoning accuracy.\n\nSo, the **shared, task\u2011agnostic embedding that is modulated by a Transformer** is the design that drives the improved few\u2011shot or reasoning performance in both works.",
      "ground_truth": [
        [
          "modal",
          "align",
          "token",
          "embed"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "How would inputs, like questions from the ScienceQA benchmark, be handled by the HyperTransformer architecture using its current encoding setup?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**Short answer**\n\nHyperTransformer, as it is described in the paper, is a *few\u2011shot image\u2011classification* system.  \nIts \u201cencoding setup\u201d consists of a conventional CNN backbone that turns each support image into a feature vector, and a Transformer that takes those vectors (and the class\u2011labels that accompany them) and produces the weights of a task\u2011specific CNN.  \nThe architecture does **not** contain a text encoder, so a ScienceQA question (which is a natural\u2011language prompt possibly accompanied by an image) would not be processed by the current HyperTransformer pipeline.  In practice the question would either be ignored or would have to be converted into an image (or fed through a separate text encoder) before it could be fed into the existing encoder\u2013Transformer stack.\n\n---\n\n### Why the question is not handled directly\n\n| Component | What it does | What it expects | How it relates to ScienceQA |\n|-----------|--------------|-----------------|-----------------------------|\n| **CNN feature extractor** | Maps an image to a dense embedding | Raw image pixels | Only the image part of a ScienceQA example can be fed here. The question text is not an image. |\n| **Transformer weight generator** | Takes the set of support embeddings (and their labels) and outputs a full set of CNN weights | A list of embeddings + class labels | It never sees the question text; it only sees the support set that defines the task. |\n| **No text encoder** | \u2013 | \u2013 | The question text is not encoded at all. |\n\nBecause the model is trained to generate a *classification* CNN from a few labeled examples, it has no mechanism to interpret a natural\u2011language question.  The only way a ScienceQA question could be used is if you first turned the question into a visual representation (e.g., a rendered image of the text) or if you added a separate text\u2011embedding module and fed that into the Transformer as part of the support set.  Neither of those steps is part of the current HyperTransformer design.\n\n---\n\n### What would happen if you tried to feed a ScienceQA question as\u2011is\n\n1. **Tokenization / embedding** \u2013 The question text would be tokenized by the standard text tokenizer (if you had one), but the current model has no embedding layer for tokens, so the tokens would be dropped or cause an error.\n2. **Image encoder** \u2013 If you simply passed the raw text string to the CNN backbone, the CNN would treat it as a 1\u2011channel image and produce a meaningless embedding.\n3. **Transformer** \u2013 The Transformer would receive a set of embeddings that do not correspond to any real visual support samples, so the generated CNN weights would be nonsensical for the ScienceQA task.\n\nIn short, **the current HyperTransformer cannot directly process ScienceQA questions**.  To use it on ScienceQA you would need to augment the architecture with a text\u2011encoding branch (or convert the question into an image) and then feed the resulting embeddings into the Transformer as part of the support set.",
      "ground_truth": [
        [
          "flatten"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which property of the self-attention mechanism, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and described in their Method sections, underpins both CNN weight generation and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The key property is the **permutation\u2011invariant (order\u2011agnostic) nature of self\u2011attention**.  \nBoth Figure\u202f2 of *HyperTransformer* and Figure\u202f4 of *Multimodal\u2011CoT* show a Transformer that treats its inputs as a set rather than a fixed\u2011sequence. In the Method sections each paper explains that this property lets the model:\n\n* **HyperTransformer** \u2013 generate CNN weights from a support set, regardless of the order in which the few\u2011shot examples are presented, and handle unbalanced or variable\u2011size sets.  \n* **Multimodal\u2011CoT** \u2013 fuse language and vision tokens and produce a rationale that depends only on the content of the multimodal input, not on the order in which the tokens appear.\n\nThus, the permutation\u2011invariant self\u2011attention mechanism underpins both the generation of task\u2011specific CNN weights and the generation of multimodal rationales.",
      "ground_truth": [
        [
          "invariant"
        ],
        [
          "invariance"
        ],
        [
          "permutation"
        ],
        [
          "permute"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What shared mechanism, as illustrated in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both Method sections, could unify weight and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Both papers use the **same Transformer\u2011based generative head** to turn a task description into a sequence of outputs.  \nIn HyperTransformer the Transformer (shown in Fig.\u202f2) takes the support set and produces a sequence of weight vectors that are then reshaped into a CNN.  \nIn Multimodal\u2011CoT (Fig.\u202f4) the same style of Transformer (a T5\u2011style encoder\u2011decoder) takes the multimodal prompt and generates a rationale (a text sequence) that is later fed to the answer head.  \n\nThus, the shared mechanism that could unify weight and rationale generation is a **Transformer decoder that, via self\u2011attention and learned embeddings, maps the task description (support samples or multimodal input) to a generated sequence\u2014either CNN weights or a reasoning chain\u2014in a single forward pass.**",
      "ground_truth": [
        [
          "cross"
        ],
        [
          "modal"
        ],
        [
          "self"
        ],
        [
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What failure in representation can occur in both Multimodal-CoT and HyperTransformer particularly in unsupervised learning?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "",
      "ground_truth": [
        [
          "drift"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which design feature, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both texts, supports permutation invariance for few-shot generalization?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The design feature is the **self\u2011attention (Transformer) module**.  \nBoth Figure\u202f2 of *HyperTransformer* and Figure\u202f4 of *Multimodal\u2011CoT* show a Transformer\u2011based architecture whose self\u2011attention layers are invariant to the ordering of the support samples, thereby providing permutation\u2011invariant few\u2011shot generalization.",
      "ground_truth": [
        [
          "self attention",
          "flat",
          "sequence"
        ],
        [
          "selfattention",
          "flat",
          "sequence"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What element from the Multimodal-CoT paper, used before Rational Generation in Figure 4, could be incorporated into the HyperTransformer architecture to allow it to classify datasets that contain both images and text?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "In Figure\u202f4 of the Multimodal\u2011CoT paper, the very first step before the **Rational Generation** stage is a **vision\u2011encoder (image\u2011feature extractor)** that turns each image into a dense embedding.  \nThat image embedding is then concatenated (or fused) with the text tokens and fed into the language\u2011model head that produces the rationale.  \n\nAdding a similar vision\u2011encoder (or a multimodal fusion layer that merges image embeddings with the text representation) into the HyperTransformer pipeline would give it the ability to process and classify datasets that contain both images and text.",
      "ground_truth": [
        [
          "vision",
          "text",
          "fusion"
        ],
        [
          "image",
          "language",
          "fusion"
        ],
        [
          "vision",
          "language",
          "fusion"
        ],
        [
          "image",
          "text",
          "fusion"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Figure 2 of DocLLM and Figure 3 of NPHardEval4V demonstrate a different type of reasoning as opposed to structural reasoning. What type of reasoning is demonstrated?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]",
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The figures illustrate **spatial (layout\u2011aware) reasoning** \u2013 i.e., reasoning that relies on the visual/positional layout of the content rather than on purely structural or textual relationships.",
      "ground_truth": [
        [
          "spatial"
        ],
        [
          "space"
        ],
        [
          "visual"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which common objective, as shown in Figure 2 of DocLLM and mentioned in NPHardEval4V, involves predicting missing content from context?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]",
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The common objective is **infilling** \u2013 an autoregressive \u201cfill\u2011in\u2011the\u2011middle\u201d or \u201cblank\u2011infilling\u201d task in which the model predicts missing text segments from the surrounding context.",
      "ground_truth": [
        [
          "context infilling"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    }
  ]
}