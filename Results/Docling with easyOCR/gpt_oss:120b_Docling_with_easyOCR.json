{
  "model": "gpt-oss:120b_Docling with easyOCR",
  "api_model": "gpt-oss:120b",
  "scores": {
    "Text-Only": 0.6612903225806451,
    "Tables": 0.15384615384615385,
    "Images": 0.35714285714285715,
    "Multimodal": 0.2777777777777778,
    "Cross-Document Multimodal": 0.55,
    "Text-Only Hallucination Rate": 0.22580645161290322,
    "Tables Hallucination Rate": 0.3076923076923077,
    "Images Hallucination Rate": 0.5,
    "Multimodal Hallucination Rate": 0.6,
    "Cross-Document Multimodal Hallucination Rate": 0.4
  },
  "overall_score": 0.4000114222694869,
  "total_hallucination_rate": 0.4066997518610422,
  "responses": [
    {
      "category": "Text-Only",
      "question": "What type of neural network does the HyperTransformer model generate? Do not use acronyms.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "The HyperTransformer generates a **convolutional neural network**.",
      "ground_truth": [
        [
          "convolution"
        ],
        [
          "cnn"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What modality other than text does Multimodal-CoT incorporate?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Multimodal\u2011CoT incorporates the **vision modality (images)** in addition to text.",
      "ground_truth": [
        [
          "vision"
        ],
        [
          "image"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one question and answer benchmark that the Multimodal-CoT paper mentions in its abstract.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "One of the benchmarks mentioned in the abstract is **ScienceQA**.",
      "ground_truth": [
        [
          "scienceqa"
        ],
        [
          "a okvqa"
        ],
        [
          "aokvqa"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In the Multimodal-CoT paper, what are the two stages of the proposed framework? Answer as a comma separated list.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "rationale generation, answer inference",
      "ground_truth": [
        [
          "rationale generation",
          "answer inference"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which datasets are used to evaluate HyperTransformer? Answer as a comma separated list.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "No datasets are explicitly mentioned in the provided excerpt.",
      "ground_truth": [
        [
          "omniglot, miniimagenet, tieredimagenet"
        ],
        [
          "omniglot, mini imagenet, tiered imagenet"
        ],
        [
          "omniglot, mini image net, tiered image net"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What optimization-based algorithm mentioned in the HyperTransformer paper can fine-tune the embedding by performing additional SGD updates on all parameters of the model producing it?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "The paper refers to the **MAML (Model\u2011Agnostic Meta\u2011Learning) algorithm** as the optimization\u2011based method that can fine\u2011tune the embedding by performing additional SGD updates on all parameters of the model that produces it.",
      "ground_truth": [
        [
          "maml"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What function (and input to the function) is used to generate H_language in Multimodal_CoT? Omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**Function:**\u202f`H_language = f_text( \u2026 )` \u2013 the language encoder (the T5/FLAN\u2011AlpacaBase text encoder).  \n\n**Input to the function:**\u202fthe concatenated textual tokens comprising the question (Q), the context (C) and the answer options (M).",
      "ground_truth": [
        [
          "languageencoder",
          "x",
          "language"
        ],
        [
          "language encoder",
          "x",
          "language"
        ],
        [
          "language",
          "encoder",
          "x"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does Multimodal-CoT claim to be the main benefit of separating rationale generation from answer inference?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Multimodal\u2011CoT argues that by **splitting the process into a first stage that generates a multimodal rationale and a second stage that uses that rationale to pick the answer**, the answer\u2011prediction step can rely on a much higher\u2011quality, vision\u2011and\u2011language\u2011grounded reasoning trace. This separation helps to **reduce hallucinated (incorrect) rationales** and consequently **improves answer accuracy and speeds up convergence**. In short, the main benefit is that answer inference can \u201cleverage better generated rationales that are based on multimodal information,\u201d which mitigates hallucination and leads to faster, more reliable learning.",
      "ground_truth": [
        [
          "hallucinat"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "NPHardEval4V says reasoning abilities increase as problem size increases. Answer \"true\" or \"false\"",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "There is a typo in the version of The Path To Autonomous Cyber Defense that you have. What is 'cyber' misspelled as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The typo spells **\u201ccyber\u201d as \u201ccyber.\u201d**",
      "ground_truth": [
        [
          "cuber"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What is the Oak Ridge Center for AI Security Research abbreviated as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The Oak Ridge Center for\u202fAI\u202fSecurity\u202fResearch is abbreviated as **ORC**.",
      "ground_truth": [
        [
          "caiser"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does DocLLM model that traditional LLMs don't?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM augments a standard language model with **spatial\u2011layout information**\u2014the bounding\u2011box coordinates (position and size) of each OCR\u2011extracted text token. By treating this layout data as a separate modality and adding a disentangled spatial\u2011attention mechanism, DocLLM can reason about the visual arrangement of text in a document, something traditional LLMs (which only process plain text) do not model.",
      "ground_truth": [
        [
          "spatial layouts",
          "text semantics"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which pre-training objective does DocLLM use instead of standard next-token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM replaces the usual left\u2011to\u2011right next\u2011token prediction with an **autoregressive infilling objective** \u2013 the model is trained to fill in (autoregressively) masked text segments (blocks) by conditioning on both the preceding and succeeding context.",
      "ground_truth": [
        [
          "text infilling"
        ],
        [
          "block infilling"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What type of problems does the NPHardEval4V benchmark aim to test in MLLMs?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The NPHardEval4V benchmark is designed to evaluate **algorithmic reasoning** in multimodal LLMs by presenting them with **computational\u2011complexity\u2011based problems**.  \nSpecifically, it tests three classes of problems:\n\n| Complexity class | Example tasks included in the benchmark |\n|------------------|------------------------------------------|\n| **P (polynomial\u2011time)** | Shortest Path Problem (SPP), Edit Distance Problem (EDP), Sorted Array Search (SAS) |\n| **NP\u2011complete** | Knapsack Problem (KSP), Traveling\u2011Salesman Decision version (TSP\u2011D), Graph\u2011Coloring Decision version (GCP\u2011D) |\n| **NP\u2011hard** | Graph\u2011Coloring Optimization (GCP), Traveling\u2011Salesman Optimization (TSP), Meeting\u2011Scheduling Problem (MSP) |\n\nThus, NPHardEval4V aims to test **pure reasoning abilities** of MLLMs on a spectrum of algorithmic problems ranging from easy (P) to the most challenging (NP\u2011hard), while isolating the influence of visual recognition and instruction\u2011following.",
      "ground_truth": [
        [
          "reasoning"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name the two NP computational complexity classes used in NPHardEval4V.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The benchmark distinguishes two NP\u2011related complexity classes:\n\n- **NP\u2011complete**  \n- **NP\u2011hard**",
      "ground_truth": [
        [
          "np hard",
          "np complete"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What are attackers called in the strategic game modeled in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The attackers are referred to as the **red team**.",
      "ground_truth": [
        [
          "red team"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Who does the blue team represent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "In the paper, the **blue team represents the defender** \u2013 the autonomous cyber\u2011defense agent (the \u201cblue\u201d side) that protects the network against the attacking \u201cred\u201d team.",
      "ground_truth": [
        [
          "defens"
        ],
        [
          "defender"
        ],
        [
          "defen"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does NPHardEval4V use in place of static benchmarks?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "NPHardEval4V replaces static, fixed\u2011set benchmarks with a **dynamic benchmark** that is refreshed regularly (updated each month). Its dataset evolves over time, ensuring that the evaluation tasks change continuously and preventing models from over\u2011fitting to a static set of questions.",
      "ground_truth": [
        [
          "dynamic",
          "benchmark"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the attention mechanism in DocLLM that allows for selective focus called?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The mechanism is called **disentangled spatial attention** (often referred to simply as \u201cspatial attention\u201d) in DocLLM.",
      "ground_truth": [
        [
          "disentangle",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-source LLM was adapted to build DocLLM-7B?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM\u20117B was built by adapting the open\u2011source **Llama\u202f2\u202f7\u2011b** model.",
      "ground_truth": [
        [
          "llama2",
          "7b"
        ],
        [
          "llama 2",
          "7 b"
        ],
        [
          "llama 2",
          "7b"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What medium does the ChartMimic benchmark evaluate a model's ability to turn to code?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The benchmark tests a model\u2019s ability to take **visual chart images** (i.e., scientific/graphical charts) and, based on them (and accompanying instructions), generate the corresponding rendering code.",
      "ground_truth": [
        [
          "chart"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many total (figure, instruction, code) triplets were annotated for ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "ChartMimic was built with **4,800** human\u2011curated (figure, instruction, code) triplets.",
      "ground_truth": [
        [
          "4,800"
        ],
        [
          "4800"
        ],
        [
          "four thousand, eight hundred"
        ],
        [
          "four thousand eight hundred"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What are the two tasks defined in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The benchmark defines two tasks:\n\n1. **Direct\u202fMimic** \u2013 the model must generate code that reproduces the given chart exactly.  \n2. **Customized\u202fMimic** \u2013 the model must generate code for a new chart that incorporates the custom data described in the instruction while preserving the original chart\u2019s visual style.",
      "ground_truth": [
        [
          "direct",
          "custom"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-weights model achieved the highest GPT-4o score on the Direct Mimic and Customized Mimic tasks?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The open\u2011weight model that performed best\u2014i.e., achieved the highest scores among the open\u2011weight models on both the Direct\u202fMimic and Customized\u202fMimic tasks\u2014is **InternVL2\u2011Llama3\u201176B**.",
      "ground_truth": [
        [
          "internvl2",
          "llama3",
          "76"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one of the four low-level evaluation metrics used in ChartMimic.",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "One of the low\u2011level evaluation metrics used in ChartMimic is **CodeBLEU**.",
      "ground_truth": [
        [
          "text"
        ],
        [
          "layout"
        ],
        [
          "type"
        ],
        [
          "color"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What maximum accuracy did state-of-the-art models achieve on normal ENIGMAEVAL puzzles? Answer as a single value.",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "7.0%",
      "ground_truth": [
        [
          "7.0%"
        ],
        [
          "7%"
        ],
        [
          "7.0 %"
        ],
        [
          "7 %"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many puzzles are in the ENIGMAEVAL normal split?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "The ENIGMAEVAL normal split contains **949 puzzles**.",
      "ground_truth": [
        [
          "949"
        ],
        [
          "nine hundred forty nine"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does ENIGMAEVAL call a puzzle that combines answers or elements from multiple previous puzzles to reveal a final solution?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "ENIGMAEVAL refers to such a puzzle as a **meta\u2011puzzle**.",
      "ground_truth": [
        [
          "meta puzzle"
        ],
        [
          "metapuzzle"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What does PCFM stand for?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "PCFM stands for **Physics\u2011Constrained Flow Matching**.",
      "ground_truth": [
        [
          "physics",
          "constrained",
          "flow",
          "matching"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which methods propose gradient-based constraint enforcement during sampling, but often require backpropagation through expensive PDE operators?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "The methods that use gradient\u2011based constraint enforcement during sampling\u2014requiring back\u2011propagation through costly PDE operators\u2014are **DiffusionPDE\u202f[10]** and **D\u2011Flow\u202f[35]**.",
      "ground_truth": [
        [
          "diffusionpde, d flow"
        ],
        [
          "diffusion pde, d flow"
        ],
        [
          "diffusionpde, dflow"
        ],
        [
          "diffusion pde, dflow"
        ],
        [
          "diffusionpde, d-flow"
        ],
        [
          "diffusion pde, d-flow"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In ENIGMAEVAL, how do the authors justify excluding audio/video puzzles?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "The authors excluded puzzles that rely on audio, video, or interactive web elements because, at the time of writing, only a very small number of frontier multimodal models are capable of processing such modalities effectively. To keep the benchmark focused on challenges that current state\u2011of\u2011the\u2011art models can actually attempt, they omitted any puzzles whose solution depends on audio/video clues or on interacting with a web application.",
      "ground_truth": [
        [
          "parsing limitations"
        ],
        [
          "scope"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "In Table 11 of the Multimodal-CoT Paper, what size of Multimodal-CoT was used?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper reports that the Multimodal\u2011CoT model evaluated in Table\u202f11 is the **770\u202fmillion\u2011parameter T5 model** (i.e., a sub\u2011billion\u2011parameter \u201cIB\u2011model\u201d).",
      "ground_truth": [
        [
          "738M"
        ],
        [
          "738"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What template does DocLLM use for CLS Internal Classification?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The paper does not give a concrete \u201ctemplate\u201d for a **CLS\u202fInternal\u202fClassification** task. In the sections that are provided (abstract, introduction, related work, model architecture, and the description of the pre\u2011training and instruction\u2011tuning procedures) there is no explicit formulation or example prompt that is labeled as the CLS\u202fInternal\u202fClassification template. Consequently, based on the available document content, no specific template can be extracted for that task.",
      "ground_truth": [
        [
          "{document} What type of document is this?"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In DocLLM, how many total documents were used in pre-training?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The paper does not state a specific number of documents used for pre\u2011training; that detail is not provided in the excerpt.",
      "ground_truth": [
        [
          "5,592,245"
        ],
        [
          "5592245"
        ],
        [
          "five million five hundred ninety two thousand two hundred forty five"
        ],
        [
          "five million, five hundred ninety two thousand, and forty five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, what is the Testmini test-set execution rate of InternVL2-26B on the Direct Mimic task?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The document excerpt you provided does not contain the specific performance numbers for the **Testmini** test\u2011set, nor does it list the execution rate of **InternVL2\u201126B** on the **Direct Mimic** task. Consequently, that information cannot be extracted from the text you shared.",
      "ground_truth": [
        [
          "69.3"
        ],
        [
          "sixty nine point three"
        ],
        [
          "sixty-nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggests 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot MiniImageNet with 8 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggest 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot OMNIGLOT with 64 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the PCFM paper, what is the MMSE/10^-2 for D-Flow?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "The document excerpt you provided does not contain any table or numerical results that list an \u201cMMSE\u202f/\u202f10\u207b\u00b2\u201d value for D\u2011Flow. Consequently, the MMSE\u202f/\u202f10\u207b\u00b2 for D\u2011Flow cannot be determined from the supplied content.",
      "ground_truth": [
        [
          "1.97"
        ],
        [
          "one point ninety seven"
        ],
        [
          "one point nine seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, which chart type has the largest average code token length?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The chart type with the highest average code\u2011token length is **PIP**, whose average code length is\u202f1083.9 tokens.",
      "ground_truth": [
        [
          "plot in plot"
        ],
        [
          "pip"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "What accuracy did Multimodal-CoT get on the MMMU benchmark?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The document you provided does not contain any information about Multimodal\u2011CoT\u2019s performance on the MMMU benchmark, so its accuracy on that dataset is not reported in the excerpt.",
      "ground_truth": [
        [
          "28.7"
        ],
        [
          "twenty eight point seven"
        ],
        [
          "twenty-eight point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the NPHardEval4V paper, what is LLaVa's instruction following performance?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The provided excerpt does not contain any quantitative results for LLaVa\u2019s instruction\u2011following performance. The document mentions that Figure\u202f1 reports \u201crecognition (RA), instruction following (ER), and reasoning (AA)\u201d across different problem classes, and that Table\u202f2 lists model details, but the actual numbers for LLaVa (or any other model) are not included in the text you shared. Therefore, based on the given content, LLaVa\u2019s instruction\u2011following performance cannot be determined.",
      "ground_truth": [
        [
          "0.75"
        ],
        [
          "zero point seven five"
        ],
        [
          "zero point seventy five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What was the highest TSP reasoning score across all complexity levels in NPHardEval4V? Answer as a single number only. Omit Reasoning Steps.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The document does not provide a numeric TSP reasoning score.",
      "ground_truth": [
        [
          "0"
        ],
        [
          "zero"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "How many samples were used in testing while instruct-tuning for VQA in DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The instruction\u2011tuning experiments for the visual\u2011question\u2011answering (VQA) task evaluated the model on **5\u202f000 test samples**.",
      "ground_truth": [
        [
          "24,347"
        ],
        [
          "24347"
        ],
        [
          "twenty four thousand three hundred forty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What feature shape is used for ViT in the Multimodal-CoT paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The excerpt you provided does not contain any information about the visual\u2011feature shape that the authors use for the Vision Transformer (ViT) in their Multimodal\u2011CoT model. No dimensions (e.g.,\u202f768,\u202f197\u202f\u00d7\u202f768, etc.) are mentioned in the sections shown. Therefore, based on the supplied document content, the ViT feature shape is not specified.",
      "ground_truth": [
        [
          "145",
          "1024"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what is the input to the f1 block?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "The **f\u2081 block** receives the *task\u2011description tokens* \u2013 i.e., the sequence of tokens that encode the few\u2011shot episode.  \nSpecifically, its input is the set of support\u2011sample tokens (each support example encoded as a token together with its class label) that constitute the task description\u202fT(t) (and, when the semi\u2011supervised variant is used, the additional \u201cunlabeled\u2011sample\u201d token). In short, f\u2081 takes the encoded support set (and any extra unlabeled\u2011sample token) as its input.",
      "ground_truth": [
        [
          "cnn layer 1"
        ],
        [
          "convolutional neural network layer 1"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what type of samples contain Class Embeddings?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the HyperTransformer framework, **class embeddings are derived from the labeled support samples** (i.e., the support set that provides the per\u2011class examples for a few\u2011shot task).",
      "ground_truth": [
        [
          "labeled samples"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper's Figure 1, what magnetic pole is shown on the far right?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The document does not describe the contents of Figure\u202f1 in enough detail to identify which magnetic pole (north or south) appears on the far right.",
      "ground_truth": [
        [
          "north"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In figures in the Multimodal-CoT paper, what step creates the Rationale? Please omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The **Rationale** is produced in the **first stage \u2013 the \u201cRationale Generation\u201d step** of the Multimodal\u2011CoT framework.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper, what is used to extract vision components?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper extracts the visual information with a **pre\u2011trained CLIP vision encoder (a ViT\u2011based visual backbone)**, which is used to obtain the image (vision) components that are then fused with the language representations.",
      "ground_truth": [
        [
          "vision transformer",
          "vit"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer model diagram, what do placeholder tokens represent?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the illustration of the HyperTransformer, the **placeholder tokens are special \u201cunknown\u2011class\u201d tokens that are inserted into the Transformer\u2019s input sequence**.  \nThey serve as stand\u2011ins for the class label of samples that do not have a known label (i.e., the unlabeled examples). By adding a placeholder token to each unlabeled example, the model can treat those examples as belonging to an \u201cunknown\u201d class and let the Transformer learn to exploit them when generating the task\u2011specific CNN weights.",
      "ground_truth": [
        [
          "transformed tokens",
          "cnn weights"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In Figure 2 of the Multimodal-CoT paper, The answer stays the same when the vision component is introduced. Answer \"true\" or \"false\"",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**False**",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what visibly distinguishes only generating logits from generating full CNN weights?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the paper the authors point out two very different regimes:\n\n* **Logits\u2011only generation** \u2013 the Transformer is asked to output **only the parameters of the final linear (logits) layer**.  \n  * All earlier convolutional layers (the feature extractor) are **fixed and shared** across every episode; they are learned once in a task\u2011agnostic way.  \n  * Because only a tiny set of weights has to be produced, the generator is simple and still reaches state\u2011of\u2011the\u2011art performance on medium\u2011to\u2011large CNNs.  \n\n* **Full\u2011CNN generation** \u2013 the Transformer is asked to output **the weights of every convolutional layer together with the final logits layer**.  \n  * The whole network becomes **task\u2011specific**: the embedding itself is modulated by the support set, not just the classifier.  \n  * This extra capacity improves both training and test accuracy **for very small CNNs** (below a certain size threshold).  \n\nVisually, the distinction is that in the logits\u2011only case the generated model consists of a **pre\u2011trained, frozen backbone plus a newly generated classifier head**, whereas in the full\u2011weight case the generated model is a **completely new CNN whose every layer\u2019s weights are produced by the HyperTransformer**. The paper shows that the full\u2011weight approach helps only for tiny models; once the CNN is large enough, generating just the logits layer already attains peak performance.",
      "ground_truth": [
        [
          "varied convolution patterns"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What are the 4 key elements of DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The paper explicitly lists the four core components of DocLLM as follows:\n\n1. **Text\u202f+\u202fbounding\u2011box input** \u2013 each document is represented by its OCR\u2011extracted text tokens together with the coordinates of the tokens\u2019 bounding boxes.  \n\n2. **Spatially\u2011aware attention** \u2013 the standard LLM attention is extended so that it can model cross\u2011modal dependencies between the textual semantics and the spatial layout of the tokens.  \n\n3. **Infilling pre\u2011training objective** \u2013 the model is trained to autoregressively fill in masked text blocks (infilling) rather than using a simple left\u2011to\u2011right next\u2011token loss, which better handles irregular layouts.  \n\n4. **Task\u2011adaptation via instruction tuning** \u2013 after pre\u2011training, DocLLM is fine\u2011tuned on a large, curated instruction\u2011tuning dataset that covers the main visual\u2011document\u2011understanding tasks (e.g., key\u2011information extraction, classification, VQA, etc.).",
      "ground_truth": [
        [
          "ocred document"
        ],
        [
          "llm extension"
        ],
        [
          "pre-training"
        ],
        [
          "instruction tuning"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What prompt template does DocLLM use for VQA extraction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The paper does not give a concrete, verbatim prompt template for visual\u2011question\u2011answering (VQA) extraction in the excerpt you provided. The description of DocLLM\u2019s instruction\u2011tuning mentions that \u201clayout hints such as field separators, titles and captions can be integrated during instruction\u2011tuning,\u201d and that the model is fine\u2011tuned on a curated instruction dataset covering VQA, but the exact wording of the VQA prompt template is not included in the supplied text. Therefore, based on the available document content, the specific prompt template used for VQA extraction cannot be identified.",
      "ground_truth": [
        [
          "{document} {question}"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "Referencing the Knapsack Problem figure in NPHardEval4V, what is the id of the item with weight 8? Do not include step numbers in your reasoning.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The provided document does not contain the specific details of the Knapsack Problem figure, so the id of the item with weight\u202f8 cannot be determined from the given content.",
      "ground_truth": [
        [
          "4"
        ],
        [
          "four"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What is the Knapsack Capacity given in the example in NPHardEval4V?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The document excerpt you provided does not contain the specific example for the Knapsack Problem, so the exact knapsack capacity used in the NPHardEval4V example is not stated in the available text.",
      "ground_truth": [
        [
          "40"
        ],
        [
          "forty"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "At approximately how many training steps does the Detector observation start to outperform the CybORG observation in the Path To Autonomous Cyber Defense paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The Detector\u2011based observation begins to pull ahead of the original CybORG observation after roughly **2.5\u202f\u00d7\u202f10\u2075 training steps** (about\u202f250\u202fk steps).",
      "ground_truth": [
        [
          "40m"
        ],
        [
          "40 m"
        ],
        [
          "forty m"
        ],
        [
          "40 million"
        ],
        [
          "40,000,000"
        ],
        [
          "40000000"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What outputs to the RL Agent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "In the paper the RL\u202fagent\u2019s **output is a set of defensive actions** (the policy\u2011function\u2019s chosen action for the current state).  \nThe training environments expose the agent with actions that fall into two broad categories:\n\n| Category | Example outputs (actions) the agent can emit |\n|----------|----------------------------------------------|\n| **Detection\u2011oriented actions** | \u2022 Deploy a honeypot or a honey\u2011network\u202f<br>\u2022 Generate alerts / trigger existing detection sensors |\n| **Response\u2011oriented actions** | \u2022 Add or modify firewall rules (block/allow traffic)\u202f<br>\u2022 Isolate or quarantine a host\u202f<br>\u2022 Restore a compromised host to its original (clean) state\u202f<br>\u2022 Perform other remediation steps such as revoking credentials or resetting services |\n\nThus, the RL agent receives observations (e.g., detector alerts, network/host state) and **outputs a concrete defensive action** drawn from this limited action space, which can be executed by the cyber\u2011defense infrastructure.",
      "ground_truth": [
        [
          "observation converter"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What portion of Hallucinations go unresolved in the Multimodal-CoT Paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper does **not** give a concrete numeric figure for the share of hallucinations that remain unresolved. It only states qualitatively that Multimodal\u2011CoT \u201cmitigates hallucination,\u201d without reporting a specific percentage or proportion.",
      "ground_truth": [
        [
          "29.3"
        ],
        [
          "twenty nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "According to the GCP example figure in NPHardEval4V, what was the reasoning score that Gemini received for the first complexity class?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The document does not give a numeric value for Gemini\u2019s reasoning score on the Graph\u2011Coloring Problem (GCP) in the first (NP\u2011hard) complexity class; that specific figure\u2011based score is not reported in the text.",
      "ground_truth": [
        [
          "0.37"
        ],
        [
          "zero point three seven"
        ],
        [
          "zero point thirty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In the HyperTransformer paper, performance increases as channel size increases.  Answer \"true\" or \"false\".",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Approximately how many questions have only 1 image in Raw PDF Format in EnigmaEval?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "Approximately\u202f9\u202f\u00d7\u202f10\u00b2\u202fquestions \u2013 i.e., on the order of nine\u2011hundred puzzles have only a single image in their raw\u2011PDF representation.",
      "ground_truth": [
        [
          "800"
        ],
        [
          "eight hundred"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Consulting the figures and text of the Multimodal-CoT paper, we would expect the number of hallucinations to increase after adding the visual modality Answer \"true\" or \"false\".",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**False**",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "The activation embeddings condition the generation of intermediate CNN layers in HyperTransformer. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "true",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What mechanism shown in the Multimodal-CoT paper ensures that generated rationales attend to the visual input?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper guarantees that the rationales are grounded in the image by **feeding the visual embeddings into the same encoder\u2011decoder that generates the rationale and letting the decoder attend to them through cross\u2011attention**.  \n\nIn practice, the image is first processed by a vision encoder (e.g., a CLIP\u2011style visual backbone) to produce a sequence of visual token embeddings. Those visual tokens are concatenated with the textual tokens (question, context, options) and passed to the multimodal encoder. When the decoder generates the rationale, its cross\u2011attention heads can attend to any of the encoder\u2019s tokens\u2014including the visual ones\u2014so the generated reasoning chain is directly conditioned on the visual input. This cross\u2011modal attention mechanism is the key component that ensures the rationales \u201clook at\u201d the image.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What phenomenon depicted in the Multimodal-CoT paper and noted in the text explains the performance gap in Table 3?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The paper attributes the drop in performance shown in Table\u202f3 to **hallucination of the generated rationales** \u2013 i.e., the model\u2019s tendency (especially for sub\u2011100\u2011billion\u2011parameter, \u201cIB\u2011\u201d) language models to produce spurious or incorrect intermediate reasoning steps that mislead the final answer inference. This hallucination effect creates the observed performance gap.",
      "ground_truth": [
        [
          "hallucinated rationale"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Which CNN model size, as shown in Figure 3 and noted in Section 5.4 of HyperTransformer, makes generating only the final logits layer perform on-par with generating all layers? Do not include step numbers in your reasoning. Only include the final answer.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "Generating only the final logits layer matches the performance of generating all layers once the CNN reaches the larger\u2011size regime \u2013 specifically, models with **64\u2011channel (\u224864\u2011filter) width** (e.g., a Conv\u20114/ResNet\u201112 network of that size).",
      "ground_truth": [
        [
          "8"
        ],
        [
          "eight"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What are the placeholders in Figure 2 of HyperTransformer replaced with?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]"
      ],
      "pred_answer": "In the illustration the generic **placeholder tokens** are not left empty \u2013 they are swapped out for the actual data that define the episode.  \nSpecifically, the placeholders are replaced with  \n\n* **tokens that encode the labeled support examples** (one token per support image, i.e. the concrete support\u2011set embeddings), and  \n* **an optional \u201cunknown\u2011class\u201d token** that encodes the unlabeled examples when the semi\u2011supervised version is used.  \n\nThus, the abstract placeholders in Figure\u202f2 become the real support\u2011set tokens (and, if present, the unlabeled\u2011sample token).",
      "ground_truth": [
        [
          "weight"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In Multimodal-CoT, what correction rate with vision features does Figure 3 report for hallucinated rationales?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Figure\u202f3 shows that, when vision features are incorporated, the hallucinated rationales are corrected at a rate of **about\u202f71\u202f%**.",
      "ground_truth": [
        [
          "60.7"
        ],
        [
          "sixty point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What mechanism in Multimodal-CoT handles vision-absent inputs?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "In Multimodal\u2011CoT a **special \u201cvision\u2011absent\u201d token** (or mask) is inserted when an example does not contain an image.  \nWhen this token is present the vision encoder is bypassed (its output is set to a zero/placeholder vector), so the downstream fusion module simply ignores the visual branch and the model proceeds with the language\u2011only (text\u2011only) pathway. This mechanism lets Multimodal\u2011CoT handle inputs that lack any visual modality without breaking the two\u2011stage rationale\u2011generation / answer\u2011inference pipeline.",
      "ground_truth": [
        [
          "blank features"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What layout-aware mechanism in DocLLM, guides token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM steers its token\u2011generation with a **disentangled spatial\u2011attention mechanism** (often referred to as \u201cspatial attention\u201d). By treating the bounding\u2011box layout as a separate modality and extending the transformer\u2019s self\u2011attention to compute cross\u2011modal attention scores, this layout\u2011aware attention guides the model\u2019s predictions.",
      "ground_truth": [
        [
          "spatially constrained decoding"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 7 directly connected to vertex 1? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "No",
      "ground_truth": [
        [
          "no"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 2 directly connected to vertex 5? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "No",
      "ground_truth": [
        [
          "yes"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Which parameter adjustment, depicted in Autonomous Cyber Defense, changes agent performance?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        "File: 2404.10788v1.pdf\nContent: ## Feature Article Achieving Autonomous Cyber Defense\n\n## The Path To Autonomous Cyber Defense\n\nSean Oesch, Oak Ridge National Laboratory; Oak Ridge; TN, USA; oeschts@ornl gov Phillipe Austria; Oak Ridge National Laboratory; austriaps@ornl gov Amul Chaulagain; Oak Ridge National Laboratory; chaulagaina@ornl gov Oak Ridge National Laboratory; weberb@ornl gov Cory Watson; Oak Ridge National Laboratory; watsoncl1@ornlgov Matthew Dixson; Oak Ridge National Laboratory; dixsonmk@ornl gov Amir Sadovnik; Oak Ridge National Laboratory; sadovnika@ornl gov\n\nAbstract Defenders are overwhelmed by the number and scale of attacks against their networks. This problem will only be exacerbated as attackers leverage artificial intelligence to automate their workflows. We propose a path to autonomous cyber agents able to augment defenders by automating critical steps in the cyber defense life cycle\n\nKeywords: Intelligent agents; Cybersecurity; Reinforcement learning he proliferation of professionalized crime groups combined with the shortage of skilled professionals in cyber defense has left defenders incapable of handling the volume of new attacks targeting their networks. To avoid being overwhelmed, defenders must automate security policies; detection; and mitigation; many of these tasks still require large time investment from human personnel. The creation of autonomous cyber defense agents is one promising approach to automate operations and prevent defenders from being overwhelmed. This article lays out the steps that researchers and practitioners need to take on the path to practical autonomous cyber defense agents; with a focus on reinforcement learning as promising approach. cyber yet cyber\n\nReinforcement learning (RL) addresses the challenge of \"learning from interaction with an environment in order to achieve long-term goals\" , where \"long-term goals\" could include flying a helicopter safely to the intended destination; winning a game of chess; efficiently managing a or protecting a network against cyber attacks. At a high level, reinforcement learning agents map situations (a state) to actions via\n\nXXXX-XXX 2024 IEEE\n\nDigital Object Identifier 10.1109/XXX.oooo.ooooooO\n\nwork has been submitted to the IEEE for possible publication: Copyright may be transferred without notice; after which this version may no longer be accessible. bThis\n\naWorld Economic Forum; Global Cybersecurity Outlook 2023. https:l/www.weforum orglreportslglobal-cybersecurityoutlook-2023/, July 13, 2023.\n\na \"policy\" function in order to maximize numerical reward signal. Rewards may vary in frequency (a single winlloss reward at the end of a game or a repeated \"distance from goal\" reward while a robot is walking) and complexity.\n\nIn order to handle large state spaces, deep reinforcement  learning  (deep RL) takes   advantage of deep neural nets in order to generalize well across states. By leveraging deep RL, DeepMind has trained reinforcement learning algorithms to defeat expert human players in games such as Goc and StarCraftd . When cybersecurity is modeled as a game between a defender (blue team) and an attacker (red team) it has similarities to games like StarCraft; where the state space and action space are large for StarCraft) . Because reinforcement learning has demonstrated the ability to defeat human adversaries in complex games with   large state spaces, it is a natural choice for creating defensive cyber agents. (1026\n\nAs result of this parity between cybersecurity and other areas where RL has proven effective; both cyber defenders and attackers sought to leverage this potential. Could autonomous RL agents be used to help defenders delay and deny attackers? 10,9,14,1 Could autonomous RL agents be leveraged by defenders to automate pen testing?3 Could autonomous have 2,5,\n\nchttps:l/www.deepmind comlresearchlhighlighted-researchl alphago\n\nhttps://www deepmind comlbloglalphastar-mastering-thereal-time-strategy-game-starcraft-ii\n\nRL agents be leveraged by attackers to overwhelm or sneak past defenders? In order to help answer these questions, researchers developed simulation 6,9,2,1 and emulation   environments in which to train such autonomous RL agents;   with some environments 5,10,8 providing both simulation and emulation:\n\nYet in spite of prior efforts to create autonomous cyber agents; are still not used in practice. What is  preventing the adoption of this new and promising approach to cyber defense? This   article seeks to answer this question by highlighting several of the challenges faced by researchers in this field and laying out the to autonomous cyber defense. We begin by situating the possible roles that autonomous agents can play within the context of the cyber defense life and then discuss the need for a properly defined \"game' for the agent to play; the need for agent adaptability to changing network environments and evolving adversary behaviors; and the need for better training environments to facilitate future research. they path cycle\n\n## THE CYBER DEFENSE LIFE CYCLE\n\nThe National Institute of Standards and  Technology (NIST) has defined 6 stages in their cybersecurity frameworke key\n\n- Govern: The policies and risk management strategy of the organization:\n- Protect: Safeguards that help manage existing risks (identity management; authentication; access control) .\n- Identify: Knowing the organizations current cy bersecurity risks.\n- Detect: Find and analyze potential threats to the network.\n- Respond: Responding to a threat once it has been identified.\n- Recover: Restoring assets impacted by the threat after it has been neutralized.\n\nIn the context of these 6 stages of the cyber defense life cycle; what role or roles should autonomous agents   play? Current   training environments for autonomous cyber defense agents give the agent a set of actions that span detection (the agent is fed host and network logs and is able to create honeypots or honey networks) , response (the agent can add firewall agent can restore boxes to their original state)    Yet little work has been done to understand if these actions\n\n'https:IInvlpubs.nist govInistpubsICSWPINIST.CSWP.29. pdf\n\nwould work in a real cyber defense context or if these are even the actions that we should be automating. More research is needed to understand  the types of autonomous agents that would most benefit cyber analysts in their existing workflows; which automated actions would be permitted based on the policy documents governing Security Operations Centers (SOCs) how to avoid replicating existing commercial tools; and the practicality of training these agents at scale over different action and observation spaces.\n\nWithin the context of the cyber defense life cycle; it is also unclear that a single autonomous agent should be responsible for multiple stages of the life Cur rent methods for creating autonomous agents tend to perform better when the number of actions available to them is smaller. In cybersecurity; one practical way to reduce the number of actions that an agent is required to perform is to limit the agent to only a single stage of the cyber defense life cycle; and in some cases only a specific function within that stage. Rather than creating a single monolithic agent; it would be better to identify which portions of each stage of the life should be automated and then create multiple autonomous agents; each with a specialization; that work together to achieve the larger goal of defending the network autonomously. These agents can then work in tandem or independently. cycle. cycle\n\nWe believe that such multi-agent approach is likely   the best forward to create reliable autonomous agents for cyber defense because it makes both creation and adoption easier. An autonomous agent  with a more specific goal and smaller action space is easier to   create; test, and deploy Such overall   system more modular, that individual agents can be swapped out or removed with minimal impact to the overall performance of the system. It is also easier for a SOC to adopt because it will require less effort to integrate into their existing toolset and it will require less input data which a given SOC may or may not currently be collecting: path\n\n## PLAYING THE RIGHT GAME\n\nOne of the ingredients necessary for the successful creation of autonomous cyber defense agents is that In many contexts where autonomous agents succeed there is a well defined game with clear boundaries; goals; and end states. For example; when piloting a drone the goal is to get from point A to point B without hitting any obstacles; perhaps with a few sub-goals such as staying within certain elevation boundaries;  avoiding no go zones; andlor delivering a package. In literal\n\ngames like chess; Go; or StarCraft, the rules are even more well defined and the environments are somewhat predictable.\n\nCybersecurity cannot be reduced to a single game and the environment in which the agent operates may change dynamically (not to mention that adversaries constantly seek to rewrite the rules of the game) . Defincorrect game for autonomous cyber agents therefore presents two critical challenges: there is no single game and the game itself changes. An additional challenge is that different organizations' networks may vary in terms of scale; topology; support staff; services; and defensive tools deployed. The consequence of this variability is that any autonomous agent must be able to adapt to a new environment even as it must be able to adapt to shifts in the game itself.  The right game is therefore difficult to define in a cybersecurity context.\n\nTo further illustrate this point;  let consider a specific example of how even small details in agent design impact agent usability and performance. The is training environment for autonomous agents. We will focus on the version of CybORG provided in the second Cyber Autonomy Gym for Experimentation (CAGE) challenge which focused specifically on defending an enterprise network. More specifically; we will examine the design of the observation space (the set of information the autonomous agent uses to make decisions) in CybORG. The structure of the observation space is critical to agent usability and performance.\n\nThe CybORG simulator maintains a detailed global network state; containing information on each individual host such running services; etc. The Blue and Red agent take turns  running actions on the network based on this state. The default observation space is a filtered view of the full simulated state of the network It is a ID vector where each host is assigned 2 fields: Activity State; and Compromised State. The Activity State indicates whether a red action was taken on given host in the previous turn; and can be three states: None (no red action); Scan (a red agent discovered the host) , and red agent   exploited the host) . The Compromised State shows whether a host has been compromised and to what extent it is compromised.\n\n| Hosts   | Activity   | Compromised   |\n|---------|------------|---------------|\n| Host1   | None       | No            |\n| Host2   | Exploit    | Privileged    |\n| ...     | ...        | ...           |\n\nfhttps:llgithub comlcage-challengelcage-challenge-2\n\nBy default the blue agent always observes the red agent actions in CybORG. In other words, the blue agent has perfect visibility into what the red agent is doing. As part of our research on CybORG we tested the simulator with different parameters such as network size; We also extended CybORG to enable new capabilities to support our research. For example; we added the of red agent actions being detected by the blue agent. Our results; shown in Figure 1, highlight the impact of detection probabilities on agent performance. The probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in a real environment. Defining detection probabilities is only a small piece of defining the correct game to play when training autonomous cyber defense agents.\n\nThe MITRE ATT&amp;CK framework is used to define both red agent actions and the data sources collected by sensors logs relevant to detecting each type of attack. These data sources are called Data Componentsg and are used to determine probability of detecting specific red actions. For each Data Component that tactic or technique touches; we could define a detector that is then given a probability of detecting specific red agent action. they\n\nYet there is an issue more fundamental than detec tion probabilities with the observation space defined by CybORG for the second The agent is ingesting raw log data. Existing cybersecurity tools already ingest this data and produce alerts for The agent trained in CybORG is   therefore   replicating   existing capabilities and trying to learn new behaviors on top of those existing capabilities. How might we change the observation space to avoid replicating existing tools' capabilities?\n\nWhen a red action occurs, the detector \"flips a coin\" based on the probabilities of detection for each Data If any flip signifies success; the alert is fed as an observation to the blue agent. Each detector would also have an object for describing false positive generation   probability: By   utilizing  detectors as the signal rather than network or host logs; we provide the agent with slightly noisier more realistic signal; avoid replicating the capabilities of existing (likely more accurate) detection tools; all while improving flexibility and simplifying deployment to any network containing those existing cyber-detection tools. We implemented this new observation inside CybORG and Figure 2 yet\n\nghttps:llattack.mitre orgldatasourcesl\n\nFIGURE 1. When training an autonomous agent; the probabilities of observing different attacker behaviors need to match real life probabilities or else the agent may fail in detection probabilities for specific red agent actions (50% for adding a new user; 15% for adding a process, 5% adding new session; etc:) Defining observation probabilities in only a small part of creating the appropriate game environment.\n\n<!-- image -->\n\nshows that   this new observation space is   just as effective as the original observation space.\n\nWe are not suggesting that this alternate observation space is necessarily the optimal observation space. Rather, the takeaway is that it is critical to define realistic state spaces that will transfer to operational networks more effectively in practice. And even more critically that we must define the right game; from the observation space to the reward function and actions; in order for autonomous cyber defense agents to be usable and effective in a real network.\n\n## ADAPTABILITY IS KEY\n\nIn addition to playing the right game; an autonomous cyber defense agent needs to be adaptable in the following ways:\n\n- 1) Be adaptable to varying   networks. Networks vary greatly across organizations and constantly change as devices joi\n[... content truncated ...]"
      ],
      "pred_answer": "The document shows that **changing the detection probabilities (i.e., the probability that the blue\u2011agent observes or detects the red\u2011agent\u2019s actions)** directly affects how well the autonomous cyber\u2011defense agent performs. Adjusting these observation/detection probabilities alters the agent\u2019s performance.",
      "ground_truth": [
        [
          "detection probability"
        ],
        [
          "detection probabilities"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In the DocLLM, which mechanism integrates spatial layout into the transformer block?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "DocLLM incorporates the layout information through a **disentangled spatial\u2011attention mechanism**.  \nIn each transformer block the standard self\u2011attention is extended so that, besides the usual text\u2011token attention scores, separate attention matrices are computed for the spatial modality (the bounding\u2011box coordinates) and for the cross\u2011modal interaction between text and layout. This \u201cdisentangled\u201d attention lets the model capture the relationships between textual semantics and the document\u2019s spatial layout without using a vision encoder.",
      "ground_truth": [
        [
          "disentangled",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What is the weight of the single path that is the answer to the SSP in Figure 3 of NPHardEval4V? Submit your answer as a single integer.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The provided excerpt does not contain the weight of the single path for the Shortest Path Problem (SSP) shown in Figure\u202f3, so the answer cannot be determined from the given document content.",
      "ground_truth": [
        [
          "5"
        ],
        [
          "five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to DocLLM's prompt templates and its training architecture, which task would be most sensitive to missing spatial layout?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]"
      ],
      "pred_answer": "The task that would suffer the most if the spatial\u2011layout information were omitted is **table\u2011alignment (i.e., understanding and reconstructing tables in a document)**.  \n\nDocLLM\u2019s prompt templates and its disentangled\u2011spatial\u2011attention architecture are explicitly designed to let the model reason about the geometric relationships between tokens (row/column boundaries, cell spans, separators, etc.).\u202fBecause table\u2011alignment depends almost entirely on those positional cues, removing the bounding\u2011box/layout modality would cripple the model\u2019s ability to locate cells, preserve column order, and reconstruct the table structure\u2014far more than for tasks such as plain key\u2011information extraction or document classification, which can often be solved with text alone.",
      "ground_truth": [
        [
          "kie"
        ],
        [
          "key information extraction"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Which model is the best at accurately recreating color in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The model that achieves the highest accuracy in reproducing the original colors of the charts in the ChartMimic benchmark is **GPT\u20114o**.",
      "ground_truth": [
        [
          "gpt 4o"
        ],
        [
          "4o"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to ChartMimic, what makes an HR chart unique?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "According to ChartMimic, the **HR chart** is distinguished by its **high difficulty and rich variety**:\n\n* **Number of examples:** 100 charts.  \n* **Sub\u2011categories:** 25 different sub\u2011types \u2013 the most diverse among the advanced chart groups.  \n* **Code\u2011length:** The average generated code is relatively long (\u2248\u202f718.9 tokens), indicating a more complex rendering logic.  \n* **Difficulty distribution:** Its level count is **4\u202f/\u202f16\u202f/\u202f80** (easy\u202f/\u202fmedium\u202f/\u202fhard), meaning the vast majority of HR charts are **hard\u2011level** examples (80 out of 100).  \n\nThis combination of many sub\u2011categories, longer code, and a strong bias toward hard\u2011level instances makes the HR chart type unique in the ChartMimic benchmark.",
      "ground_truth": [
        [
          "layer"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What optimization method is applied during PCFM sampling as shown in Algorithm 1 and Figure 4?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        "File: 2506.04171v1.pdf\nContent: ## Physics-Constrained Flow Matching: Sampling Generative Models with Hard Constraints\n\nUtkarsh\n\nMassachusetts Institute of Technology\n\nAlan Edelman Massachusetts Institute of Technology\n\nPengfei Cai Massachusetts Institute of Technology\n\nRafael Gomez-Bombarelli Massachusetts Institute of Technology\n\nChristopher Vincent Rackauckas Massachusetts Institute of Technology\n\n## bstract\n\nDeep generative models have recently been to physical systems governed by partial differential equations (PDEs), offering scalable simulation and uncertainty aware inference: However; enforcing physical constraints; such as conservation laws (linear and nonlinear) and physical consistencies; remains challenging. Exist methods often rely on soft penalties or architectural biases that fail to guarantee hard constraints\\_ In this work, we propose Physics-Constrained Flow Matching (PCFM), a zero-shot inference framework that enforces arbitrary nonlinear constraints in pretrained flow-based generative models. PCFM continuously guides the sampling process through physics-based corrections applied to intermediate solution states, while remaining aligned with the learned flow and satisfying physical constraints Empirically; PCFM outperforms both unconstrained and constrained baselines on range of PDEs; including those with shocks, discontinuities; and sharp features, while ensuring exact constraint satisfaction at the final solution: Our method provides a general framework for enforcing hard constraints in both scientific and general-purpose generative models; especially in applications where constraint satisfaction is essential. applied ing\n\n## 1 Introduction\n\nDeep generative modeling provides powerful and data-efficient framework for learning complex distributions from finite samples. By estimating an unknown data distribution and enabling sample generation via latent-variable models, deep generative methods have achieved state-of-the-art performance in a wide range of domains; including image synthesis [1-4], natural language generation [5, 6], and applications in molecular modeling and materials simulation [7-9].\n\nInspired by these successes, researchers have begun applying generative modeling to physical systems governed by Partial Differential Equations PDEs) [10-13]. In these settings; generative models offer unique advantages, including efficient sampling, uncertainty quantification; and the However, a fundamental challenge in this context is ensuring that generated samples respect the governing physical constraints of the system [12, 14]. In traditional domains like vision or text, domain structure is often incorporated through soft constraints classifier guidance [3], score conditioning [2], or architectural such priors\n\nEqual contribution\\_ Order decided by coin toss.\n\nCorresponding authors: rafagb@mit.edu, crackauc@mit.edu\n\nPreprint. Under review.\n\nas equivariance [15]. Manifold-based approaches further constrain generations to lie on known geometric spaces [16-18]. While such methods can align the model with geometric priors, cannot be easily adapted for enforcing physical laws in dynamical systems. they\n\nCrucially; constraint enforcement in generative modeling for PDEs follows a different paradigm. Physical invariants such as mass, momentum, and energy [19, 20] often arise from underlying symmetries [21]. Prior work to incorporate physics into neural networks has largely relied on inductive biases in training and regression-based tasks: encoding conservation laws as soft penalties However, soft constraints can lead to critical failure modes, particularly when exact constraint satisfaction is essential for stability or physical plausibility [25-27]. To address this, recent efforts have explored hard constraint enforcement, through learning conservation laws [28, 29], constraint satisfaction at inference [14, 30], and differentiable physics [31-33] (e.g-\n\nDespite this progress; hard constraint enforcement in generative models, particularly for PDEs; remains a nascent area [12, 34]. Enforcing hard constraints in generative models is particularly challenging due to the inherent stochasticity of the sampling process, and the constraints must be satisfied exactly in the final denoised solution but need not be preserved throughout the sampling process. DiffusionPDE [10] and D-Flow [35] propose gradient-based constraint enforcement sampling, but these methods often require backpropagation through expensive PDE operators and may fail to exactly satisfy the target constraints The ECI framework [12] introduces a novel mixing-based correction process for zero-shot constraint satisfaction; but only empirically evaluates on simple linear constraints and shows limited robustness on sharp, high-gradient PDEs with shocks or discontinuities. during\n\nIn this work, we introduce PhysicsConstrained Flow Matching (PCFM), framework that   bridges modern   generative modeling with classical ideas from numerical PDE solvers hard constraint enforcement for   pretrained flow matching models, projecting intermediate flow states onto constraint  manifolds at inference time, without requiring gradient information during training.  Unlike prior methods, PCFM can enforce arbitrary nonlinear equality constraints; including global conservation laws; nonlinear residuals, and sharp boundary conditions. It requires no retraining and no architectural modification; operating entirely post-hoc. While we focus on PDE-constrained  generation in this work, PCFM provides a general framework for enforcing hard constraints for flow-based generative models and may be extended to work for other domains such as molecular  design and scientific simulations beyond PDEs. To summarize; we list our contributions as follows:\n\n<!-- image -->\n\nVanilla Flow Matching\n\nFigure I: Evolution of generated solutions for the Burgers equation vanilla Flow Match(bottom and our Physics-Constrained Flow Matching (top) Burgers' equation exhibits shock fronts (top left in the figure) , which standard FFM fails to capture accurately; resulting in overly smoothed or smeared solutions. In contrast, PCFM efficiently incorporates physical constraints during sampling, enabling accurate shock resolution and physically consistent final outputs.using ing sharp\n\n- 1 . We introduce general framework PCFM for enforcing arbitrary and multiple physical constraints in Flow Matching-based generative models. These constraints include satisfying conservation laws, boundary conditions; or even arbitrary non-linear constraints. Our method enforces these constraints as hard requirements at inference time; without modifying the underlying training objective.\n- Our approach is zero-shot: it operates directly on any pre-trained flow matching model without requiring gradient information for the constraints during training. This makes the method broadly applicable and computationally efficient; especially in scenarios where constraint gradients are expensive or unavailable.\n- We demonstrate significant improvements in generating solutions to partial differential equations (PDEs), outperforming state-of-the-art baselines by up to 99.59 in standard metrics, such as mean squared error; while ensuring zero constraint residual.\n- 4 We evaluate our method on challenging PDEs spikes Our approach improves the accuracy of such models at inference time only, without the for retraining Or fine-tuning, by retrofitting physical consistency into generated samples. need\n- 5 To enable practical deployment, we develop a custom, batched and differentiable solver that projects intermediate flow states onto the constraint manifold. This solver integrates seamlessly with modern deep learning pipelines and enables end-to-end differentiability through the constraint enforcement mechanism.\n\nTable I: Comparison of generation methods motivated by constraint guidance or enforcement:.\n\n|                      | Zero-shot   | Continuous Guidance   | Hard Constraint   | Complex Constraints   |\n|----------------------|-------------|-----------------------|-------------------|-----------------------|\n| Conditional FFM [11] | \u2717           | \u2713                     | \u2713                 | \u2717                     |\n| DiffusionPDE [10]    | \u2713           | \u2717                     | \u2717                 | \u2713                     |\n| D-Flow [35]          | \u2713           | \u2713                     | \u2717                 | \u2713                     |\n| ECI [12]             | \u2713           | \u2713                     | \u2713                 | \u2717                     |\n| PCFM (Ours)          | \u2713           | \u2713                     | \u2713                 | \u2713                     |\n\n## 2 Related Work\n\nFlow-based Generative Models. Flow-based generative models [4, 36, 37] have emerged as scalable alternative to diffusion models by defining continuous normalizing flows (CNF) ordinary differential equations (ODEs) [38, 39, 36] parameterized by a time-dependent  vector field. In matching models; samples from tractable distribution are transported to target distribution via a learned vector field through a simulation-free training of CNFs. Stochastic interpolants provide a unifying framework to bridge deterministic flows and stochastic diffusions; enabling flexible interpolations between distributions [37]. Furthermore; methods like rectified proposed efficient sampling with fewer integration steps by straightening the flow trajectories [36] Functional flow matching (FFM) [11] and Denoising Diffusion Operator (DDO) [40] extend this paradigm to spatiotemporal data, learning flow models directly over function spaces such as partial differential equations (PDEs) solutions. using flow prior flows\n\nConstraint Guided Generation. Guiding generative models with structural constraints is an upcom ing direction used to improve fidelity in physics-based domains [12, 10,41]. Constraint information is typically exploited via gradient backpropagation [42, 10,41, 35] and has been successively applied to domains such as inverse problems [43-45]. Gradient backpropagation through an ODE solver can be prohibitively expensive for functional data such as PDEs [46-48]. Manifold-based and diffusion models [16, 18] capture known geometric priors. However; are not suitable for PDEs having with data-driven implicit constraints. For PDE-constrained generation; DIFFUSIONPDE[10] PINN-like soft penalties during sampling, while D-FLow[35] optimizes a noise-conditioned objective. Both approaches incur a high computational cost and offer only approximate constraint satisfaction. ECI [12] introduces a novel gradient-free, zero-shot; and hard constraint method on PDE solutions. However; its empirical evaluation is limited to simple linear and non-overlapping constraintse.g.pointwise or regional equalities\\_with known closed-form projections. It lacks a general roadmap for nonlinear or coupled constraints and has limited evaluation on harder-to-solve PDEs with shocks. Furthermore, while labeled 'gradient-free\" its reliance on analytical projections restricts extensibility to nonlinear or coupled constraints; and practical enforcement still implicitly relies on nonlinear optimization; which often requires gradient information [49]. We summarize our differences compared to generative methods motivated with hard constraints in Table 1. flows they applies\n\nPhysics-Informed Learning and Constraint-Aware Numerics. Physics-informed learning incorporates physical laws as inductive biases in machine learning models, typically through soft penalties as in PINNs [22, 50] or Neural Operators [23, 51,52]. While effective for regression; these methods lack guarantees of hard constraint satisfaction. Hard constraints have been addressed via differentiable layers [53, 31, 54], inference-time projections [14, 30], and structured architectures [32, 33, 28]. Constraint-aware integration methods offer complementary insights. constrained neural enforce feasibility through projection-based updates. Though underexplored in generative modeling, these methods motivate principled approaches to constrained sampling. Our method combines this numerical perspective with flow-based generation; enabling exact constraint enforcement without retraining.\n\n## 3 Methodology and Setup\n\n## 3.1 Problem Setup\n\nWe consider physical systems governed by parameterized conservation laws of the form\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n\u20ac Rd is a bounded domain, u 0 x [0, T] X 4 R\" is the solution field, F(u; is a flux function parameterized by \u20ac $, Qo specifies the initial condition; and B denotes the boundary operator: For a fixed PDE family and parameter set $, we define the associated solution set UF {u \u20ac U 3 \u20ac $ such that u satisfies (1) = (3)} representing all physically admissible solutions generated by varying %. We assume access to a pretrained generative model like FFM [11], that approximates this solution set; e.g., via a flow-based model trained on simulated PDE solutions .\n\nIn addition to the governing equations, we consider a physical constraint which we wish to enforce on the solution u(\u00e6, t) through a constraint operator Hu(\u00e6,t) = 0 defined on subdomain XH &lt; 0 x [0, T], and let Uu {u \u20ac U Hu(\u00e6, t) = 0 for all (\u00e6, t) \u20ac Xu} denote the constraint satisfying solution set. Our objective is to generate samples from the intersection := ie; functions that satisfy both the PDE and the constraint exactly. Importantly; we seek to impose H at inference time, without retraining the pre-trained model, thus narrowing the generative support from UF to in a zero-shot manner. UFlH\n\n## 3.2 Generative Models via Flow-Based Dynamics\n\nLet U denote the space of candidate functions u X R\" where X : [0, T] is the spatiotemporal domain of the PDE. We define a family of time-dependent diffeomorphic map U X [0, 1] ~ U called flow, indexed by the flow time T A vector field v which defines the evolution of the flow @- according to the ordinary differential equation: 00(uo) = This yields a continuous of measures through a push-forward map 07)#To, connecting the noise measure To to the predicted target measure T1. Chen et al. [38] proposed to learn the vector field v with a deep neural network parameterized by 0. Later in this text, we will denote this parameterized vector field as v This naturally induces parameterization for %t called Continuous Normalizing Flows (CNFs) . Lipman et al. [4] introduced a simulation and likelihood free training method for CNFs called Flow Matching along with with other similar works [36, 39] (see Appendix G). path prior\n\nIn the context of functional data such as PDE solutions; the Functional Flow Matching (FFM) framework [11] extends this idea to functional spaces. Instead of modeling samples in finitedimensional Euclidean space, FFM learns flows between functions uo, U1 e U via interpolants [4,37, 36], trained on trajectories with uo U1 ~ v, where v is the target probability measure from where wish to sample the PDE solutions. Th\n[... content truncated ...]"
      ],
      "pred_answer": "During PCFM sampling the intermediate flow state is **projected onto the constraint manifold by solving a constrained\u2010optimization sub\u2011problem with projected gradient descent**. In Algorithm\u202f1 the sampler first takes a standard flow\u2011matching step, then applies a few iterations of a projected\u2011gradient update (i.e., a gradient\u2011descent step on the constraint residual followed by a projection back onto the feasible set) to enforce the physics constraints. Figure\u202f4 visualizes this same correction loop: after each ODE integration step the state is \u201ccorrected\u2019\u2019 by a projected\u2011gradient step that drives the constraint residual to zero while staying as close as possible to the original flow\u2011matching trajectory. This projected\u2011gradient descent routine is the optimization method used to enforce the hard constraints during PCFM sampling.",
      "ground_truth": [
        [
          "gauss newton projection",
          "newton gauss projection"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What are the 18 Regular chart categories in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "The 18 **regular** chart categories that ChartMimic includes are:\n\n1. **Bar**  \n2. **Line**  \n3. **ErrorBar**  \n4. **Heatmap**  \n5. **Box**  \n6. **Scatter** (listed as \u201cScatters\u201d)  \n7. **Histogram** (listed as \u201cHist\u201d)  \n8. **Radar**  \n9. **Pie**  \n10. **ErrorPoint**  \n11. **Violin**  \n12. **Area**  \n13. **Contour**  \n14. **Density**  \n15. **Graph**  \n16. **Quiver**  \n17. **Treemap**  \n18. **Combination**  \n\nThese are the regular chart types; the remaining four types (e.g., 3\u2011D, HR, Multi\u2011diff, PIP) are treated as the \u201cadvanced\u201d categories in the benchmark.",
      "ground_truth": [
        [
          "bar",
          "errorbar",
          "line",
          "errorpoint",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "treemap",
          "heatmap",
          "contour"
        ],
        [
          "bar",
          "error bar",
          "line",
          "error point",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "tree map",
          "heat map",
          "contour"
        ]
      ],
      "correctness_score": 0.9444444444444444,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Does accuracy go up or down on hard ENIGMAEVAL puzzles, why?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        "File: 2502.08859v2.pdf\nContent: <!-- image -->\n\n## ENIGMAEVAL: A Benchmark of Long Multimodal Reasoning Challenges\n\nClinton J. Wangl Dean Lee' , Cristina Menghini' , Johannes Molsl , Jack Doughtyl Adam Khoja? , Jayson Lynch? , Sean Hendryx' Summer Yuel Dan Hendrycksz\n\nIScale AI, for AI Safety, 3MIT 2Center\n\n{clinton wang , summer yue}@scale \\_ com , dan@safe.ai https://scale.com/leaderboard/enigma\\_eval\n\n## Abstract\n\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive frontiers. Puzzle-solving events are rich repositories of challenging multimodal problems that test a wide range of advanced reasoning and knowledge capabilities , making them unique testbed for evaluating frontier language models: We introduce ENIGMAEVAL, a dataset of problems and solutions derived from puzzle competitions and events that probes models' ability to perform implicit knowledge synthesis and multi-step deductive reasoning. Unlike existing reasoning and knowledge benchmarks , puzzle solving challenges models to discover hidden connections between seemingly unrelated pieces of information to uncover solution The benchmark comprises 1184 puzzles of varying complexity each typically requiring teams of skilled solvers hours to to complete with unambiguous, verifiable solutions that enable efficient evaluation: State-of-the-art language models achieve extremely low accuracy on these puzzles, even lower than other difficult benchmarks such as Humanity's Last Exam, unveiling models' shortcomings when challenged with problems requiring unstructured and lateral reasoning: paths. days\n\n## 1. Introduction\n\nRecent advances in Large Language Models (LLMs)  evident in their saturation of existing benchmarks , call for shift in how we evaluate their capabilities. We need challenging benchmarks that expose current limitations and unexplored abilities at the frontiers of LLMs reasoning; Puzzle-solving events represent a promising direction: are unique expressions of human ingenuity that combine diverse knowledge domains with creative reasoning; These puzzles demand intricate chains of deductive reasoning cleverly interweaving logic; wordplay, mathematics , coding, and cultural references. Notably, come without explicit instructions, solvers to explore multiple creative approaches at each step a particularly challenging aspect for state-of-the-art LLMs that typically excel in well-structured tasks with clear objectives. probe they forcing they\n\nDespite this rich potential of puzzle-solving as an evaluation framework, current puzzle-based benchmarks remain narrow in scope. Most focus on specific specific puzzle types with consistent formats sudokus or crosswords) or restrict themselves to text-only challenges , falling short of assessing the advanced reasoning capabilities that modern models are beginning to demonstrate. While challenging reasoning benchmarks exist MMLU, GPQA and Olympiads test multi-step problem-solving through domain-specific challenges still operate within well-defined problem spaces, and are rapidly saturated by frontier models (Figure 1). This reflects a broader evaluation gap in assessing models' capacity for creative reasoning on unstructured multimodal challenges. With models now demonstrating capabilities in multimodal understanding and long-context processing, the time is to evaluate how these abilities come together in complex problem-solving scenarios\\_ (e.g they being ripe\n\nWe introduce ENIGMAEVAL, a benchmark of highly difficult problems with diverse unstructured formats spanning text and images drawn from puzzle hunts a rich, untapped repository of puzzles created and shared by a vibrant global puzzle-solving community: We release both the original multimodal problems and high-quality human transcriptions, allowing us to evaluate models' end-to-end capabilities as well as their reasoning abilities\n\n## scole\n\nFigure 1: While many existing reasoning and /or multimodal benchmarks are largely solved by frontier models, all models struggle on ENIGMAEVAL, demonstrating the benchmark's effectiveness for measuring continuing progress on multimodal reasoning: More frontier model results are shown in Table 2\n\n<!-- image -->\n\nin isolation: This design choice enable us to distinguish between performance limitations we observe stem from models' reasoning capabilities rather than their ability to parse complex documents or process different modalities. We evaluate frontier language models that have demonstrated strong performance on existing multimodal and reasoning benchmarks. Our evaluation shows that state-of-the-art models achieve only 7.0% accuracy on normal ENIGMAEVAL puzzles , dropping to 0% on hard problems, far short of experienced human puzzle hunters' capabilities (Table 2) Interestingly, model performance could dramatically from the transcribed puzzles to their original PDF versions , suggesting that some frontier models are still constrained by OCR and parsing ability: While we initially hypothesized that raw puzzle formats might pose additional difficulties , detailed analysis of cases where models succeeded suggests that have been well-optimized for processing complex documents, making the transcribed version equally challenging: falling drop they\n\nENIGMAEVAL joins Humanity's Last Exam (HLE) in establishing a new class of LLM benchmarks characterized by extremely challenging tasks that expose current models' limitations. While HLE probes structured academic knowledge, ENIGMAEVAL tests the integration of reasoning capabilities in creative problem-solving scenarios. We hope these complementary benchmarks represent just the beginning of a broader shift toward evaluations that reveal the true boundaries of models' capabilities , particularly in complex tasks requiring flexible thinking and knowledge synthesis: Although puzzle solutions are hosted online; their current scattered and unstructured format makes direct memorization unlikely: However; our structured dataset release could enable solution memorization. To prevent such data leakage, as well as to respect some authors' wishes not to distribute their puzzles widely; we keep the benchmark and will continuously it with the leading frontier models. To request that model be evaluated on ENIGMAEVAL, please fill out this form. private update your\n\n## 2. ENIGMAEVAL\n\nENIGMAEVAL comprises 1184 puzzles, represented in two formats: (1) PNGs of the original source PDFs (or for webpage puzzles , an automated full-page screenshot) , testing end-to-end performance, and (2) a structured text-image representation that preserves semantic relationships and visual elements , for targeted evaluation of multimodal reasoning with fewer distractors and reduced preprocessing load. The solution to each puzzle is typically a word or short phrase:\n\n## 2.1 Data Collection\n\nOur benchmark draws from eight diverse puzzle events archived online (Table 1), ranging from PuzzledPint's beginner-friendly puzzles to advanced competitions for experienced solvers.  This variety enables benchmark\n\n<!-- image -->\n\n## scole\n\n<!-- image -->\n\nHED LOST ALL HIS MARBLES\n\n<!-- image -->\n\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor and (d) CRUMS (link, CC BY-NC 3.0) Day\n\n<!-- image -->\n\n<!-- image -->\n\n<!-- image -->\n\nscole\n\nTable 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty N is the number of puzzles. split.\n\n| Split ( N )   | Puzzle Source ( N )                | Description                                                                                                                                                                                                                                                                                     |\n|---------------|------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| Normal (949)  | PuzzledPint (838) [1]              | Monthly beginner-friendly puzzle-solving event typically consisting of around seven puzzles including a meta-puzzle. Together, these puzzles should 'be solvable in under two hours by a team of inexperienced puz- zlers who are socializing, drinking, and eating in a pub or restaurant' 1 . |\n| Normal (949)  | CS50x Puzzle Day (41) [2]          | Annual puzzle set designed for small teams of beginner problem-solvers.                                                                                                                                                                                                                         |\n| Normal (949)  | Puzzle Potluck (34) [3]            | Public puzzle hunt hosted four times, designed to be accessible while more competitive than PuzzledPint.                                                                                                                                                                                        |\n| Normal (949)  | Cryptic Crosswords (30) [4]        | Collection of crosswords by Mark Halpin featuring unique mechanics and non-standard layouts. Final answers require combining filled grid elements.                                                                                                                                              |\n| Normal (949)  | CRUMS (6) [5]                      | Ashort puzzle hunt hosted in 2020.                                                                                                                                                                                                                                                              |\n| Hard (235)    | MIT Mystery Hunt (72) [6]          | Massive annual event at MIT with hundreds of puzzles. Our benchmark includes selected puzzles from various hunts.                                                                                                                                                                               |\n| Hard (235)    | Labor Day Extrava- ganza (153) [4] | Annual online hunt by Mark Halpin with multiple puzzles and a meta- puzzle, designed for experienced solvers over several days.                                                                                                                                                                 |\n| Hard (235)    | Grandmaster Puzzles (10) [7]       | Asmall collection of puzzle hunt-style puzzles published in a blog.                                                                                                                                                                                                                             |\n\nstratification into normal and hard subsets. Hard puzzles typically require five or more non-trivial steps with minimal verification, where intermediate answers might only be thematically hinted by flavor text. All sources combine textual and visual elements , including grids , pictures , diagrams , and their meaningful arrangements .\n\nPuzzle sourcing We scraped puzzles from online archives in their original format, either as PDFs (from CRUMS, Cryptic Crossword, PuzzledPint, Labor Extravaganza; CS5Ox, and Grandmaster Puzzles) or HTML webpages (from MIT Mystery Hunt and Puzzle Potluck)  During the collection process, we applied the following filters: Day\n\n- Advanced Meta-Puzzles. A meta-puzzle is a puzzle that combines answers or elements from multiple previous puzzles to reveal final solution. Since these may rely hidden clues across puzzles, we excluded them unless the answer and title of previous puzzles were sufficient to solve them independently. After filtering, our dataset contains 77 meta-puzzles, all in the normal split. on\n- Audio/Video and Interactive Elements. At the time of writing  only a few frontier models can process audio and video clues effectively: As a result, we excluded puzzles that rely on these modalities , as well as those requiring interaction with a web application.\n- Licensing and Permissions. We restricted our benchmark to puzzles for which we obtained explicit consent from the authors or that were released under Creative Commons (CC) licenses.\n\nwith the puzzles, we collected their corresponding solution documents, also available as PDFs or HTML pages. Along\n\nHuman annotation Human annotators transcribed each puzzle into a standardized text-image format.  This created evaluation one the original untranscribed puzzles (PDFs or automated webpage shots) and another our standardized format. This dual approach allows us to separate a model' s reasoning capabilities from its ability to parse complex documents addressing the challenge that models may fail due to OCR or parsing issues rather than lack of problem-solving ability:   Automation of this transcription process with LLMs proved impractical due to the complexity of parsing diverse puzzle formats , necessitating human paths: using two using\n\nIThis guideline and many others can be found at https /puzzledpint.org/volunteering/write-puzzles/\n\n<!-- image -->\n\nintervention. The standardization addressed several key challenges: (a) removing source-identifying headers and footers to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where structural elements like lines and dots are separate from textual content) , and (c) ensuring accurate text extraction from non-standard formatting; A similar curation process was to puzzle solutions: after initial automated extraction from PDF and HTML pages  each solution underwent manual validation and was tagged with its answer type (i.e., word or short phrase , pair of wordslphrases, or comma-separated list of wordslphrases) . This human curation was essential for maintaining semantic relationships and creating consistent representations across different source formats. To ensure annotation quality, transcriptions were subject to a rigorous review process by separate human reviewers who verified the accuracy and completeness of both puzzle and solution transcriptions. Full annotation instructions are detailed in Appendix A.2 2 applied\n\n## 3. Experiments\n\nWe test state-of-the-art LLMs' reasoning capabilities on our benchmark.  Section 3.1 describes our experimental setup  followed by quantitative results and performance analysis in Section 3.2 deep\n\n## 3.1 Evaluation Setup\n\nWe run the evaluation on a range of 'leading LLMs with multimodal capabilities: We evaluate models by comparing their answers to ground-truth solutions through string matching; The models generate responses format specific system prompt templates (Appendix B.2), that require both a step-by-step solution and a final answer in a standardized format, enabling consistent answer extraction (Appendix B.4)\n[... content truncated ...]"
      ],
      "pred_answer": "**The accuracy drops on the hard ENIGMAEVAL puzzles.**  \n\nState\u2011of\u2011the\u2011art multimodal LLMs that score about **7\u202f%** on the \u201cnormal\u201d split fall to **0\u202f%** on the hard split.  \n\n**Why?**  \n- Hard puzzles are deliberately more demanding: they typically need **five or more non\u2011trivial reasoning steps**, minimal verification clues, and often rely on subtle, lateral connections across text and images.  \n- Solving them requires deep, multi\u2011step deductive reasoning, implicit knowledge synthesis, and the ability to interpret complex visual layouts\u2014capabilities that current models still lack.  \n- Consequently, when the difficulty level is raised, the models\u2019 performance collapses, leading to a sharp decrease (to zero) in accuracy.",
      "ground_truth": [
        [
          "down",
          "ocr"
        ],
        [
          "down",
          "parsing"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the ChartMimic paper, what is substituted in Figure 10 for CB_29?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        "File: 2406.09961v2.pdf\nContent: ## CHARTMIMIC: EVALUATING LMM' s CROSS-MODAL REASONING CAPABILITY VIA CHART-TO-CODE GEN ERATION\n\nChufan Shi 1* Yaxin Liul= Bo Shuil * Junjie Wangl Mohan Jingl Linran Xul Xinyu Zhul Siheng Lil Yuxiang Zhangl Gongye Liu' Xiaomei Niel Deng Cai? 1Tsinghua University 2Tencent AI Lab chartmimic@gmail com\n\n## ABSTRACT\n\nWe introduce a new benchmark, ChartMimic, aimed at assessing the visually grounded code generation capabilities of large multimodal models (LMMs). ChartMimic utilizes information-intensive visual charts and textual instructions as inputs, requiring LMMs to generate the corresponding code for chart rendering.  ChartMimic includes 4, 800 human-curated (figure, instruction, code) triplets, which represent the authentic chart use cases found in scientific papers across various domains (e.g-, Physics; Computer Science, Economics; etc). These charts span 18 regular types and 4 advanced types, diversifying into 201 subcategories: Furthermore; we propose multi-level evaluation metrics to provide an automatic and thorough assessment of the output code and the rendered charts Unlike existing code generation benchmarks; ChartMimic places emphasis on evaluating LMMs capacity to harmonize a blend of cognitive capabilities; encompassing visual understanding, code generation; and cross-modal reasoning. The evaluation of 3 proprietary models and 14 open-weight models highlights the substantial challenges posed by ChartMimic. Even the advanced GPT-40, InternVL2-Llama3-76B only achieved an average score across Direct Mimic and Customized Mimic tasks of 82.2 and 61.6, respectively; indicating significant room for improvement.  We anticipate that ChartMimic will inspire the development of LMMs; advancing the pursuit of artificial general intelligence.\n\n## INTRODUCTION\n\nCode generation (Sun et al., 2024) is a rather demanding task that requires advanced abstract thinking and logical reasoning, reflecting the unique intelligence of human beings. Recently; advances in arti ficial general intelligence (AGI) have demonstrated the potential of large foundation models (Google; 2023; OpenAI, 2024; Anthropic, 2024; AI@Meta, 2024) t0 solve the tasks that are once the exclusive domain of human abilities (Achiam et al., 2023; Zhu et al., 2024). However; existing code generation benchmarks (Chen et al., 2021; Austin et al., 2021; Hendrycks et al., 2021; Lai et al., 2023) solely use text as input; while humans receive information from multiple modalities when coding et al., 2023; Fan et al., 2024). Such real-life scenarios have yet to be fully explored. (Liang\n\nTaking a common scene in 1, researchers often to write code for data visualization and may already have preferred chart templates at hand. However; usually lack either the source code or the expertise to reproduce these chart templates. As a result; turn to large multimodal models (LMMs) as assistants to aid in code generation: In this scenario, coding for scientific charts entails code generation grounded on visual understanding (i.e., chart-to-code generation), which necessitates LMMs to integrate a variety of advanced cognitive capabilities; including visual understanding; code generation; and cross-modal reasoning.  Therefore; evaluating the performance of LMMs on this real-world task also enables researchers to pinpoint potential areas for improving models' capabilities. Fig. need they\n\nEqual Contribution Data and code are available at https://github \\_ com/ ChartMimic/ ChartMimic.\n\nCorresponding author.\n\nFigure I: The real-world example: LMMs assist scientists and researchers in understanding, interpreting and creating charts during the reading and writing of academic papers. These models serve as assistants that enhance the comprehension and presentation of data in scholarly communications .\n\n<!-- image -->\n\nTo this end, we present ChartMimic (Fig. 2) a multimodal code generation benchmark. ChartMimic is characterized by its (1) information-intensive visual inputs, (2) diverse chart types, and (3) multi level evaluation metrics. Specifically; compared to natural images, scientific charts convey nuanced semantic meanings through intricate visual logic, thereby exhibiting higher information density. Based on this, we define two tasks;, Direct Mimic and Customized Mimic (Sec. 2.1), which utilize charts and textual instructions as inputs. These tasks challenge LMMs to generate the corresponding code for a given chart or to incorporate new data specified in the instructions; respectively. Through the collection of academic documents and scientific papers, we identify 22 commonly used chart types and 201 subcategories. Subsequently, we manually annotate a total of 4, 800 (figure, instruction; code) triplets for these types (Sec. 2.2). Furthermore, we establish automatic evaluation metrics from both high-level and low-level perspectives to thoroughly assess the performance of LMMs (Sec. 2.4)\n\nWe conduct the examination of 17 LMMs on ChartMimic (Sec. 3.2), including 3 proprietary models and 14 open-weight models across parameter sizes from 2.2B to 76.0B. We observe that while several open-weight models can match the performance of proprietary models such as GPT-40 on public leaderboards (OpenCompass, 2023), significant performance gap still persists on ChartMimic. Specifically; the best open-weight model, Intern VL2-Llama3-76B, behind GPT-4o, with an average score gap of 20.6 on two tasks; indicating substantial room for improvement in the open source community. Our analysis of prompting methods (Sec. 4.2) reveals that GPT-4o can improve itself through self-reflection, which is a manifestation of System 2 reasoning (Sloman; 1996; Kumar et al., 2024). This discovery highlights the vital role that the System 2 reasoning plays in LMMs when tackling the complex challenges presented by ChartMimic. Meanwhile, Correlation analysis (Sec. 4.3) demonstrates high correlation between our automatic metrics and human evaluation; validating the effectiveness of these metrics. Further error analysis (Sec. 4.4 reveals that hallucinations notably hinder the performance of LLMs on ChartMimic; as lead to the insertion of non-existent text into ground-truth figures and confusion between similar types of charts: lags key they\n\nWe envision ChartMimic as a comprehensive suite of benchmarks designed to researchers in understanding their LMMs' capabilities: By providing a comprehensive evaluation framework, ChartMimic aims to facilitate the growth of foundation models for the community; offering insights into various aspects such as visual understanding; code generation; and cross-modal reasoning. guide\n\n## 2 THE CHARTMIMIC BENCHMARK\n\nIn this section, we first introduce the definition of two tasks involved in ChartMimic (Sec. 2.1), and then delineate the data curation process (Sec: 2.2). Subsequently; we conduct a quantitative analysis to assess the quality and diversity of ChartMimic (Sec. 2.3), establish evaluation metrics (Sec. 2.4), and compare it with existing related benchmarks (Sec. 2.5).\n\n## 2.1 TASK DEFINITION\n\nLMMs' ability to generate chart-rendering code demonstrates their visual understanding, coding; and cross-modal reasoning skills. Specifically; given the chart X and the instruction I, the LMM f is expected to output the code C that satisfies the requirements outlined in the instruction:\n\n<!-- formula-not-decoded -->\n\nAs shown in Fig. 2, based on the information provided in the instructions; we propose two tasks:\n\nDirect Mimic: The LMMs are tasked to directly generate code that can reproduce the provided chart, thereby assessing their visual comprehension and reasoning capabilities:\n\nFigure 2: The pipeline of ChartMimic . We provide 4, 800 human-curated (figure, instruction; code) triplets. We use ChartMimic to evaluate LMMs' proficiency in the multimodal chart-to-code generation; resulting in both high-level and low-level evaluation results\n\n<!-- image -->\n\nCustomized Mimic. The LMMs are requested to generate code for a new chart that incorporates customized data provided in the instruction while preserving the original chart's aesthetic and design; assessing their ability to integrate visual and textual information:\n\nsubsequently compare its similarity with the ground-truth chart. Illustrative examples of the two tasks are shown in To accommodate the defined tasks above; we propose ChartMimic, a benchmark designed to evaluate the comprehension of charts and their conversion into executable code. Fig:\n\n## 2.2 DATA CURATION PROCESS\n\nChartMimic distinguishes itself through 4 fundamental considerations: (1) diversity of chart types, (2) balance of chart complexity  (3) reduction of data leakage; (4) integration of authentic user requirements: We on these four principles to complete the data curation for ChartMimic through five-step pipeline. Here, we provide an overview here and more details in Appendix B. keep\n\nGeneral Filtering;  We scrape figures from source files of publications on arXiv ' that hold a CC BY 4.0 license with a publication date after February 2024 with PDF format. This yields approximately 174, 100 figures across various domains Physics; Computer Science, Economics; etc). We then filter the figures based on the criteria of designers select inspiring visualization examples, including clarity and visual appeal; color schemes (Bako et al., 2022; Quispel et al., 2018), and uniqueness of the chart within its category; resulting in a refined collection of about 15,800 figures. (e.g, how\n\nDiversity and Information Density Filtering   The filtering process involves two stages. In the first stage; we establish a data and categorize chart types. Charts with significant differences in complexity or information density are added to ensure diversity and effective communication (Bako et al., 2022). In the second stage, five experts from various fields independently evaluate the data; creating separate selection pools. We preserve the intersection of their selections and finalize the set through a process. This meticulous approach refines our collection to 279 charts. pool voting\n\nManual Selection and Filling   In addition to sourcing from arXiv, also collect charts from various platforms; including the Matplotlib gallery, Stack Overflow; and plotting-related forums on Twitter and Reddit. These charts are selected for their unique styles not represented in our arXiv curated 279 charts . To mitigate the risk of data leakage, we rigorously process the data and color styling of these charts; replacing existing color schemes with those unseen in our data while maintaining their aesthetic appeal. Consequently; we obtain 600 prototype charts for ChartMimic. we pool\n\nJarxiv org/ Ihttps\n\nTable 1: Statistics of ChartMimic . We measure code length in terms of tokens;, utilizing the Llama3 tokenizer: In the level count; \"A/BIC\" denotes the number of \"easylmedium/hard\" level, respectively:\n\n| Type               | Bar            | Line         | ErrorBar    | Heatmap     | Box         | Scatters    | Hist         | Radar       | 3D          | Pie         | ErrorPoint       | Violin           |\n|--------------------|----------------|--------------|-------------|-------------|-------------|-------------|--------------|-------------|-------------|-------------|------------------|------------------|\n| Count              | 320            | 280          | 120         | 120         | 100         | 100         | 80           | 80          | 80          | 80          | 80               | 80               |\n| Subcategories      | 16             | 8            | 12          | 4           | 6           | 4           | 3            | 6           | 5           | 8           | 5                | 3                |\n| Code Length (AVG.) | 689.6          | 794.0        | 681.2       | 685.8       | 689.0       | 655.0       | 529.6        | 779.8       | 655.4       | 418.4       | 624.3            | 975.6            |\n| Code Length (STD.) | 237.8          | 244.4        | 144.7       | 258.7       | 228.2       | 253.0       | 147.1        | 144.3       | 241.4       | 99.5        | 197.7            | 252.3            |\n| Level Count        | 176 / 120 / 24 | 256 / 24 / 0 | 68 / 52 / 0 | 0 / 76 / 44 | 60 / 40 / 0 | 80 / 20 / 0 | 52 / 28 / 0  | 52 / 28 / 0 | 8 / 48 / 24 | 52 / 28 / 0 | 44 / 28 / 8      | 32 / 44 / 4      |\n| Type               | Area           | Contour      | Density     | Graph       | Quiver      | Treemap     | Combination  | HR          | Muiltidiff  | PIP         | Total            | Total            |\n| Count              | 80             | 80           | 80          | 80          | 80          | 80          | 120          | 100         | 100         | 80          | 2400             | 2400             |\n| Subcategories      | 2              | 3            | 4           | 4           | 4           | 4           | 30           | 25          | 25          | 20          | 201(101 + 100)   | 201(101 + 100)   |\n| Code Length (AVG.) | 774.4          | 489.4        | 540.0       | 564.5       | 893.4       | 342.2       | 697.4        | 718.9       | 798.2       | 1083.9      | 696.0            | 696.0            |\n| Code Length (STD.) | 161.8          | 87.8         | 104.7       | 117.5       | 631.0       | 36.3        | 163.6        | 265.5       | 271.2       | 290.1       | 278.4            | 278.4            |\n| Level Count        | 52 / 28 / 0    | 0 / 28 / 52  | 44 / 32 / 4 | 56 / 24 / 0 | 0 / 52 / 28 | 52 / 28 / 0 | 12 / 76 / 32 | 4 / 16 / 80 | 0 / 48 / 52 | 0 / 0 / 80  | 1100 / 868 / 432 | 1100 / 868 / 432 |\n\nCode and Instruction Writing: We propose to manually write codes and instructions for ChartMimic based on the collected 600 prototype charts.   Initially; annotators are tasked with meticulously reproducing the 600 prototype charts Python code; resulting in set of 600 (figure, code, instruction) triplets for the Direct Mimic task. Although other coding languages such as JavaScript and R can be used to create charts, current LMMs perform poorly when chart-to-code these other languages (Sun et al., 2024) Therefore, we focus on Python code generation in the current version of ChartMimic. Subsequently; to simulate the scenario of Customized Mimic, annotators are instructed to modify the chart data in the Direct Mimic task by integrating new data from various domains . are then required to modify the corresponding code and instructions; leading to the 600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently; we establish the ChartMimic benchmark, comprising 1, 200 high-quality seed data. using doing using They\n\nData Augmentation. Following the development of seed triplets; we initiate a process of manual data augmentation Annotators are tasked with altering various elements of each seed triplet;, including data; color schemes, mark styles; etc., to produce augmented triplets. For each seed triplet; we create 3 additional augmented triplets: This process enhances ou\n[... content truncated ...]"
      ],
      "pred_answer": "In Figure\u202f10, the original color\u2011palette token **CB_29** is swapped out for the new palette **CB_30**.",
      "ground_truth": [
        [
          "dataset"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "From both papers (Multimodal-CoT and HyperTransformer), what do models experience as Transformer capacity continues to grow across model size and modality?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Both papers report that **the gains from making the Transformer larger eventually level\u2011off**.  \n\n* **HyperTransformer** \u2013 The authors show that adding capacity to the weight\u2011generating Transformer markedly improves the performance of *small* generated CNNs (the extra capacity can \u201cdecouple the complexity of the task space from the complexity of the individual task\u201d). However, when the target CNN itself is already large, the same increase in Transformer size brings little or no additional benefit, i.e. the performance \u201cdoes not provide a clear advantage\u201d and the returns diminish.  \n\n* **Multimodal\u2011CoT** \u2013 The study finds that, even when the multimodal model is scaled up (the Transformer that fuses vision and language), the principal improvements are seen for the modest\u2011size (\u2248\u202f1\u202fB\u2011parameter) models; beyond that, further increases in Transformer capacity yield only marginal gains and the model begins to experience **diminishing returns** (the extra capacity mainly helps with hallucination mitigation and convergence speed, but does not keep raising accuracy).  \n\nThus, across both works, as Transformer capacity continues to grow\u2014whether the downstream model is a CNN of varying size or a multimodal reasoning system\u2014the models start to **experience diminishing returns / performance saturation**, with the most noticeable benefits occurring for the smaller downstream architectures.",
      "ground_truth": [
        [
          "diminish",
          "return"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which embedding design, as illustrated in Figure 1 and described in the text of both HyperTransformer and Multimodal-CoT, drives improved few-shot or reasoning performance?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**Answer:**  \nBoth papers show that the performance boost comes from using a **task\u2011agnostic (or modality\u2011agnostic) feature\u2011embedding backbone whose representations are then conditioned by a Transformer\u2011based module**.  \n\n* In **HyperTransformer**, a conventional CNN (or other feature extractor) is first trained to produce a *generic image embedding* for every input. The support set is fed to a self\u2011attention Transformer, which reads those embeddings and **generates the weights of a new, task\u2011specific CNN (or at least its final\u2011layer classifier)**. The Transformer\u2011generated weights modulate the fixed embedding, so the same backbone can be reused across many episodes while the Transformer injects the task\u2011specific information.\n\n* In **Multimodal\u2011CoT**, the image is encoded by a vision encoder and the text (question, context, options) by a language encoder; both produce **modalities\u2011agnostic embeddings**. A Transformer (or cross\u2011modal encoder) then fuses these embeddings into a **joint multimodal representation** that is used first to generate a rationale and subsequently the answer.  \n\nThus, the **embedding design that drives the improvement** is **a shared, task\u2011independent feature extractor whose outputs are fed into a Transformer that conditions on the support (or multimodal) information to produce task\u2011specific weights or a fused multimodal embedding**. This decouples the complexity of the overall task space (handled by the high\u2011capacity Transformer) from the size of the downstream model, yielding stronger few\u2011shot classification in HyperTransformer and more reliable chain\u2011of\u2011thought reasoning in Multimodal\u2011CoT.",
      "ground_truth": [
        [
          "modal",
          "align",
          "token",
          "embed"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "How would inputs, like questions from the ScienceQA benchmark, be handled by the HyperTransformer architecture using its current encoding setup?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**How a Science\u2011QA question would flow through the HyperTransformer as it is described in the paper**\n\nThe HyperTransformer (HT) is built around a *single\u2011layer self\u2011attention encoder* that receives a **set of \u201csupport tokens\u2019\u2019** and, in a single forward pass, emits the complete weight vector of a tiny convolutional network (the \u201cgenerated CNN\u2019\u2019).  \nThe way the paper describes the encoding pipeline is the following:\n\n| Step | What the paper says | What this means for a Science\u2011QA question |\n|------|----------------------|-------------------------------------------|\n| **1. Tokenisation of the support set** | \u201cthe vanilla Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category.\u201d  The support set is turned into a **sequence of vectors** (one vector per sample).  In the image\u2011only experiments each vector is a *flattened image patch* (or a global image embedding) that is linearly projected to the Transformer dimension. | A Science\u2011QA question is **not an image**; the current HT implementation does not contain a text\u2011specific front\u2011end (e.g. a word\u2011piece or BERT encoder).  To feed a question into HT you would have to **first embed the question** into a fixed\u2011size vector (e.g. by averaging word embeddings, using a pretrained sentence encoder, or even rendering the text as an image and feeding the resulting visual embedding).  That vector would then be treated exactly like any other support token. |\n| **2. Positional information** | \u201cwithout attention masking or positional encodings.\u201d  The self\u2011attention treats the input as an *unordered set*; the order in which tokens appear does not affect the output. | The question\u2011embedding token would be **indistinguishable** from the other support tokens (e.g. labeled examples of the few\u2011shot task).  No special \u201cquestion\u2011position\u2019\u2019 marker is added, so the model would have to infer from the content of the token that it is a *task description* rather than a labeled example. |\n| **3. Self\u2011attention processing** | A single\u2011layer (or a few\u2011layer) self\u2011attention block computes pair\u2011wise interactions among all tokens, producing a **task\u2011specific representation** that is later linearly projected into the weight space of the generated CNN. | The question\u2011embedding token participates in the same attention matrix as the support\u2011example tokens.  Its content can influence the attention scores and therefore the *generated weights* of the CNN.  In practice, the Transformer would learn to use the question token to bias the generated classifier toward the semantics of the query. |\n| **4. Weight generation** | \u201cthe Transformer \u2026 generates an entire inference model by producing all model weights in a single pass.\u201d  The output of the Transformer is reshaped into the weight tensors of a small CNN (e.g. conv\u20111 weights, bias, final\u2011logits layer). | The **generated CNN** is then used as the *task\u2011specific solver*: the question (or its embedding) is fed through the CNN (or through a downstream linear head that the CNN produces) and the resulting logits are interpreted as the answer probabilities for the multiple\u2011choice options of Science\u2011QA.  Because the CNN is tiny, the heavy computation stays in the Transformer; inference on the query itself is cheap. |\n| **5. Training** | The whole pipeline (Transformer\u202f+\u202foptional feature extractor) is trained end\u2011to\u2011end with a cross\u2011entropy loss on the query answers. | During training on Science\u2011QA, the model would see many episodes: each episode contains a **support set** (the few labelled examples that define the task) **plus the question token**.  The loss is computed on the answer produced by the generated CNN, and gradients flow back through the weight\u2011generation head into the self\u2011attention encoder, thereby teaching the encoder how to turn a question\u2011embedding token into useful weight adjustments. |\n\n### Bottom\u2011line answer\n\nWith the **current encoding setup** described in the HyperTransformer paper, a Science\u2011QA question would have to be **first turned into a fixed\u2011dimensional vector** (e.g. by a separate text encoder or by treating the rendered text as an image). That vector would then be **concatenated with the other support\u2011sample vectors** and fed into the *same* permutation\u2011invariant self\u2011attention encoder that processes the support set. The encoder\u2019s output would be reshaped into the weights of a tiny CNN; the generated CNN would finally consume the question (or its embedding) and output the answer logits.\n\nBecause the paper does **not** provide a built\u2011in text encoder or any special handling for multimodal inputs, handling Science\u2011QA questions \u201cout\u2011of\u2011the\u2011box\u2019\u2019 would require an **extra front\u2011end** to embed the textual question into the same token space that the HyperTransformer expects. Once that embedding is supplied, the rest of the pipeline (self\u2011attention \u2192 weight generation \u2192 inference) proceeds exactly as described for image\u2011only few\u2011shot tasks.",
      "ground_truth": [
        [
          "flatten"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which property of the self-attention mechanism, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and described in their Method sections, underpins both CNN weight generation and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The key property is **the permutation\u2011invariant (order\u2011agnostic) nature of the self\u2011attention mechanism**.  \n\n* In **HyperTransformer** (Fig.\u202f2, Method \u00a74) the Transformer\u2019s self\u2011attention treats the support set as an unordered set \u2013 it can ingest any number of labeled (and unlabeled) examples in any order and still produce a coherent representation that is used to generate the CNN\u2019s weights.  \n\n* In **Multimodal\u2011CoT** (Fig.\u202f4, Method \u00a73) the same self\u2011attention layer fuses tokens from different modalities (text, image patches, etc.) without relying on a fixed sequence order, allowing the model to build a unified rationale from a variable\u2011size, multimodal input.\n\nThus, the **order\u2011invariance (set\u2011processing) capability of self\u2011attention** underlies both the generation of task\u2011specific CNN parameters and the generation of multimodal reasoning chains.",
      "ground_truth": [
        [
          "invariant"
        ],
        [
          "invariance"
        ],
        [
          "permutation"
        ],
        [
          "permute"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What shared mechanism, as illustrated in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both Method sections, could unify weight and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Both papers rely on the **same conditional\u2011generation engine \u2013 a self\u2011attention (Transformer) module that turns a task description into a sequence of outputs**.  \n\n* In **HyperTransformer** (Fig.\u202f2) the Transformer receives the support set (or any other task description) as a set of tokens, attends over them with multi\u2011head self\u2011attention, and then emits a flat sequence that is reshaped into the CNN\u2019s weights.  \n\n* In **Multimodal\u2011CoT** (Fig.\u202f4) the same kind of Transformer (or Transformer\u2011based encoder\u2011decoder) ingests the multimodal input (question, image features, optional context) and, via self\u2011attention, generates a textual chain\u2011of\u2011thought \u2013 the rationale \u2013 which is later used for answer inference.  \n\nThus, the shared mechanism is **a Transformer\u2011based conditional generator (self\u2011attention network) that maps the task\u2011specific input representation to a generated output sequence**, whether that sequence is a set of model parameters or a reasoning rationale. This unified generation process can be viewed as a single \u201cgeneration\u2011by\u2011attention\u201d block that can be plugged in to produce either weights or rationales.",
      "ground_truth": [
        [
          "cross"
        ],
        [
          "modal"
        ],
        [
          "self"
        ],
        [
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What failure in representation can occur in both Multimodal-CoT and HyperTransformer particularly in unsupervised learning?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "Both papers point out that, when the learner is forced to work with **unlabeled (or otherwise unsupervised) data**, the generator can end up producing **meaningless or \u201challucinated\u201d representations** \u2013 i.e., feature vectors or weight values that do not faithfully reflect the underlying visual or semantic content.  \n\n* In **Multimodal\u2011CoT**, the authors explicitly observe that small (\u202f<\u202f1\u202fB\u2011parameter) language models tend to **hallucinate** rationales: the intermediate reasoning chain is fabricated rather than grounded in the image\u2011text input, which in turn misleads the final answer.  \n\n* In **HyperTransformer**, the same phenomenon appears when the Transformer is asked to incorporate unlabeled examples: if the model is too shallow (e.g., a single\u2011layer self\u2011attention block) it fails to **encode a useful nearest\u2011neighbour\u2011style mapping** for the unlabeled samples, and the generated CNN weights are effectively based on **spurious or noisy embeddings**. This \u201crepresentation\u2011hallucination\u201d limits the benefit of the extra unlabeled data.  \n\nThus, the shared failure mode in unsupervised (or semi\u2011supervised) settings is **the generation of hallucinated/invalid representations that do not correspond to the true structure of the data**.",
      "ground_truth": [
        [
          "drift"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which design feature, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both texts, supports permutation invariance for few-shot generalization?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "**The permutation\u2011invariant self\u2011attention (Transformer) module that encodes the support set as a set.**  \n\nBoth Figure\u202f2 in *HyperTransformer* and Figure\u202f4 in *Multimodal\u2011CoT* illustrate a Transformer\u2011based \u201cset encoder\u201d (a self\u2011attention block) that processes the few\u2011shot examples without any positional bias. Because self\u2011attention treats its input tokens as an unordered collection, the generated weights (in HyperTransformer) or the multimodal rationale (in Multimodal\u2011CoT) are invariant to the order of the support samples, enabling robust few\u2011shot generalization.",
      "ground_truth": [
        [
          "self attention",
          "flat",
          "sequence"
        ],
        [
          "selfattention",
          "flat",
          "sequence"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What element from the Multimodal-CoT paper, used before Rational Generation in Figure 4, could be incorporated into the HyperTransformer architecture to allow it to classify datasets that contain both images and text?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        "File: 2201.04182v3.pdf\nContent: ## HyperTransformer: Model Generation for Supervised and Semi-Supervised Few-Shot Learning\n\nAndrey Zhmoginov Mark Sandler Max Vladymyrov\n\n## Abstract\n\nwe propose HyperTransformer, Transformer-based model for supervised and semi-supervised few-shot learning that generates weights of a convolutional neural network (CNN) directly from support samples: Since the dependence of a small generated CNN model on specific task is encoded by a high-capacity Transformer model, we effectively decouple the com plexity of the large task space from the complexity of individual tasks. Our method is particularly ef fective for small target CNN architectures where learning a fixed universal task-independent em bedding is not optimal and better performance is attained when the information about the task can modulate all model parameters For models we discover that generating the last layer alone allows us to produce competitive or better results than those obtained with state-of-the-art methods while end-to-end differentiable. larger being\n\n## 1. Introduction\n\nIn few-shot learning, a  conventional   machine  learning paradigm of fitting a parametric model to training data is taken to a limit of extreme data scarcity where entire cat egories are introduced with just one or few examples. A generic approach t0 solving this problem uses training data to identify parameters % of a solver that given a small batch of examples for a particular task (called a support set) can solve this task on unseen data (called a query set) .\n\nOne broad family of few-shot image classification methods frequently referred to as metric-based learning, relies on pretraining an embedding e\u00f8 and then some dis tance in the embedding space to label query samples based using\n\n'Google Research. Correspondence to: Andrey Zhmoginov &lt;azhmogin@google.com&gt;\n\nProceedings of the International Conference on Machine right 2022 by the author(s). 39th Copy - on their closeness to known labeled support samples. These methods proved effective on numerous benchmarks (see Tian et al. (2020) for review and references) , however the capabilities of the solver are limited by the capacity of the architecture itself, as these methods try to build a universal embedding function.\n\nOn the other hand, optimization-based methods such as seminal MAML algorithm (Finn et al., 2017) can fine-tune the embedding e\u00f8 by performing additional SGD updates on all parameters of the model producing it. This partially addresses the constraints of metric-based methods by learning new embedding for each new task. However, in many of these methods; all the knowledge extracted during training on different tasks and describing the solver a\u00f8 still has to ~fit\" into the same number of parameters as the model itself. Such limitation becomes more severe as the target models get smaller; while the richness of the task set increases\n\nIn this paper we propose a new few-shot learning approach that allows us to decouple the complexity of the task space from the complexity of individual tasks The main idea is to use the Transformer model Vaswani et al., 2017) that given a few-shot task episode, generates an entire inference model by producing all model weights in a single pass. This allows us to encode the intricacies of the available training data inside the Transformer model, while producing ized tiny models for a given individual task . Reducing the size of the generated model and moving the computational overhead to the Transformer-based weight generator, we can lower the cost of the inference on new images. This can reduce the overall computation cost in cases where the tasks change infrequently and hence the weight generator is only used sporadically. Note that here we follow the inductive inference paradigm with test samples processed one-by-one (by the generated inference model) and do not target other settings like; for example, transductive inference (Liu et al., 2019) that consider relationships between test samples. special -\n\nWe start by observing that the self-attention mechanism is well suited to be an underlying mechanism for fewshot CNN weight generator: In contrast with earlier CNN(Zhao et al., 2020) or BiLSTM-based approaches (Ravi &amp;\n\nLarochelle, 2017), the vanilla' Transformer model is invariant to sample permutations and can handle unbalanced datasets with a varying number of samples per category. Fur thermore, we demonstrate that a single-layer self-attention model can replicate simplified gradient-descent-based learning algorithm.   Using a Transformer to generate the logits on top of a conventionally end-to-end learned embedding, we achieve competitive results on several common few-shot learning benchmarks. For smaller generated CNN models, our approach shows significantly better per formance than MAML++ (Antoniou et al., 2019) and RFS (Tian et al., 2020), while also closely matching the perfor mance of many state-of-the-art methods for larger CNN models.  Varying Transformer parameters we demonstrate that this high performance can be attributed to additional capacity of the Transformer model that decouples its complex ity from that of the generated CNN. While this additional capacity proves to be very advantageous for smaller genmodels, larger CNNs can accommodate sufficiently complex representations and our approach does not provide a clear advantage compared to other methods in this case. layer erated\n\nWe additionally can extend our method to support unlabeled by appending a input token that encodes unknown classes to all unlabeled examples. In our experiments outlined in Section 5.3, observe that adding unlabeled samples can significantly improve model performance. Interestingly; the full benefit of additional data is only realized if the Transformers use two or more layers. This result is consistent with the basic mechanism described in Section 4.2, where we show that a Transformer model with at least two layers can encode the nearest-neighbor style algorithm that associates unlabeled samples with similar labeled examples. In essence; by training the weight generator to produce CNN models with best possible performance on query set, we teach the Transformer to utilize unlabeled samples without having to manually introduce additional optimization objectives. samples special we using\n\nWe also explore the capability of our approach to generate all weights of the CNN model, adjusting both the logits and all intermediate layers producing the sample embedding. We show that by generating all layers we can improve both the training and test accuracies of CNN models below certain size. Above this model size threshold, however; generation of the logits layer alone on top of a episode-agnostic embedding appears to be sufficient for reaching peak perfor mance (see Figure 3). This threshold is expected to depend on the variability and the complexity of the training taskslayer\n\nAnother important advantage of our method is that it allows to do learning end-to-end without relying on complex nested gradients optimization and other meta-learning approaches; where the number of unrolls steps is large. Our optimiza -\n\nwithout attention masking or positional encodings\n\ntion is done in a single of updates to the Transformer (and feature extractor) parameters. The code for the paper can be found at researchltreelmaster/hypertransformer. loop\n\n## 2. Related work\n\nFew-shot learning received a lot of attention from the learning community and while there are hundreds of fewshot learning methods, several common themes emerged in the past years \\_ Here we outline several existing approaches and show how relate to our method. deep they\n\nMetric-Based Learning: One family of approaches involves mapping input samples into an embedding space and then some nearest neighbor algorithm to label query samples based on the distances from their embeddings to embeddings of labeled support samples. The metric used to compute the distances can either be the same for all tasks; or can be task-dependent. This family of methods includes, for example, such methods as Siamese networks (Koch et al., 2015), Matching Networks (Vinyals et al., 2016), Prototypical Networks (Snell et al., 2017), Relation Networks et al., 2018) and TADAM (Oreshkin et al., 2018) It has recently been argued (Tian et al., 2020) that methods based on building powerful sample representation can frequently outperform numerous other approaches including many optimization-based methods. However; such approaches essentially amount to the 'one-model solves all\" approach and thus require larger models than needed to solve individual tasks . using (Sung\n\nOptimization-Based Learning: An alternative approach that can adapt the embedding to a new task is to incorporate optimization within the learning process\\_ A variety of such methods are based on the approach called Model Agnostic Meta-Learning, or MAML (Finn et al., 2017). The core idea of MAML is learning initial model parameters 0 that produce good models for each episode after adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss. This approach was later refined Antoniou et al., 2019) and built upon giving rise to Reptile (Nichol et al. , 2018), LEO (Rusu et al., 2019) and others. One limitation of various MAMLinspired methods is that the knowledge about the set of training tasks is distilled into parameters 0 that have the same dimensionality as the model parame ters the support set is still limited by the size of 0 Methods that use parameterized preconditioners that otherwise do not impact the model f (\u00e6; 0) can alleviate this issue, but as with MAML, such methods can be difficult to train (Antoniou et al., 2019). being Ttrain\n\nWeight Modulation and Generation. The idea of a task specification to directly generate or modulate model weights has been previously explored in the generalized supervised learning context (Requeima et al., 2019; Ratzlaff &amp; Li, 2019), few-shot learning (Guo &amp; Cheung, 2020) and in specific language models (Pilault et al., 2021; Mahabadi et al., 2021; shot learning methods described above also employ this approach and use task-specific generation or modulation of the weights of the final classification model. For example, in LGM-Net (Li et al., 2019b) the matching network approach is used to generate a few layers on top of a task-agnostic embedding. Another approach abbreviated as LEO (Rusu et al., 2019) utilized a similar weight generation method to generate initial model weights from the training dataset in few-shot learning setting, much like what is proposed in this article. However, in Rusu et al. (2019), the generated weights were also refined using several SGD steps similar to it is done in MAMLHere we explore a similar idea; but largely inspired by the HYPERNETWORK approach (Ha et al., 2017), we instead propose to directly generate an entire task-specific CNN model. Unlike LEO, do not rely on pre-computed embeddings for images and generate the model in single step without additional SGD steps; which simplifies and stabilizes training. using Tay how we\n\nTransformers in Computer Vision and Few-Shot LearnTransformer models Vaswani et al. , 2017) originally proposed for NLP applications; had since become a useful tool in practically every field of deep learning. In computer vision; Transformers have recently seen an explosion of applications ranging from state-of-the-art classification results (Dosovitskiy et al., 2021; Touvron et al., 2021) to object detection (Carion et al., 2020; Zhu et al., 2021), segmentation Ye et al., 2019), image super-resolution Yang et al. , 2020), generation (Chen et al., 2021) and many others. There are also several notable applications in few-shot image classification For example; in Liu et al. (2021), the Transformer model was used for generating universal representations in the multi-domain few-shot learning scenario. And closely related to our approach, in Ye et al. (2020), the authors proto accomplish embedding adaptation with the help of Transformer models. Unlike our method that generates an entire end-to-end image classification model, this approach task-dependent perturbation to an embedding erated by an independent task-agnostic feature extractor. In Gidaris &amp; Komodakis (2018), a simplified attention-based model was used for the final generation. image posed applied gen layer\n\n## 3. Analytical Framework\n\nHere we establish a general framework that includes shot learning as a special case, but allows us to extend it to cases when more information is available beyond few few - supervised samples; e.g. using additional unlabeled data.\n\n## 3.1. Learning from Generalized Task Descriptions\n\nConsider a set of tasks {t|t \u20ac T} each of which is associated with a loss L(f;t) that quantifies the correctness of any model f attempting to solve t. A task can be associated with a classification; regression; learning a reinforcement learning policy or any other kind of problem. with the loss; each task also is characterized by a task description T(t) that is sufficient for communicating this task and the optimal model that solves it. This task description can include any available information about t, like labeled and unlabeled samples, image metadata; textual descriptions; etc\\_ Along finding\n\nThe weight generation algorithm can then be viewed as a method of using a set of training tasks Ttrain for discovering those present in the training set, produces an optimal model f* \u20ac F minimizing L(fx,t). In this paper; we learn a\u00f8 by performing gradient-descent optimization of\n\n<!-- formula-not-decoded -->\n\nwith p(t) being the distribution of training tasks from train.\n\n## 3.2 Special Case of Few-Shot Learning\n\nFew-shot learning is a special case of the framework described above. In few-shot learning, the loss Lt of a task t is defined by a labeled query set Q(t). The task description T(t) is then specified via a support set of examples. In a classical \"n-way-k-shot\" setting, each task t \u20ac is sampled by first randomly choosing n distinct classes Ct from a large training dataset and then sampling examples without replacement from these classes to generate T(t) and Q(t). The support set T(t) in this setting always contains k labeled samples {\u00e6(c) \u20ac Xli \u20ac [1, k]} for each of classes c \u20ac Ct. training Ttrain\n\nThe quality of a particular few-shot learning algorithm is evaluated separate test space of tasks Ttest . By formTtest from classes unseen at training time; we can eval uate generalization of the trained solver a+ by computing accuracies of models are expected to capture the structure present in the training set, extrapolating it to novel, but related tasks. using ing\n\nEquation 1 describes the general framework for learning to solve tasks given their descriptions T(t) When T is given by supervised samples; recover classic few-shot learning. But the freedom in the definition of T permits us, for example\n[... content truncated ...]",
        "File: 2302.00923v5.pdf\nContent: ## Multimodal Chain-of-Thought Reasoning in Language Models\n\n## Zhuosheng Zhang'\n\non OpenReview: https:// openreview. net/ forum? id=ylpPWFVfvR\n\nzhangzs@sjtu.edu.cn\n\nSchool of Electronic Information and Electrical Engineering; Shanghai Jiao Tong University\n\n## Aston Zhang*\n\n@astonzhang.com\n\nGenAI, Meta\n\n## Mu Li\n\nmuli@cs.cmu. edu\n\nAmazon Web Services\n\n## Hai Zhao\n\nzhaohai@cs.sjtu. edu.cn\n\nDepartment of Computer Science and Engineering; Shanghai Jiao Tong University\n\n## George Karypis\n\ngkarypis@amazon. com\n\nAmazon Web Services\n\n## Alex Smola\n\naler@smola.org\n\nAmazon Web Services\n\nReviewed\n\n## Abstract\n\nlanguage models (LLMs) have shown impressive performance on complex reasoning by leveraging chain-of-thought (CoT) prompting to generate intermediate reasoning chains as the rationale to infer the answer\\_ However, existing CoT studies have primarily focused on the language modality.  We propose Multimodal-CoT that incorporates language (text) and vision (images) modalities into a two-stage framework that separates rationale generation and answer inference In this way; answer inference can leverage better generated rationales that are based on multimodal information. Experimental results on ScienceQA and A-OKVQA benchmark datasets show the effectiveness of our proposed approach. With MultimodalCoT, our model under 1 billion parameters achieves state-of-the-art performance on the ScienceQA benchmark.  Our analysis indicates that Multimodal-CoT offers the advantages of mitigating hallucination and enhancing convergence speed. Code is publicly available at https I/github.com/amazon-science\\_ mm-cot Large\n\n## 1 Introduction\n\nImagine reading a textbook with no figures or tables. Our ability to knowledge acquisition is greatly Recently; large language models (LLMs) (Brown et al, 2020; Thoppilan et al., 2022; Rae et al., 2021; Chowdhery et al., 2022) have shown impressive performance in complex reasoning by generating intermediate reasoning steps before inferring the answer. The intriguing technique is called chain-of-thought (CoT) reasoning et al. 2022b; Kojima et al., 2022; Zhang et al., 2023d) . (Wei\n\nWork done at Amazon Web Services. Correspondence to: Zhuosheng Zhang and Aston Zhang.\n\nHowever; existing studies related to CoT reasoning are largely isolated in the language modality (Wang et al., 2022), with little consideration of multimodal scenarios . To elicit CoT reasoning in multimodality; we advocate Multimodal-CoT paradigm. Given the inputs in different modalities; Multimodal-CoT decomposes multi-step problems into intermediate reasoning steps (rationale) and then infers the an-swer\\_ Since vision and language are the most popular modalities; we focus on those two modalities in this work. An example is shown in Figure 1\\_\n\nFigure 1: Example of the multimodal CoT task.\n\n<!-- image -->\n\nIn general, Multimodal-CoT reasoning can be elicited through two primary paradigms:  (i) prompting LLMs and (ii) fine-tuning smaller models. We will delve into these paradigms and delineate their associated challenges as follows.\n\nThe most immediate way to perform Multimodal-CoT is to transform the input of different modalities into a For example; it is possible to generate a caption for an image by a captioning model and then concatenate the caption with the original language input to be fed into LLMs (Lu et al. The development of multimodal models such as GPT-4V (OpenAI, 2023) and Gemini (Reid et 2024) has notably enhanced the quality of generated captions; resulting in finer-grained and more detailed descriptions. However; the captioning process still incurs significant information loss when transforming vision signals into textual descriptions. Consequently;   using image captions rather than vision features may suffer from a lack of mutual synergy in the representation space of different modalities. In addition; LLMs either have paywalls or resource-consuming to deploy locally. large al.\n\nTo facilitate the interaction between modalities; another potential solution is to fine-tune smaller language As this approach allows the flexibility of adjusting model architectures to incorporate multimodal features; we study fine-tuning models in this work instead of prompting LLMs. The challenge is that language models under 100 billion parameters tend to generate hallucinated rationales that mislead the answer inference (Ho et al., 2022; key\n\nTo mitigate the challenge of hallucination, we propose Multimodal-CoT that incorporates language (text) vision (images) modalities into a two-stage framework that separates rationale generation and answer inference 2 In this way; answer inference can leverage better generated rationales that are based on multimodal information\\_ Our experiments were conducted on the ScienceQA (Lu et al. , 2022a) and A-OKVQA (Schwenk et al., 2022) datasets; which are the latest multimodal reasoning benchmarks with annotated reasoning chains. and\n\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. that Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence: Our contributions are summarized as follows:\n\n- To the best of our knowledge; this work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature.\n- (ii) We propose a two-stage framework by fine-tuning language models to fuse vision and language representations to perform Multimodal-CoT. The model is able to generate informative rationales to facilitate inferring final answers.\n- (iii) We elicit the analysis of why the naive way of employing CoT fails in the context and how incorporating vision features alleviates the problem. The approach has been shown to be generally effective across tasks and backbone models.\n\nrefer to small models as models with less than 1 billion parameters (hereinafter dubbed as IB-models) 1We\n\nwork focuses on the language and vision modalities. 2This\n\nTable 1: Representative CoT techniques (FT: fine-tuning; KD: knowledge distillation) . Segment 1: in-context learning techniques; Segment 2: fine-tuning techniques.  To the best of our knowledge; our work is the first to study CoT reasoning in different modalities in scientific peer-reviewed literature. Besides; we focus on\n\n| Models                                      | Mutimodal   | Model / Engine   | Training   | CoT Role    | CoT Source     |\n|---------------------------------------------|-------------|------------------|------------|-------------|----------------|\n| Zero-Shot-CoT (Kojima et al., 2022)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Template       |\n| Few-Shot-CoT (Wei et al., 2022b)            | \u2717           | PaLM (540B)      | ICL        | Reasoning   | Hand-crafted   |\n| Self-Consistency-CoT (Wang et al., 2022b)   | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Least-to-Most Prompting (Zhou et al., 2022) | \u2717           | Codex (175B)     | ICL        | Reasoning   | Hand-crafted   |\n| Retrieval-CoT (Zhang et al., 2023d)         | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Auto-generated |\n| PromptPG-CoT (Lu et al., 2022b)             | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Auto-CoT (Zhang et al., 2023d)              | \u2717           | Codex (175B)     | ICL        | Reasoning   | Auto-generated |\n| Complexity-CoT (Fu et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| Few-Shot-PoT (Chen et al., 2022)            | \u2717           | GPT-3.5 (175B)   | ICL        | Reasoning   | Hand-crafted   |\n| UnifiedQA (Lu et al., 2022a)                | \u2717           | T5 (770M)        | FT         | Explanation | Crawled        |\n| Fine-Tuned T5 XXL (Magister et al., 2022)   | \u2717           | T5 (11B)         | KD         | Reasoning   | LLM-generated  |\n| Fine-Tune-CoT (Ho et al., 2022)             | \u2717           | GPT-3 (6.7B)     | KD         | Reasoning   | LLM-generated  |\n| Multimodal-CoT (our work)                   | \u2713           | T5 (770M)        | FT         | Reasoning   | Crawled        |\n\n## 2 Background\n\nThis section reviews studies eliciting CoT reasoning by prompting and fine-tuning language models.\n\n## 2.1 CoT Reasoning with LLMs\n\nRecently; CoT has been widely used to elicit the multi-step reasoning abilities of LLMs (Wei et al, 2022b) . Concretely; CoT techniques encourage the LLM to generate intermediate reasoning chains for solving a problem. Studies have shown that LLMs can perform CoT reasoning with two major paradigms of techniques: Zero-Shot-CoT (Kojima et al., 2022) and Few-Shot-CoT (Wei et al. 2022b; Zhang et al. 2023d) . For Zero-Shot-CoT, Kojima et al. (2022) showed that LLMs are decent zero-shot reasoners by adding a prompt like \"Let's think step by step' after the test question to invoke CoT reasoning. For Few-Shot-CoT, few step-by-step reasoning demonstrations are used as conditions for inference. Each demonstration has a question and a reasoning chain that leads to the final answer. The demonstrations are commonly obtained by hand-crafting or automatic generation These two techniques, hand-crafting and automatic generation are thus referred to as Manual-CoT (Wei et al., 20226) and Auto-CoT (Zhang et al., 2023d) .\n\nWith effective demonstrations; Few-Shot-CoT often achieves stronger performance than Zero-Shot-CoT and has attracted more research interest.  Therefore; most recent studies focused on how to improve Few-Shot-CoT. Those studies are categorized into two major research lines:   (i) optimizing the demonstrations; (ii) optimizing the reasoning chains. Table 1 compares typical CoT techniques.\n\nOptimizing Demonstrations The performance of Few-Shot-CoT relies on the quality of demonstrations As reported in Wei et al. (20226), demonstrations written by different annotators results in dramatic accuracy disparity in reasoning tasks. Beyond hand-crafting the demonstrations ; recent studies have the semantically similar demonstrations with the test instance:.  However this approach shows a degraded performance when there are mistakes in the reasoning chains (Zhang et al., 2023d) . To address the limitation; Zhang et al. (2023d) found that the is the diversity of demonstration questions and proposed Auto-CoT: (i) partition questions of a given dataset into a few clusters; (ii) sample representative question from each cluster and generate its reasoning chain using Zero-Shot-CoT with simple heuristics. In addition; reinforcement learning (RL) and complexity-based selection strategies were proposed to obtain effective demonstrations Fu et al. (2022) chose examples with complex reasoning chains (i.e., with more reasoning steps) as the demonstrations. a candidate and maximize the prediction rewards on given training examples when interacting with GPT-3.5. using key pool\n\nOptimizing Reasoning Chains A notable way to optimize reasoning chains is problem decomposition. Zhou et al. (2022) proposed least-to-most prompting to decompose complex problems into sub-problems and then solve these sub-problems sequentially. As a result , solving given sub-problem is facilitated by the answers to previously solved sub-problems:  Similarly, Khot et al. (2022) used diverse decomposition structures and designed different prompts to answer each sub-question.  In addition to prompting the reasoning chains as natural language texts; Chen et al. (2022) proposed program-of-thoughts (PoT), which modeled the reasoning process as a program and prompted LLMs to derive the answer by executing the generated Another trend is to vote over multiple reasoning paths for a test question.  Wang et al. (20226) introduced a self-consistency decoding strategy to sample multiple outputs of LLMs and then took a majority over the final answers et al. (2022c) and Li et al. (2022c) introduced randomness in the input space to produce more diverse outputs for voting. Wang\n\n## 2.2 Eliciting CoT Reasoning by Fine-Tuning Models\n\nrecent interest is eliciting CoT reasoning by fine-tuning language models: Lu et al. (2022a) fine-tuned the encoder-decoder T5 model on a large-scale dataset with CoT annotations.   However; a dramatic performance decline is observed when CoT to infer the answer ie , generating the reasoning chain before the answer (reasoning) . Instead, CoT is only used as an explanation after the answer . Magister et al. (2022) and Ho et al. (2022) employed knowledge distillation by fine-tuning a student model on the chain-of-thought outputs generated by a larger teacher model. et al. (2022a) proposed an iterative context-aware prompting approach to dynamically synthesize prompts conditioned on the current step's contexts. using Wang\n\nThere is a key challenge in training IB-models to be CoT reasoners. As observed by Wei et al. (20226) , models under 100 billion parameters tend to produce illogical CoT that leads to wrong answers\\_ In other words, it might be harder for IB-models to generate effective CoT than directly generating the answer.  It becomes even more challenging in multimodal setting where answering the question also requires understanding the multimodal inputs. In the following part; we will explore the challenge of Multimodal-CoT and investigate how to perform effective multi-step reasoning:\n\n## 3 Challenge of Multimodal-CoT\n\nExisting studies have suggested that the CoT reasoning ability may emerge in language models at a certain scale; e.g.over 100 billion parameters (Wei et al., 2022a) . However\\_ it remains an unresolved challenge to elicit such reasoning abilities in 1B-models; let alone in the multimodal scenario This work focuses on 1B-models as they can be fine-tuned and deployed with consumer-grade GPUs (e.g-, 32G memory) . In this section; we will investigate why 1B-models fail at CoT reasoning and study how to design an effective approach to overcome the challenge.\n\n## 3.1 Towards the Role of CoT\n\nTo begin with, we fine-tune a text-only baseline for CoT reasoning on the ScienceQA benchmark (Lu et al., 2022a) . We adopt FLAN-AlpacaBase as the backbone language model.3 Our task is modeled as text generation problem, where the model takes the textual information as the input and generates the output sequence that consists of the rationale and the answer .\n\nAs an example shown in Figure 1, the model takes the concatenation of tokens of the question text (Q), the context text   (C), and multiple options (M) as the input. To study the effect of CoT, we compare the performance with three variants: (i) No-CoT which predicts the answer directly (QCM-A); (ii) Reasoning where answer inference is conditioned to the rationale\n\n| Table 2: Method   | Effects of CoT in the Format   |   one-stage setting. Accuracy |\n|-------------------|--------------------------------|-------------------------------|\n| No-CoT            | QCM \u2192 A                        |                         81.63 |\n| Reasoning         | QCM \u2192 RA            \n[... content truncated ...]"
      ],
      "pred_answer": "The component that appears **just before the \u201cRationale Generation\u201d block in Figure\u202f4 of the Multimodal\u2011CoT paper is the **visual (image) encoder** \u2013 the module that turns raw pictures into a dense visual embedding (often a CNN or a Vision\u2011Transformer) and then fuses that embedding with the textual tokens.  \n\nBy plugging such a visual\u2011encoder\u202f+\u202ffusion front\u2011end into HyperTransformer (e.g., feeding the combined image\u2011+\u2011text representation to the Transformer that generates the CNN weights), the system would gain the ability to handle tasks whose support set contains both images and text, enabling it to generate task\u2011specific classifiers for multimodal datasets.",
      "ground_truth": [
        [
          "vision",
          "text",
          "fusion"
        ],
        [
          "image",
          "language",
          "fusion"
        ],
        [
          "vision",
          "language",
          "fusion"
        ],
        [
          "image",
          "text",
          "fusion"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Figure 2 of DocLLM and Figure 3 of NPHardEval4V demonstrate a different type of reasoning as opposed to structural reasoning. What type of reasoning is demonstrated?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]",
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The two figures are meant to illustrate **algorithmic (computational) reasoning** \u2013 i.e., reasoning that involves solving or infilling algorithmic problems (such as masked\u2011text infilling in DocLLM and the NP\u2011hard problem tasks in NPHardEval4V), rather than merely reasoning about the document\u2019s structural/layout layout.",
      "ground_truth": [
        [
          "spatial"
        ],
        [
          "space"
        ],
        [
          "visual"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which common objective, as shown in Figure 2 of DocLLM and mentioned in NPHardEval4V, involves predicting missing content from context?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        "File: 2401.00908v1.pdf\nContent: ## DOcLLM: A LAYOUT-AWARE GENERATIVE LANGUAGE MODEL FOR MULTIMODAL DOCUMENT UNDERSTANDING\n\n## Dongsheng Wang* Natraj Raman Mathieu Sibue* Zhiqiang Ma, Petr Babkin, Simerjot Kaur, Yulong Pei, Armineh Nourbakhsh, Xiaomo Liu\n\n{first.last}@jpmchase.com\n\nJPMorgan AI Research\n\n## ABSTRACT\n\nEnterprise documents such as forms, invoices, receipts, reports, contracts; and other similar records often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper; we present DocLLM; a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents , into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices \\_ Furthermore, we devise a pre-training objective that learns to infill text segments\\_ This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents The pre-trained model is fine-tuned large-scale instruction dataset;, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks; and generalizes well to 4 out of 5 previously unseen datasets . taking spatial using\n\nKeywords DocAI VRDU LLM GPT\n\n## Introduction\n\nDocuments with rich layouts, including invoices; receipts, contracts; orders, and forms, constitute a significant portion of enterprise corpora. The automatic interpretation and analysis of these documents offer considerable advantages [1], bespoke type-setting, and often exhibit variations in templates; formats and quality  Although Document AI (DocAI) has made tremendous progress in various tasks including extraction; classification and question answering, there remains a significant performance gap in real-world applications. In particular; accuracy, reliability; contextual understanding and generalization to previously unseen domains continues to be a challenge [2].\n\nDocument intelligence is inherently a multi-modal problem with both the text content and visual layout cues critical to understanding the documents. It requires solutions distinct from conventional large language models such as GPT-3.5 [3], Llama [4], Falcon [5] or PaLM [6] that primarily accept text-only inputs and assume that the documents exhibit simple layouts and uniform formatting, which may not be suitable for handling visual documents. Numerous vision-language frameworks [7, 8] that can process documents as images and capture the interactions between textual and visual modalities are available. However, these frameworks necessitate the use of complex vision backbone architectures [9] to encode image information; and often make use of spatial information as an auxiliary contextual signal [10, 11] being they\n\nIn this paper we present DocLLM, light-weight extension to standard LLMs that excels in several visually rich form understanding tasks Unlike traditional LLMs, it models both layouts and text semantics; and therefore is spatial\n\nThese authors contributed equally to this work.\n\nSpatial Attention\n\nelements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention mechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text blocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions: Key\n\n<!-- image -->\n\nintrinsically multi-modal.  The layout information is incorporated through bounding box coordinates of the text tokens obtained typically using optical character recognition (OCR), and does not rely on any vision encoder component. Consequently; our solution preserves the causal decoder architecture, introduces only marginal increase in the model size; and has reduced processing times; as it does not rely on a complex vision encoder: We demonstrate that merely including the spatial layout structure is sufficient for various document intelligence tasks such as form understanding, table alignment and visual question answering. spatial\n\nExisting efforts to incorporate spatial layout information typically involve either concatenating spatial and textual embeddings [12] or summing the two [13] In contrast, we treat the spatial information as a distinct modality and compute its inter-dependency with the text modality in a disentangled manner [14]. In detail, we extend the self-attention mechanism of standard transformers to include new attention scores that capture cross-modal relationships. This is motivated by the observation that there is often a correlation between the content; position and size of the fields in a form. Representing their alignments at various abstraction levels across the transformer layers can enhance document understanding.\n\ncommon characteristic of visual documents is their heterogeneous content; irregular layouts, and disjointed text segments. When working with such documents; employing a classical next token prediction objective during the self-supervised pre-training phase can be restrictive. In particular; the preceding tokens may not always be relevant due to the diverse arrangements of text; which can be positioned horizontally, vertically; or even in a staggered manner: To tackle this issue, we propose two modifications to the pre-training objective: (a) adopting cohesive blocks of text that account for broader contexts, and (b) implementing an infilling approach by conditioning the prediction on both preceding and succeeding tokens. Due to these modifications; the model is better equipped to address misaligned text; contextual completions, intricate layouts, and mixed data types. Although text spans and infilling tasks have been studied before [15], our solution is tailored for visual documents with an emphasis on semantically coherent blocks.\n\nWe adapt the pre-trained knowledge of DocLLM for several document intelligence tasks by fine-tuning it on instruction data curated from several datasets . These tasks encompass key information extraction; natural language inference; visual question-answering and document classification: Our instruction-tuning data covers both single and multi-page documents. Layout hints such as field separators, titles and captions can be integrated during instruction-tuning to facilitate learning the logical structure of the documents . We observe that the modifications introduced by DocLLM result in a performance improvement ranging from 159 to 619 for the Llama2-7B model in four out of five previously unseen datasets-\n\n## Fig. 1 summarizes the framework. Our contributions include:\n\n- 1.\n- 2 A disentangled spatial attention mechanism that captures cross-alignment between text and layout modalities.\n- 3 . An infilling pre-training objective tailored to address irregular layouts effectively.\n- 4 An instruction-tuning dataset specially curated towards visual document intelligence tasks.\n5. Comprehensive experiments and valuable insights into the model behavior:\n\n## Related Work\n\n## 2.1 LLMs\n\nThe remarkable success of ChatGPT has generated substantial research interest in LLMs across academia and industry. Subsequently; numerous LLMs have been introduced starting from text-based LLMs [16, 17, 4, 18] to multimodal review these recent advances in LLMs and discuss their connection to and distinctions from our work. we\n\nText-based LLMs. The introduction of the transformer model in 2017 [24] has been foundational for the pre-trained models such as BERT [25], GPT [26], and T5 [27], each designed with specific pre-training objectives. The emergence of ChatGPT and GPT-4 marked a notable shift, characterized by a substantial increase in both model parameters and training data size. This enhancement has resulted in remarkable zero-shot generalization capabilities; allowing these models to excel in tasks previously unseen. Such success of LLMs has prompted the development of additional LLMs achieves comparable or better performance to both open and closed-sourced models, including ChatGPT, PaLM and Falcon; with enhanced safety strategies. Llama2 employs the standard Transformer architecture with pre-normalization [28], SwiGLU activation function [29], and rotary positional embeddings [30]. The pre-training data consists of two trillion tokens from publicly available sources -\n\nMultimodal LLMs. Multimodal LLMs extend the scope of text to diverse modalities, with a focus on visual input. These models can be categorized into two tropes: general-purpose multimodal LLMs [19, 20, 21, 22, 23] and models that are tailored for visually-rich document understanding [31,32, 33, 34, 12]. The general-purpose multimodal LLMs exhibit promising performance in identifying and reasoning with image information. However; have not yet been vigorously evaluated on VRDU tasks. As an example; the GPT-4 Technical Report [16] highlights diverse multimodal test cases, such as explaining meme picture distinctiveness; but very few examples are included for visual document use cases Prior to the advent of large language models, fine-tune-based models relying on vision only were less effective than layout (and vision) modality models in processing visual documents \\_ For example, models like UDOP [12] and LayoutLM [13] outperform vision-only models such as Donut [35] and Pix2Struct [34] in VRDU tasks. But such models require task - and dataset-specific fine-tuning, and are thus excluded in our analysis. The more recent mPLUG-DocOwl [31] and UReader [32], built upon LLMs; undergo instruction finetuning on a diverse set of VRDU, visual, and textual datasets; and exhibit impressive zero-shot generalization capabilities. Hence; we include those as baselines in our evaluation in Section 4. they\n\nDespite the remarkable performance of LLMs, unimodal models aren't equipped to process multimodal input; and multimodal LLMs rely on complex and memory intensive open-domain vision encoders. Our proposed model, DocLLM; addresses these challenges by explicitly modeling spatial layouts and text semantics; enabling effective comprehension of visual documents. Notably; DocLLM offers an extension to the unimodal architecture by adding the spatial signal to\n\n## 2.2 LLM Architectures\n\nAutoregressive Infilling: There are two main autoregressive infilling approaches: \"fill-in-the-middle\" (FIM) where a single span is sampled, and 'blank infilling' with multiple spans.\n\nThe OpenAI FIM approach [36] uses the template (prefix middle\\_ suffix) to divide a document into three segments. Next; these segments are reorganized into (prefix suffix, middle) , enabling the model to predict the middle segment. This process relies on three tokens, [PRE] [SUF] and [MID] which structure document as: [PRE] prefix [SUF] suffix [MID] middle. The [MID] token denotes the start for prediction; while the other two special tokens guide the model on where to infill. This method demonstrates that autoregressive models can learn to infill text where the middle part is missing. Fill-in Language Model (FiLM) [37] is a subsequent development that enables flexible generation at arbitrary positions, unconstrained by predefined generation order. In contrast; approaches like GLM [15] sample multiple spans for infilling. For each blank to be infilled; a of special tokens is used: [blank\\_mask] and [start\\_to\\_fill] The multiple spans not only require special tokens but also global indicators to distinguish which middle span the model should infill. This global indicator is implemented with ID token positions; ensuring that each of the two tokens, [blank\\_mask] and [start to\\_fill] share the same positions. We similar infilling object with the to prevent disconnected next-token predictions while avoiding breaking sparse documents into very short segments;, e.g., word pieces andlor phrase pieces. special pair pair special adopt ie, goal\n\nDisentangled attention. Disentangled attention is introduced in the DeBERTa model [38], where token embeddings and relative positional encodings were kept separate rather than summed together; and each used independently when computing attention weights disentangled matrices. The motivation behind this was to facilitate the learning of using\n\nFigure 2: DocLLM model architecture with disentangled spatial attention and infilling objective: left: Input document with text tokens Si and bounding boxes bi Some text segments are randomly masked (two segments here) and the model predicts the tokens in these text segments autoregressively. right: The infilling sequence is created by replacing the sampled segments with [M] and prepending them with [S] . The attention mechanism is extended to account for cross-attention between text and spatial modalities.\n\n<!-- image -->\n\ndecoupled attention alignments based on content and position separately . This innovation proved effective as it allowed DeBERTa to outperform RoBERTA-large and T5 on NLU benchmarks, as well as to surpass the human baseline on SuperGLUE [39]. In our work, given considerably more complex position encodings used in visually rich documents; disentanglement becomes ever more important to our model's performance.\n\n## 3 DocLLM Framework\n\nIn this section; we discuss the architecture of DocLLM and outline the pre-training and instruction tuning procedures. Figure 2 presents an overview of the model architecture .\n\n## 3.1 Model Architecture\n\nDocLLM is constructed upon the foundation of an auto-regressive transformer language model [4] following a causal decoder structure It is composed of stacked transformer blocks, where each block contains a multi-head self-attention layer and a fully connected feed forward network: Standard language models are typically unimodal; accepting only sequence of text tokens as input. In contrast, DocLLM is a multi-modal system that integrates lightweight visual information by utilizing the spatial positions and dimensions of text tokens obtained OCR. Simply augmenting the text with bounding box information via additive positional encoding may not capture the intricate relationships between text semantics and spatial layout; especially for visually rich documents [10]. Consequently; we treat the spatial information about the text tokens as a distinct modality. In particular; we use separate vectors to represent these two modalities and extend the self-attention mechanism of the transformer architecture to compute their inter-dependencies in a disentangled manner; as explained in the following section: Furthermore, instead of the traditional left-to-right next token predic\n[... content truncated ...]",
        "File: 2403.01777v2.pdf\nContent: ## NPHardEval4V: A Dynamic Reasoning Benchmark of Multimodal Large Language Models\n\nFigure 1: Multimodal Large Language Models 's performance on recognition (RA), Instruction following (ER), and reasoning (AA) on polynomial time; NP-complete; and NP-hard problems.\n\n<!-- image -->\n\nLizhou Fan; Wenyue and Li made equal contributions. Correspondence to: lizhouf@umich.edu, wenyue hua @rutgers.edu, yongfeng.zhang @rutgers.edu. More author information are available in Appendix. Hua. Xiang\n\n## bstract\n\nUnderstanding the reasoning capabilities of Multimodal Large Language Models (MLLMs) is an important area of research. In this study, we introduce a dynamic benchmark, NPHardEval4V , aimed at addressing the existing gaps in evaluating the pure reasoning abilities of MLLMs. Our benchmark aims to provide a venue to disentangle the effect of various factors such as image recognition and instruction following, from the overall  performance of the models, allowing us to focus solely on evaluating their reasoning abilities. It is built by converting textual description of questions from NPHardEval to image representations: Our findings reveal significant discrepancies in reasoning abilities across different models and highlight the relatively weak performance of MLLMs compared to LLMs in terms of reasoning. We also investigate the impact of different prompting styles; including of MLLMs, demonstrating the different impacts of multimodal inputs in model performance. Unlike traditional benchmarks; which focus primarily on static evaluations; our benchmark will be updated monthly to prevent overfitting and ensure a more authentic and 'grained evaluation of the models. We believe that this benchmark can aid in understanding and the further development of reasoning abilities in MLLMs. The benchmark dataset and code are available at https://github.com/lizhouf /NPHardEval4V. fine guide\n\n## Introduction\n\nThe evolution of Multimodal Large Language Models (MLLMs) marks a significant milestone in the pursuit of artificial general intelligence (AGI), following the advancement of Large Language Models (LLMs) [1, 2] It introduces new capabilities for understanding and generating content that spans both text and visual inputs, contributing to enhanced multimedia interaction systems and sophisticated cross-modal decision-making tools [3,4]. Reasoning is a critical ability for MLLMs as it is a fundamental aspect of problem-solving and task completion: The ability to reason enables MLLMs to understand complex relationships between different modalities, draw logical conclusions; and make informed decisions based the information available. on\n\nThe assessment of reasoning abilities in MLLMs is crucial aspect of evaluating their overall performance and guiding the development of more advanced models. For instance; MLLMs can be tasked with reasoning about the minimum number of operations required to transform one into another by analyzing the visual representation of the two and identifying the differences between them. Similarly; given a map; MLLMs can be tasked about reasoning on the optimal route between two locations on map by analyzing the visual representation of the map and identifying the shortest based on the available information: The ability to reason enables MLLMs to draw logical conclusions and make informed decisions based on the information available. In this paper, we introduce a benchmark that focuses solely on evaluating the reasoning abilities of various MLLMs. By assessing the extent of their reasoning capabilities; we aim to provide valuable insights into the strengths and limitations of current MLLMs and guide future research towards developing more various factors such as image recognition and instruction following, from the overall performance of the models; allowing us to focus solely on evaluating their reasoning abilities. string strings path\n\nNumerous benchmarks have been developed to assess the capabilities of MLLMs in various domains; such as visual question answering [5, 6, 7, 8], Optical Character Recognition [9], robustness [10], hallucination [11], and holistic overall performance [12, 13, 14, 15]. However; despite the breadth of these evaluations; none of them specifically focus on assessing the pure reasoning abilities of MLLMs; leaving significant gap in our understanding of their reasoning capabilities. Furthermore; many of these benchmarks are static in nature, making them prone to overfitting and limiting their effectiveness in providing a comprehensive view of MLLMs abilities [16]. To address these limitations, there is a need for a dynamic benchmark that specifically targets the evaluation of MLLMs' reasoning abilities and updates regularly to prevent overfitting.\n\nIn response to these limitations, we introduce NPHardEval4V , a dynamic benchmark specifically designed to quantitatively and rigorously evaluate the reasoning ability of MLLMs across a diverse\n\nset of tasks. This benchmark aims to provide a rigorous framework for assessing MLLMs' reasoning, leveraging the computational complexity hierarchy to explore the depths of models' reasoning abilities. The dynamic updating mechanism also makes the benchmark robust in a way that static benchmarks cannot. Moreover; the dynamic nature of NPHardEval4V with datasets that evolve over time mitigates the risk of overfitting; ensuring that assessments remain relevant and challenging. This is crucial for fostering models that are genuinely capable of learning and adapting, rather than merely optimizing for static benchmarks.\n\nThe proposed benchmark is developed based on the NPHardEval benchmark, as presented in [17]. The NPHardEval benchmark comprises nine types of algorithmic problems; categorized into three mic problem consists of 100 instances with varying difficulty levels: To enable a direct comparison between MLLMs and LLMs, we have retained the problems from NPHardEval and converted their textual descriptions into visual representations.\n\nOur research questions are as follow:\n\n- 1 . Reasoning Performance Evaluation of MLLMs: This study seeks to understand the variation in reasoning abilities among different MLLMs and to identify the factors that contribute to any observed performance gaps. By analyzing and separating the influence of recognition and instruction following; we evaluate the core reasoning abilities of MLLMs vary with the nature of complexity and difficulty of problems. This investigation will provide insights into the relative strengths and weaknesses of MLLMs in reasoning tasks. We provide the model details of the MLLMs we evaluate in Table 2-\n- 2 Effects of Vision Input on MLLMs' Performance: We investigate how the inclusion of vision prompts affects MLLMs reasoning abilities compared to pure text inputs. Through ablation studies of three prompt types, including 'Figures   with Limited Instructional  Text Setup' performance and examine how the combination of vision and text inputs can affect MLLMs performance.\n\n## 2 Related Work\n\n## 2.1 Multimodal Large Language Models (MLLMs) and Their Reasoning Abilities\n\nMLLMs [4] can process and interpret various multimodal data streams; including imagery and textual content [18]. As such, MLLMs are able to surpass singularly moded LLMs and unlock new avenues for performing real-world applications: summarized by Yin et al. (2023) [3], MLLMs are more intelligent, user-centric, and holistic as compared to their LLM counterparts\\_ MLLMs can mimic the way of how humans perceive the environment by assimilating multi-sensory inputs that can complement each other. can also foster intuitive user interactions and communications. The scope of tasks that MLLMs can assist with is significantly broader compared to LLMs, reinforcing their versatility in applications [3] such as engineering [19] and healthcare [20]. As They\n\nReasoning is one of the fundamental intelligent behaviors; essential for solving complex real-world tasks [21, 22]. However; even in text-centric settings; LLMs lack proper reasoning abilities; such as dealing with NP-hard (nondeterministic polynomial time) mathematical problems [17]. The quest to explore and improve reasoning capabilities of Strong AI particularly within MLLMs remains a persistent challenge and a pursuit of ongoing research [23, 24, 25].\n\nIn the domain of MLLMs, researchers have explored a range of techniques, such as instruction tuning and prompt engineering, to enhance multimodal reasoning [4]. The practice of instruction tuning, vital for in-context learning (ICL), has emerged as one of the techniques in enhancing the reasoning abilities of these models. For example; frozen LLM is an initial MLLM to showcase ICL ability [26].  Later; the Flamingo model demonstrated strong ICL capabilities with a more sophisticated LLM coupled with massive-scale image and text data for its pre-training phase [27]. Prompt engineering, on the other hand; has illustrated its effectiveness in improving the reasoning abilities of MLLMs [28, 29]. As summarized by Wang et al.  (2024) [4], this approach entails a variety of strategic implementations; such as representation learning [28], exemplar generation [29], and model interactions [30]. key\n\n## 2.2 Benchmarks of Multimodal Large Language Models (MLLMs)\n\nAs the reasoning ability of MLLMs continues to advance; benchmarks have become instrumental in evaluating their performance and identifying areas that require improvement: et al. (2024) sug gest that a robust multimodal reasoning benchmark must fulfill three criteria: (1) the integration of multimodal information; (2) the categorization of reasoning, and (3) in-depth annotations of the reasoning steps [4]. Wang key\n\nPrevious research has established numerous benchmarks to gauge the performance of MLLMs [31, (2023) introduced an extensive MLLM evaluation benchmark to assess models' perception and cognition skills across 14 distinct subtasks; including commonsense reasoning and code reasoning [31]. Later; Li et (2023) presented a benchmark called SEED-Bench, comprising 19,000 multiple-choice questions with precise human annotations. SEED Bench evaluates MLLMs across 12 dimensions; capturing the ability to understand image and video modalities [13]. In a more recent study; et al. (2024) examined the self-consistency of MLLM responses in the presence of common corruptions: They created MMCBench, an extensive benchmark encompassing over 100 prominent LLMs. This benchmark assesses cross-modal interactions among text, images, and speech, incorporating generative tasks:   text-to-image, image-to-text, text-to-speech; and speech-to-text [34]. al. Zhang four key\n\nAlthough existing benchmarks evaluated the abilities of MLLMs across various dimensions, do not provide a clear picture of the pure reasoning ability of MLLMs because factors such as recognition; knowledge amount; instruction following; and others are all combined in the presented performances in these benchmarks. Given that MLLMs' general performance are intricately dependent on their recognition process and instruction-following ability; our study aims to factor out other factors in order to see the pure reasoning process and assess them of MLLMs. Moreover; these benchmarks lack dynamic updating mechanisms; which increases the risk of MLLMs becoming overfitted to these benchmarks and restricts their ability to accurately reflect the full range of MLLMs capabilities [16]. To address this research gap; our study proposes a dynamic assessment framework for evaluating the reasoning performance of MLLMs. In addition, we utilize the computational complexity hierarchy to rigorously assess the extent to which these MLLMs can achieve in reasoning tasks [17]. have they\n\n## 3 Benchmark Construction\n\nAs we mentioned above, NPHardEval4V is built upon NPHardEval [17], using the same set of problems while transformed the input type from textual to visual. This section outlines the transformation of the NPHardEval benchmark to suit the evaluation of MLLMs within NPHardEval4 V, emphasizing the multimodal aspects of the tasks and the dynamic nature of the challenges.\n\n## 3.1 NPHardEval Benchmark and Our Transformation\n\nWe explore and build upon the NPHardEval Benchmark [17], a framework that segments task complexity into three primary computational complexity classes: P (polynomial time), NP-complete (nondeterministic polynomial-time complete), and NP-hard. These classifications serve to delineate the intrinsic difficulty and the computational resources required for solving tasks within each class; showcasing an increasing order of complexity. NPHardEval4V also adopts the same three compuproblems into visual representations '\n\nTo provide a comprehensive overview of task complexity and difficulty within these classes; we introduce a hierarchical categorization illustrated in Table 1 where tasks are further divided into 10 progressive difficulty levels. This granular difficulty grading enables nuanced assessment of model performance across a spectrum of computational challenges; thereby offering valuable insights into the capabilities and limitations of current models' reasoning abilities.\n\nTo enable the direct comparison with the text-only prompt input in the NPHardEval benchmark; we transform the text-only questions and represent the data part with figures.  We further illustrate these transformations; with the data of the problem presented both textually and visually.\n\n'Robustness experiment on the benchmark is performed in [17].\n\nTable I: Complexity classes and tasks\n\n| Complexity Class       | Task                                                                                                                                     |\n|------------------------|------------------------------------------------------------------------------------------------------------------------------------------|\n| NP Hard (Most Complex) | Graph Coloring Problem Optimization Version (GCP) Traveling Salesman Problem Optimization Version (TSP) Meeting Scheduling Problem (MSP) |\n| NP Complete            | Knapsack Problem (KSP) Traveling Salesman Problem Decision Version (TSP-D) Graph Coloring Problem Decision Version (GCP-D)               |\n| P (Least Complex)      | Shortest Path Problem (SPP) Edit Distance Problem (EDP) Sorted Array Search (SAS)                                                        |\n\nGraph Data Transformation The general construction of graph data problems involves providing MLLMs with both textual description and a visual representation. For example, in the Graph Coloring Problem (see Figure 2) MLLMs are given a textual prompt alongside a figure depicting a graph, challenging them to reason across modalities. The figure is generated using Python based on the textual description of the coloring problem (GCP): it includes information about the nodes and the connections between them. graph\n\n```\nGraph coloring refers to the problem of coloring vertices of graph in such\n[... content truncated ...]"
      ],
      "pred_answer": "The common objective is **infilling** \u2013 an autoregressive \u201cfill\u2011in\u2011the\u2011middle\u201d task where the model predicts missing text (or other content) based on its surrounding context.",
      "ground_truth": [
        [
          "context infilling"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    }
  ]
}