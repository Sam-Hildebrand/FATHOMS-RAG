{
  "model": "gpt-oss:20b_ LlamaIndex RAG",
  "api_model": "gpt-oss:20b",
  "scores": {
    "Text-Only": 0.631720430107527,
    "Tables": 0.23076923076923078,
    "Images": 0.14285714285714285,
    "Multimodal": 0.2777777777777778,
    "Cross-Document Multimodal": 0.225,
    "Text-Only Hallucination Rate": 0.22580645161290322,
    "Tables Hallucination Rate": 0.46153846153846156,
    "Images Hallucination Rate": 0.5714285714285714,
    "Multimodal Hallucination Rate": 0.48,
    "Cross-Document Multimodal Hallucination Rate": 0.7
  },
  "overall_score": 0.3016249163023357,
  "total_hallucination_rate": 0.4877546969159873,
  "responses": [
    {
      "category": "Text-Only",
      "question": "What type of neural network does the HyperTransformer model generate? Do not use acronyms.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 19,
          "relevance_score": 0.7419531272984543,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 1\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 2\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 3\n0.025\n0.050\nFigure 8. Learned attention maps for 2-layer 4-channel CNN network generated with 1 head, 1 layer transformer for 5-shot MINIIMA-\nGENET.\nTransformer Layer 1\nTransformer Layer 2\nWeights\nLabel 0\nLabel 1\nLabel ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.730424219633416,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7285477764961411,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        }
      ],
      "pred_answer": "it generates convolutional neural networks.",
      "ground_truth": [
        [
          "convolution"
        ],
        [
          "cnn"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What modality other than text does Multimodal-CoT incorporate?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7476094627812999,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nOutput\nRationale: Will these magnets attract or repel? To find out, look at which poles are\nclosest to each other. The north pole of one magnet is closest to the south pole of the\nother magnet. Poles that are different attract. So, these magnets will attract each other.\nAnswer: The answer is (A).\nVision\nLanguage\nInput\nQuestion: Will these magnets attract or repel each\nother?\nContext: Two magnets are placed as shown. Hint:\nMagnets t..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 12,
          "relevance_score": 0.6574353346291462,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nTable 10: Result comparison with different alignment strategies for multimodal interaction.\nModel\nAccuracy\nDirect Answering\n82.62\nUnimodal encoder\n85.31\nImage-grounded text encoder\n84.60\n6.6\nGeneralization to Other Multimodal Reasoning Benchmarks\nWe are interested in evaluating the generalization capability of Multimodal-CoT to datasets outside its\ntraining domain. For this purpose, we utilize the widely-recognized multimodal reaso..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6482962442996708,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        }
      ],
      "pred_answer": "vision (image) data.",
      "ground_truth": [
        [
          "vision"
        ],
        [
          "image"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one question and answer benchmark that the Multimodal-CoT paper mentions in its abstract.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.7388527665026804,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7315586655646096,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 17,
          "relevance_score": 0.7212655059624421,
          "content": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. Proceedings of\nthe 40th International Conference on Machine Learning, PMLR, pp. 39755\u201339769, 2022.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual\nquestion answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA,..."
        }
      ],
      "pred_answer": "mmmu.",
      "ground_truth": [
        [
          "scienceqa"
        ],
        [
          "a okvqa"
        ],
        [
          "aokvqa"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In the Multimodal-CoT paper, what are the two stages of the proposed framework? Answer as a comma separated list.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6792781253768915,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6769671156705755,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6663782496225272,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        }
      ],
      "pred_answer": "i don\u2019t have that information.",
      "ground_truth": [
        [
          "rationale generation",
          "answer inference"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which datasets are used to evaluate HyperTransformer? Answer as a comma separated list.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.7150066731605126,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7011066177015296,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6990107807641305,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        }
      ],
      "pred_answer": "tieredimagenet",
      "ground_truth": [
        [
          "omniglot, miniimagenet, tieredimagenet"
        ],
        [
          "omniglot, mini imagenet, tiered imagenet"
        ],
        [
          "omniglot, mini image net, tiered image net"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What optimization-based algorithm mentioned in the HyperTransformer paper can fine-tune the embedding by performing additional SGD updates on all parameters of the model producing it?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 2,
          "relevance_score": 0.7136465965688713,
          "content": "This family of methods includes, for\nexample, such methods as Siamese networks (Koch et al.,\n2015), Matching Networks (Vinyals et al., 2016), Proto-\ntypical Networks (Snell et al., 2017), Relation Networks\n(Sung et al., 2018) and TADAM (Oreshkin et al., 2018).\nIt has recently been argued (Tian et al., 2020) that meth-\nods based on building a powerful sample representation\ncan frequently outperform numerous other approaches in-\ncluding many optimization-based methods. However, such\napproaches ess..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6911545327639604,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6858792223964905,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        }
      ],
      "pred_answer": "the algorithm is **model\u2011agnostic meta\u2011learning (maml)**.",
      "ground_truth": [
        [
          "maml"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What function (and input to the function) is used to generate H_language in Multimodal_CoT? Omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6794757811340596,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6566545344495281,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 14,
          "relevance_score": 0.6394692213122383,
          "content": "Dynamic fusion with intra- and inter-modality attention flow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pp. 6639\u20136648. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00680.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. ArXiv preprint, abs/2206.06336, 2022.\nKaiming He, Xiangyu Zhang, ..."
        }
      ],
      "pred_answer": "h_language is produced by applying the language\u2011specific transformation\u202ff_lang\u202fto the language representation\u202fl, i.e.\u202fh_language\u202f=\u202ff_lang(l).",
      "ground_truth": [
        [
          "languageencoder",
          "x",
          "language"
        ],
        [
          "language encoder",
          "x",
          "language"
        ],
        [
          "language",
          "encoder",
          "x"
        ]
      ],
      "correctness_score": 0.3333333333333333,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What does Multimodal-CoT claim to be the main benefit of separating rationale generation from answer inference?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.7838094372816017,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7685112266957838,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 4,
          "relevance_score": 0.7398870226582446,
          "content": "Table 2: Effects of CoT in the one-stage setting.\nMethod\nFormat\nAccuracy\nNo-CoT\nQCM\u2192A\n81.63\nReasoning\nQCM\u2192RA\n69.32\nExplanation\nQCM\u2192AR\n69.68\nAs an example shown in Figure 1, the model takes\nthe concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as\nthe input. To study the effect of CoT, we compare\nthe performance with three variants: (i) No-CoT which\npredicts the answer directly (QCM\u2192A); (ii) Reasoning\nwhere answer inference is conditioned to the ratio..."
        }
      ],
      "pred_answer": "multimodal\u2011cot argues that decoupling rationale generation from the final answer step mainly helps the model avoid hallucinations and converge more reliably, which in turn yields higher\u2011quality, more accurate predictions.",
      "ground_truth": [
        [
          "hallucinat"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "NPHardEval4V says reasoning abilities increase as problem size increases. Answer \"true\" or \"false\"",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.732449250431618,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7275086233614273,
          "content": "2.2\nBenchmarks of Multimodal Large Language Models (MLLMs)\nAs the reasoning ability of MLLMs continues to advance, benchmarks have become instrumental in\nevaluating their performance and identifying areas that require improvement. Wang et al. (2024) sug-\ngest that a robust multimodal reasoning benchmark must fulfill three key criteria: (1) the integration\nof multimodal information, (2) the categorization of reasoning, and (3) in-depth annotations of the\nreasoning steps [4].\nPrevious research has..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7206326718274495,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "There is a typo in the version of The Path To Autonomous Cyber Defense that you have. What is 'cyber' misspelled as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6752293027023976,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6512204366325942,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6164719363277363,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        }
      ],
      "pred_answer": "the word \u201ccyber\u201d is misspelled as **\u201cciber.\u201d**",
      "ground_truth": [
        [
          "cuber"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the Oak Ridge Center for AI Security Research abbreviated as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7522109436252645,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7255197448575132,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 34,
          "relevance_score": 0.6219144630171698,
          "content": "Published as a conference paper at ICLR 2025\nFigure 32: Examples of HR charts.\n34"
        }
      ],
      "pred_answer": "caiser",
      "ground_truth": [
        [
          "caiser"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does DocLLM model that traditional LLMs don't?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7487414314780718,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7060669800030915,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6908543255595933,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        }
      ],
      "pred_answer": "docllm explicitly models the spatial layout of documents\u2014capturing disentangled spatial features and using a block\u2011infilling pre\u2011training objective\u2014something that conventional language models, which treat text as a linear sequence, do not do.",
      "ground_truth": [
        [
          "spatial layouts",
          "text semantics"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Which pre-training objective does DocLLM use instead of standard next-token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7379496322229407,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6961907231170723,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 5,
          "relevance_score": 0.6958583276814789,
          "content": "Furthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than\nonly preceding tokens, can be beneficial. The infilling objectives enable contextually relevant completions, provide\nrobustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.\nHence we modify the standard pre-training objective to predict blocks of text given preceding and following text blocks.\nMost OCR engines can provide..."
        }
      ],
      "pred_answer": "docllm replaces the usual next\u2011token prediction with an **autoregressive block infilling** objective, where it predicts masked text blocks conditioned on both the preceding and following blocks.",
      "ground_truth": [
        [
          "text infilling"
        ],
        [
          "block infilling"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What type of problems does the NPHardEval4V benchmark aim to test in MLLMs?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7277517044945809,
          "content": "2.2\nBenchmarks of Multimodal Large Language Models (MLLMs)\nAs the reasoning ability of MLLMs continues to advance, benchmarks have become instrumental in\nevaluating their performance and identifying areas that require improvement. Wang et al. (2024) sug-\ngest that a robust multimodal reasoning benchmark must fulfill three key criteria: (1) the integration\nof multimodal information, (2) the categorization of reasoning, and (3) in-depth annotations of the\nreasoning steps [4].\nPrevious research has..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7216200647752443,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.7212416017097468,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        }
      ],
      "pred_answer": "nphardeval4v is designed to evaluate mllms on **reasoning tasks that span the classic computational\u2011complexity spectrum**\u2014specifically problems that fall into the **p, np\u2011complete, and np\u2011hard** classes. the benchmark presents these problems in a visual format (figures instead of text) so that the models\u2019 pure reasoning ability can be assessed independently of recognition or instruction\u2011following skills.",
      "ground_truth": [
        [
          "reasoning"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name the two NP computational complexity classes used in NPHardEval4V.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7464335593924729,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 15,
          "relevance_score": 0.7285338961250334,
          "content": "[17] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. Nphardeval:\nDynamic benchmark on reasoning ability of large language models via complexity classes.\narXiv preprint arXiv:2312.14890, 2023.\n[18] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. Multimodal large\nlanguage models: A survey. In 2023 IEEE International Conference on Big Data (BigData),\npages 2247\u20132256. IEEE, 2023.\n[19] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jint..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.7086237264375325,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        }
      ],
      "pred_answer": "the benchmark employs the two np\u2011based classes: **np\u2011complete** and **np\u2011hard**.",
      "ground_truth": [
        [
          "np hard",
          "np complete"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What are attackers called in the strategic game modeled in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7441370368814224,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7114152065926035,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6570732967261991,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        }
      ],
      "pred_answer": "in the strategic game described in the paper, the attackers are referred to as **players**.",
      "ground_truth": [
        [
          "red team"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Who does the blue team represent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.704949032457147,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6947066823698324,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.667631302532137,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        }
      ],
      "pred_answer": "the blue team represents the defenders \u2013 the organization\u2019s security operations center (soc) that is tasked with protecting the network.",
      "ground_truth": [
        [
          "defens"
        ],
        [
          "defender"
        ],
        [
          "defen"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does NPHardEval4V use in place of static benchmarks?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6758999610485175,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.675301808825663,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.668870476788268,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "nphardeval4v replaces static benchmarks with a **dynamic reasoning benchmark** for multimodal large language models.",
      "ground_truth": [
        [
          "dynamic",
          "benchmark"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the attention mechanism in DocLLM that allows for selective focus called?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6603098848955539,
          "content": "Figure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically usi..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6546677846612283,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 47,
          "relevance_score": 0.6469459370646718,
          "content": "Published as a conference paper at ICLR 2025\nFigure 44: An error case of SelfReflection Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n47"
        }
      ],
      "pred_answer": "the selective\u2011focus attention used in docllm is called the **disentangled spatial attention mechanism**.",
      "ground_truth": [
        [
          "disentangle",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-source LLM was adapted to build DocLLM-7B?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6909151178076519,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 12,
          "relevance_score": 0.6840691019035336,
          "content": "Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Si..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6682820118659288,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        }
      ],
      "pred_answer": "docllm\u20117b was built on top of the open\u2011source llama\u202f2 model.",
      "ground_truth": [
        [
          "llama2",
          "7b"
        ],
        [
          "llama 2",
          "7 b"
        ],
        [
          "llama 2",
          "7b"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What medium does the ChartMimic benchmark evaluate a model's ability to turn to code?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.8004534992256572,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.7722958130970448,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7261134194686671,
          "content": "Published as a conference paper at ICLR 2025\n5\nRELATED WORK\nLarge Multimodal Models.\nThe proprietary LMMs such as GPT-4o (OpenAI, 2024), Gem-\nini (Google, 2023), and Claude-3 (Anthropic, 2024) have enabled complex multimodal interac-\ntions. Similarly, emerging open-weight LMMs such as LLaVA (Xu et al., 2024; Li et al., 2024a),\nInternVL (Chen et al., 2023), Qwen-VL (Bai et al., 2023), DeepSeek-VL (Lu et al., 2024) have\ncontributed to the community. Despite these advancements, the effective evalua..."
        }
      ],
      "pred_answer": "chartmimic evaluates a model\u2019s ability to convert **visual charts** into code.",
      "ground_truth": [
        [
          "chart"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many total (figure, instruction, code) triplets were annotated for ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6981641740649852,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.6917231503115083,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 4,
          "relevance_score": 0.6911449113707079,
          "content": "Published as a conference paper at ICLR 2025\nTable 1: Statistics of ChartMimic. We measure code length in terms of tokens, utilizing the Llama3\ntokenizer. In the level count, \u201cA/B/C\u201d denotes the number of \u201ceasy/medium/hard\u201d level, respectively.\nType\nBar\nLine\nErrorBar\nHeatmap\nBox\nScatters\nHist\nRadar\n3D\nPie\nErrorPoint\nViolin\nCount\n320\n280\n120\n120\n100\n100\n80\n80\n80\n80\n80\n80\nSubcategories\n16\n8\n12\n4\n6\n4\n3\n6\n5\n8\n5\n3\nCode Length (AVG.)\n689.6\n794.0\n681.2\n685.8\n689.0\n655.0\n529.6\n779.8\n655.4\n418.4\n624.3\n97..."
        }
      ],
      "pred_answer": "the benchmark was built from 1,200 manually annotated (figure, instruction, code) triplets.",
      "ground_truth": [
        [
          "4,800"
        ],
        [
          "4800"
        ],
        [
          "four thousand, eight hundred"
        ],
        [
          "four thousand eight hundred"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What are the two tasks defined in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7336765125644148,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.7082085401154046,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 2,
          "relevance_score": 0.6814017147656098,
          "content": "Published as a conference paper at ICLR 2025\nFigure 1: The real-world example. LMMs assist scientists and researchers in understanding, inter-\npreting and creating charts during the reading and writing of academic papers. These models serve as\nassistants that enhance the comprehension and presentation of data in scholarly communications.\nTo this end, we present ChartMimic (Fig. 2), a multimodal code generation benchmark. ChartMimic\nis characterized by its (1) information-intensive visual inputs,..."
        }
      ],
      "pred_answer": "the benchmark defines two tasks:\n\n1. **direct mimic** \u2013 generate code that reproduces the given chart.  \n2. **customized mimic** \u2013 generate code that incorporates new data or changes specified in an instruction.",
      "ground_truth": [
        [
          "direct",
          "custom"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-weights model achieved the highest GPT-4o score on the Direct Mimic and Customized Mimic tasks?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 36,
          "relevance_score": 0.8264304683695549,
          "content": "Published as a conference paper at ICLR 2025\nFigure 34: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (bar_28).\nStability of Evaluation with GPT-4o Score. We conduct the high-level evaluation for GPT-4o on\nthe Direct Mimic task for 5 times to assess the stability of GPT-4o Score. The result indicate a mean\nGPT-4o Score of 83.4 with a standard deviation of 0.08, demonstrating the stability of GPT-4o Score.\nCost of Evaluation with GPT-4o Score. A single-round evaluation with GPT-4..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 6,
          "relevance_score": 0.8171194332751512,
          "content": "Recently, MMCode (Li et al., 2024b) attempted to create a benchmark for\nmultimodal code generation, but the vision inputs for their task are still overly simple and only have a\nsingle pass rate evaluation metric. Design2Code (Si et al., 2024) and Plot2Code (Wu et al., 2024a)\nare the most similar ones to ours. Although they use multi-level evaluation metrics like ours, their\ntest is crawled directly from the internet or existing datasets, which may pose a risk of data leakage.\nOur ChartMimic benc..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.7875147839999173,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        }
      ],
      "pred_answer": "the top\u2011performing open\u2011weights model on both tasks was **internvl2\u2011llama3\u201176b**.",
      "ground_truth": [
        [
          "internvl2",
          "llama3",
          "76"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one of the four low-level evaluation metrics used in ChartMimic.",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7771840312589479,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7538327651934523,
          "content": "They are then required to modify the corresponding code and instructions, leading to the\n600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently, we establish the\nChartMimic benchmark, comprising 1, 200 high-quality seed data.\nData Augmentation. Following the development of seed triplets, we initiate a process of manual data\naugmentation. Annotators are tasked with altering various elements of each seed triplet, including\ndata, color schemes, mark styles, etc., to pr..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.7487110036277125,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        }
      ],
      "pred_answer": "one of the low\u2011level evaluation metrics is **text**.",
      "ground_truth": [
        [
          "text"
        ],
        [
          "layout"
        ],
        [
          "type"
        ],
        [
          "color"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What maximum accuracy did state-of-the-art models achieve on normal ENIGMAEVAL puzzles? Answer as a single value.",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.736779830362694,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 5,
          "relevance_score": 0.7194737435504415,
          "content": "intervention. The standardization addressed several key challenges: (a) removing source-identifying headers and\nfooters to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where\nstructural elements like lines and dots are separate from textual content), and (c) ensuring accurate text extraction\nfrom non-standard formatting. A similar curation process was applied to puzzle solutions: after initial automated\nextraction from PDF and HTML pages, each sol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 15,
          "relevance_score": 0.7174667730347097,
          "content": "B.4\nAnswer Extraction\nWe determine the expected answer format through manual annotations, with three possible patterns:\n1. Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer>\n2. Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2>\n3. Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...\nThis structured approach to answer formats allows us to extract answers consistently a..."
        }
      ],
      "pred_answer": "7.0%",
      "ground_truth": [
        [
          "7.0%"
        ],
        [
          "7%"
        ],
        [
          "7.0 %"
        ],
        [
          "7 %"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many puzzles are in the ENIGMAEVAL normal split?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.713612456759779,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 15,
          "relevance_score": 0.7113666459909379,
          "content": "B.4\nAnswer Extraction\nWe determine the expected answer format through manual annotations, with three possible patterns:\n1. Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer>\n2. Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2>\n3. Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...\nThis structured approach to answer formats allows us to extract answers consistently a..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.6978338948749748,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        }
      ],
      "pred_answer": "the provided context does not contain the number of puzzles in the enigmaeval normal split.",
      "ground_truth": [
        [
          "949"
        ],
        [
          "nine hundred forty nine"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does ENIGMAEVAL call a puzzle that combines answers or elements from multiple previous puzzles to reveal a final solution?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7379104959811863,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.73509160291395,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.7153848097332415,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        }
      ],
      "pred_answer": "enigmaeval refers to such a puzzle as a **metapuzzle**.",
      "ground_truth": [
        [
          "meta puzzle"
        ],
        [
          "metapuzzle"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does PCFM stand for?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6315290144758492,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6255160082802426,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 6,
          "relevance_score": 0.6212768866677881,
          "content": "3.4.1\nTradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with suffici..."
        }
      ],
      "pred_answer": "pcfm stands for **physics\u2011constrained flow model**.",
      "ground_truth": [
        [
          "physics",
          "constrained",
          "flow",
          "matching"
        ]
      ],
      "correctness_score": 0.75,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Which methods propose gradient-based constraint enforcement during sampling, but often require backpropagation through expensive PDE operators?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2506.04171v1.pdf",
          "page_number": 3,
          "relevance_score": 0.764394812698855,
          "content": "Table 1: Comparison of generation methods motivated by constraint guidance or enforcement.\nZero-shot\nContinuous Guidance\nHard Constraint\nComplex Constraints\nConditional FFM [11]\n\u2717\n\u2713\n\u2713\n\u2717\nDiffusionPDE [10]\n\u2713\n\u2717\n\u2717\n\u2713\nD-Flow [35]\n\u2713\n\u2713\n\u2717\n\u2713\nECI [12]\n\u2713\n\u2713\n\u2713\n\u2717\nPCFM (Ours)\n\u2713\n\u2713\n\u2713\n\u2713\n4. We evaluate our method on challenging PDEs exhibiting shocks, discontinuities, and sharp\nspikes\u2014settings in which standard flow matching models typically fail. Our approach\nimproves the accuracy of such models at inference time ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2506.04171v1.pdf",
          "page_number": 4,
          "relevance_score": 0.7459339427625014,
          "content": "The vector field v\u03b8 is parameterized by a Neural\nOperator [23, 24] which is forward compatible to operate on functional spaces. The resulting\ngenerative model approximates the target measure \u03c01 \u2248\u03bd over the support UF through sufficient\ntraining (see Appendix G.2 for further details). In this work, we focus on augmenting such pre-trained\nmodels to enforce additional physical constraints at inference time, without requiring retraining.\n3.2.1\nConstraint Types in PDE Systems\nConstraints in PDE-gover..."
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 7,
          "relevance_score": 0.7377992636125158,
          "content": "D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of ..."
        }
      ],
      "pred_answer": "the approaches that use a gradient\u2011based enforcement of constraints during sampling are **diffusionpde** (reference\u202f[10]) and **d\u2011flow** (reference\u202f[35]). both methods rely on back\u2011propagating through the underlying pde solver (or ode integrator), which can become prohibitively expensive for functional data such as pde solutions.",
      "ground_truth": [
        [
          "diffusionpde, d flow"
        ],
        [
          "diffusion pde, d flow"
        ],
        [
          "diffusionpde, dflow"
        ],
        [
          "diffusion pde, dflow"
        ],
        [
          "diffusionpde, d-flow"
        ],
        [
          "diffusion pde, d-flow"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In ENIGMAEVAL, how do the authors justify excluding audio/video puzzles?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7062937603740322,
          "content": "Table 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty split. N is the number of puzzles.\nSplit (N)\nPuzzle Source (N)\nDescription\nNormal\n(949)\nPuzzledPint (838) [1]\nMonthly beginner-friendly puzzle-solving event typically consisting of\naround seven puzzles including a meta-puzzle. Together, these puzzles\nshould \u201cbe solvable in under two hours by a team of inexperienced puz-\nzlers who are socializing, drinking, and eating in a pub or restaurant\u201d1.\nCS50x Puzzle Day (41)\n[2]\nAnn..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6958200464162476,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6746035535654956,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        }
      ],
      "pred_answer": "the authors exclude puzzles that rely on audio, video, or interactive web elements because, at the time of writing, only a handful of frontier models are capable of effectively processing those modalities. consequently, puzzles that depend on sound, video, or web\u2011application interaction would unfairly penalize models for a limitation that is outside the scope of the benchmark\u2019s focus on multimodal reasoning.",
      "ground_truth": [
        [
          "parsing limitations"
        ],
        [
          "scope"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In Table 11 of the Multimodal-CoT Paper, what size of Multimodal-CoT was used?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6512941121427619,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6455530292766115,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 24,
          "relevance_score": 0.6417120311877401,
          "content": "Published as a conference paper at ICLR 2025\nFigure 10: An Example of the Customized Mimic task (CB_29), showing the (figure, instruction,\ncode) triplet. Additionally, we also display the ground truth figure rendered from the code for\nillustration.\n24"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "738M"
        ],
        [
          "738"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "What template does DocLLM use for CLS Internal Classification?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.7070829237739452,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6680855185974732,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.653047059380375,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "docllm employs the **internal\u2011classification instruction template** (one of the three templates listed in table\u202f1).",
      "ground_truth": [
        [
          "{document} What type of document is this?"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "In DocLLM, how many total documents were used in pre-training?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.697322679097778,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 7,
          "relevance_score": 0.6749640352157906,
          "content": "Table 2: Pre-training dataset statistics.\nNo. of Docs\nNo. of Pages\nNo. of Total Tokens\nCDIP\n5,092,636\n16,293,353\n3,637,551,478\nDocBank\n499,609\n499,609\n228,362,274\nTotal\n5,592,245\n16,792,962\n3,865,913,752\nTable 3: Instruction-tuning dataset statistics.\nTasks\nNo. of Training\nNo. of Testing\nVQA\n145,090\n24,347\nNLI\n104,360\n12,720\nKIE\n236,806\n38,039\nCLS\n149,627\n21,813\nTotal\n635,883\n96,919\nextraction template, we add the \u201cNone\u201d answer if the key does not exist in the given document. To increase diversi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6571049972639218,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "docllm\u2019s pre\u2011training phase used a total of **5,592,245 documents**.",
      "ground_truth": [
        [
          "5,592,245"
        ],
        [
          "5592245"
        ],
        [
          "five million five hundred ninety two thousand two hundred forty five"
        ],
        [
          "five million, five hundred ninety two thousand, and forty five"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, what is the Testmini test-set execution rate of InternVL2-26B on the Direct Mimic task?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.8063732701265287,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.723961477173818,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.6863052812678508,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        }
      ],
      "pred_answer": "the provided excerpt does not contain the testmini execution rate for internvl2\u201126b on the direct mimic task.",
      "ground_truth": [
        [
          "69.3"
        ],
        [
          "sixty nine point three"
        ],
        [
          "sixty-nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggests 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot MiniImageNet with 8 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.7434720029251222,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.7387828229855788,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 14,
          "relevance_score": 0.6888246499426696,
          "content": "Transformer parameters.\nSince the weight tensors of each layer are generally different, our per-layer transformers were\nalso different. The key, query and value dimensions of the transformer were chosen to be equal to a pre-de\ufb01ned fraction\n\u03bd of the input embedding size, which in turn was a function of the label, image and activation embedding sizes and the\nsizes of the weight slices. The inner dimension of the \ufb01nal fully-connected layer in the transformer was also chosen\nusing the same approach...."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggest 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot OMNIGLOT with 64 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.7347848592660252,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.7304969195845664,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.6732724868220402,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nD. Additional Supervised Experiments\nWhile the advantage of decoupling parameters of the weight generator and the generated CNN model is expected to vanish\nwith the growing CNN model size, we compared our approach to two other methods, LGM-Net (Li et al., 2019b) and LEO\n(Rusu et al., 2019), to verify that our approach can match their performance on suf\ufb01ciently large models.\nFor our comparison with the LGM-Net method, we used the same image..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the PCFM paper, what is the MMSE/10^-2 for D-Flow?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2506.04171v1.pdf",
          "page_number": 27,
          "relevance_score": 0.7268814846583977,
          "content": "10\n4\n10\n3\n10\n2\n10\n1\n100\nPenalty weight \n0.005\n0.006\n0.007\n0.008\n0.009\n0.010\n0.011\n0.012\nError\nMMSE and SMSE vs \nMMSE\nSMSE\n10\n4\n10\n3\n10\n2\nPenalty weight \n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nError\nMMSE and SMSE vs \nMMSE\nSMSE\nFigure 9: Effect of penalty weight \u03bb on MMSE and SMSE for the Reaction-Diffusion dataset. Left:\n10 flow matching steps. Right: 100 flow matching steps.\nnumber of steps\u2014for instance, in scenarios where evaluating the vector field v\u03b8 is computationally\nexpensive.\nHowever, ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2506.04171v1.pdf",
          "page_number": 7,
          "relevance_score": 0.6794449585051984,
          "content": "D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 6,
          "relevance_score": 0.6486462781742366,
          "content": "3.4.1\nTradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with suffici..."
        }
      ],
      "pred_answer": "the pcfm paper does not report an mmse value for d\u2011flow \u2013 the results for that method are omitted due to numerical instabilities.",
      "ground_truth": [
        [
          "1.97"
        ],
        [
          "one point ninety seven"
        ],
        [
          "one point nine seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, which chart type has the largest average code token length?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7158328449870294,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.6938095061481103,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 20,
          "relevance_score": 0.6905889926662062,
          "content": "Published as a conference paper at ICLR 2025\nbased on both its frequency and sophistication level (designated as 1-3 points for low/medium/high\ncomplexity, respectively). This multi-dimensional scoring approach ensures a thorough evaluation of\nthe visual complexity inherent in each chart.\nImplementation Complexity We incorporate code complexity as a quantitative metric, measured\nprimarily through code length and structural intricacy. This parameter effectively captures the\ntechnical sophisticati..."
        }
      ],
      "pred_answer": "the provided excerpts do not contain any information about the average code token length for the different chart types in chartmimic.",
      "ground_truth": [
        [
          "plot in plot"
        ],
        [
          "pip"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "What accuracy did Multimodal-CoT get on the MMMU benchmark?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6894435880763968,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6882783744691446,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 9,
          "relevance_score": 0.6618091723285339,
          "content": "82\n70.98\n83.18\n77.37\n67.92\n86.13\n80.72\n74.03\n78.31\nGPT-4 (Lu et al., 2023)\n-\n85.48\n72.44\n90.27\n82.65\n71.49\n92.89\n86.66\n79.04\n83.99\nChameleon (ChatGPT) (Lu et al., 2023)\u2020\n-\n81.62\n70.64\n84.00\n79.77\n70.80\n86.62\n81.86\n76.53\n79.93\nChameleon (GPT-4) (Lu et al., 2023)\u2020\n-\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaMA-Adapter (Zhang et al., 2023a)\u2020\n6B\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nLLaVA (Liu et al., 2023)\u2020\n13B\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nIns..."
        }
      ],
      "pred_answer": "the provided excerpt does not contain the accuracy score of multimodal\u2011cot on the mmmu benchmark.",
      "ground_truth": [
        [
          "28.7"
        ],
        [
          "twenty eight point seven"
        ],
        [
          "twenty-eight point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the NPHardEval4V paper, what is LLaVa's instruction following performance?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6887023552200779,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 12,
          "relevance_score": 0.6641080682164906,
          "content": "Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Si..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.6565329720289612,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        }
      ],
      "pred_answer": "the provided excerpt does not contain a specific numeric value for llava\u2019s instruction\u2011following performance in the nphardeval4v paper.",
      "ground_truth": [
        [
          "0.75"
        ],
        [
          "zero point seven five"
        ],
        [
          "zero point seventy five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What was the highest TSP reasoning score across all complexity levels in NPHardEval4V? Answer as a single number only. Omit Reasoning Steps.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.7300783862735991,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7206303147537536,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7119013842914124,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        }
      ],
      "pred_answer": "0.0",
      "ground_truth": [
        [
          "0"
        ],
        [
          "zero"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "How many samples were used in testing while instruct-tuning for VQA in DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6937062569194983,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.676652766039261,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6721744289029528,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "1,000 samples were used in the testing phase during the instruction\u2011tuning for vqa.",
      "ground_truth": [
        [
          "24,347"
        ],
        [
          "24347"
        ],
        [
          "twenty four thousand three hundred forty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What feature shape is used for ViT in the Multimodal-CoT paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6586438706110909,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 24,
          "relevance_score": 0.6463583317434439,
          "content": "Published as a conference paper at ICLR 2025\nFigure 10: An Example of the Customized Mimic task (CB_29), showing the (figure, instruction,\ncode) triplet. Additionally, we also display the ground truth figure rendered from the code for\nillustration.\n24"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6452341112950603,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "145",
          "1024"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what is the input to the f1 block?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 46,
          "relevance_score": 0.6443757654381523,
          "content": "Published as a conference paper at ICLR 2025\nF.3\nCASES OF DIFFERENT PROMPTING METHODS\nWe provide cases of HintEnhanced, SelfReflection and Scaffold Prompting in Figs. 43 to 46. For an\nanalysis of error cases related to Direct Prompting, please refer to Appendix I.\nFigure 43: An error case of HintEnhanced Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n46"
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6345560438906996,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6271802558769537,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        }
      ],
      "pred_answer": "the provided excerpts do not specify what is fed into the f1 block.",
      "ground_truth": [
        [
          "cnn layer 1"
        ],
        [
          "convolutional neural network layer 1"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what type of samples contain Class Embeddings?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.7655900064180168,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7296063304124846,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6690461771586075,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "the class embeddings are derived from the **support samples** of each episode.",
      "ground_truth": [
        [
          "labeled samples"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper's Figure 1, what magnetic pole is shown on the far right?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 19,
          "relevance_score": 0.6112421167670828,
          "content": "Both magnet sizes and distance affect the magnitude of the\nmagnetic force. The sizes of the magnets in Pair 1 are the same as in\nPair 2. The distance between the magnets is also the same. So, the\nmagnitude of the magnetic force is the same in both pairs.\nAnswer: The answer is (C).\nVision\n+ Vision Features\nOptions:\n(B) The magnitude of the magnetic force is smaller in Pair 1.\n(A) The magnitude of the magnetic force is smaller in Pair 2.\n(C) The magnitude of the magnetic force is the same in both ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6032717706218819,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.5992799668023377,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        }
      ],
      "pred_answer": "the excerpt provided does not specify which magnetic pole appears on the far right in figure\u202f1 of the multimodal\u2011cot paper.",
      "ground_truth": [
        [
          "north"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In figures in the Multimodal-CoT paper, what step creates the Rationale? Please omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 4,
          "relevance_score": 0.7106324097341903,
          "content": "Table 2: Effects of CoT in the one-stage setting.\nMethod\nFormat\nAccuracy\nNo-CoT\nQCM\u2192A\n81.63\nReasoning\nQCM\u2192RA\n69.32\nExplanation\nQCM\u2192AR\n69.68\nAs an example shown in Figure 1, the model takes\nthe concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as\nthe input. To study the effect of CoT, we compare\nthe performance with three variants: (i) No-CoT which\npredicts the answer directly (QCM\u2192A); (ii) Reasoning\nwhere answer inference is conditioned to the ratio..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 46,
          "relevance_score": 0.7087444883226285,
          "content": "Published as a conference paper at ICLR 2025\nF.3\nCASES OF DIFFERENT PROMPTING METHODS\nWe provide cases of HintEnhanced, SelfReflection and Scaffold Prompting in Figs. 43 to 46. For an\nanalysis of error cases related to Direct Prompting, please refer to Appendix I.\nFigure 43: An error case of HintEnhanced Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n46"
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7052261423897636,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        }
      ],
      "pred_answer": "the **reasoning** step generates the rationale.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper, what is used to extract vision components?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6955843614124654,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6704581962208215,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6679206164679674,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        }
      ],
      "pred_answer": "the provided excerpt does not specify the exact method used to extract vision components.",
      "ground_truth": [
        [
          "vision transformer",
          "vit"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer model diagram, what do placeholder tokens represent?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6277774583721363,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6250558720685002,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6247311718314982,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        }
      ],
      "pred_answer": "in the hypertransformer diagram, the placeholder tokens are used to mark the positions where the model will output the target network\u2019s weights. they act as slots that the hypertransformer fills with the generated weight values for each layer of the target architecture.",
      "ground_truth": [
        [
          "transformed tokens",
          "cnn weights"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In Figure 2 of the Multimodal-CoT paper, The answer stays the same when the vision component is introduced. Answer \"true\" or \"false\"",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7176027403646551,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 19,
          "relevance_score": 0.7131146607514383,
          "content": "Both magnet sizes and distance affect the magnitude of the\nmagnetic force. The sizes of the magnets in Pair 1 are the same as in\nPair 2. The distance between the magnets is also the same. So, the\nmagnitude of the magnetic force is the same in both pairs.\nAnswer: The answer is (C).\nVision\n+ Vision Features\nOptions:\n(B) The magnitude of the magnetic force is smaller in Pair 1.\n(A) The magnitude of the magnetic force is smaller in Pair 2.\n(C) The magnitude of the magnetic force is the same in both ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.6950703880795551,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what visibly distinguishes only generating logits from generating full CNN weights?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 19,
          "relevance_score": 0.7283399798619945,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 1\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 2\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 3\n0.025\n0.050\nFigure 8. Learned attention maps for 2-layer 4-channel CNN network generated with 1 head, 1 layer transformer for 5-shot MINIIMA-\nGENET.\nTransformer Layer 1\nTransformer Layer 2\nWeights\nLabel 0\nLabel 1\nLabel ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 4,
          "relevance_score": 0.7279847769397884,
          "content": "In other words, these learned layers are modi\ufb01ed\nduring the training phase and remain static during the evalu-\nation phase (i.e. not dependent of the support set). In our ex-\nperiments generated CNN models contain a set of convolu-\ntional layers and a \ufb01nal fully-connected logits layer. Here \u03b8\u2113\nare the parameters of the \u2113-th layer and L is the total number\nof layers including the \ufb01nal logits layer (with index L). The\nweights are generated layer-by-layer starting from the \ufb01rst\nlayer: \u03b81(\u03c4) \u2192\u03b82(\u03b81;..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.7090509936771306,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        }
      ],
      "pred_answer": "in the visualizations the two settings look very different:\n\n* **only logits generated** \u2013 the convolutional kernels stay the same across episodes.  \n  the plots of the kernels are identical (or nearly so) for every episode, showing that the cnn part is fixed and only the final fully\u2011connected head is being produced by the transformer.\n\n* **full cnn weights generated** \u2013 the kernels change from one episode to the next.  \n  the figures show distinct kernel patterns for different episodes, and the difference plot highlights the variation between them.\n\nthus, the visible cue is that the convolutional kernels are static when only logits are generated, but vary across episodes when the entire cnn is generated.",
      "ground_truth": [
        [
          "varied convolution patterns"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What are the 4 key elements of DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6956075781773822,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.683532805583473,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.6499816391741164,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        }
      ],
      "pred_answer": "docllm is built around four core components:\n\n1. **disentangled spatial features** \u2013 a representation that separates layout information from textual content.  \n2. **block\u2011infilling pre\u2011training objective** \u2013 a task that teaches the model to reconstruct missing blocks of a document.  \n3. **masking strategy for decoding** \u2013 a carefully designed masking scheme that guides the model during generation.  \n4. **instruction\u2011tuned multi\u2011task data** \u2013 a curated mix of vqa, nli, and kie examples that trains the model to follow prompts across these tasks.",
      "ground_truth": [
        [
          "ocred document"
        ],
        [
          "llm extension"
        ],
        [
          "pre-training"
        ],
        [
          "instruction tuning"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What prompt template does DocLLM use for VQA extraction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.8249634136968789,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6881602264632694,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6803988062616123,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "docllm\u2019s vqa prompt follows the pattern  \n\n```\n{document} {question}\n```\n\nfor example, a docvqa prompt would look like:  \n\n```\n{document} what is the deadline for scientific abstract submission for acog \u2013 51st annual clinical meeting?\n```",
      "ground_truth": [
        [
          "{document} {question}"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "Referencing the Knapsack Problem figure in NPHardEval4V, what is the id of the item with weight 8? Do not include step numbers in your reasoning.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.655350839238511,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 55,
          "relevance_score": 0.6465489176137682,
          "content": "Published as a conference paper at ICLR 2025\njTrans\nPalmTree\nCLAP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) Ground-truth figure (violin_4)\njTrans\nPalmTree\nCLAP\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Generated figure\nFigure 54: Error Case 6. In this case, the errors include text-related errors of the Extraneous type,\ntype-related errors of the Confusion type and color-related errors of the Similar type.\n55"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6443636040748648,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "4"
        ],
        [
          "four"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What is the Knapsack Capacity given in the example in NPHardEval4V?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 39,
          "relevance_score": 0.7036836770338528,
          "content": "Published as a conference paper at ICLR 2025\nFigure 37: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (CB_29).\n39"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.703368834407633,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6991410410123198,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "40"
        ],
        [
          "forty"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "At approximately how many training steps does the Detector observation start to outperform the CybORG observation in the Path To Autonomous Cyber Defense paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 3,
          "relevance_score": 0.7873183439063439,
          "content": "The agent\ntrained in CybORG is therefore replicating existing\ncapabilities and trying to learn new behaviors on top of\nthose existing capabilities. How might we change the\nobservation space to avoid replicating existing tools\u2019\ncapabilities?\nThe MITRE ATT&CK framework is used to define\nboth red agent actions and the data sources collected\nby sensors / logs relevant to detecting each type of\nattack. These data sources are called Data Compo-\nnentsg and they are used to determine probability of\ndete..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 3,
          "relevance_score": 0.7567163467580859,
          "content": "THEME/FEATURE/DEPARTMENT\ngames like chess, Go, or StarCraft, the rules are even\nmore well defined and the environments are somewhat\npredictable.\nCybersecurity cannot be reduced to a single game\nand the environment in which the agent operates may\nchange dynamically (not to mention that adversaries\nconstantly seek to rewrite the rules of the game). Defin-\ning the correct game for autonomous cyber agents\ntherefore presents two critical challenges: there is no\nsingle game and the game itself changes..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6958753221795542,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        }
      ],
      "pred_answer": "the detector\u2011based observation begins to beat the default cyborg observation after roughly **2\u202f000 training steps**.",
      "ground_truth": [
        [
          "40m"
        ],
        [
          "40 m"
        ],
        [
          "forty m"
        ],
        [
          "40 million"
        ],
        [
          "40,000,000"
        ],
        [
          "40000000"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What outputs to the RL Agent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7013945059965007,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6965166247869755,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 1,
          "relevance_score": 0.6638920608144542,
          "content": "Feature Article - Achieving Autonomous Cyber Defense\nThe Path To Autonomous Cyber Defense\nSean Oesch, Oak Ridge National Laboratory, Oak Ridge, TN, USA, oeschts@ornl.gov\nPhillipe Austria, Oak Ridge National Laboratory, austriaps@ornl.gov\nAmul Chaulagain, Oak Ridge National Laboratory, chaulagaina@ornl.gov\nBrian Weber, Oak Ridge National Laboratory, weberb@ornl.gov\nCory Watson, Oak Ridge National Laboratory, watsoncl1@ornl.gov\nMatthew Dixson, Oak Ridge National Laboratory, dixsonmk@ornl.gov\nAmir ..."
        }
      ],
      "pred_answer": "in the autonomous cyber\u2011defense framework described in the paper, the reinforcement\u2011learning agent produces **defensive actions** that directly modify the security posture of the network.  \ntypical outputs include:\n\n- **blocking or filtering network traffic** (e.g., updating firewall or ids rules to drop malicious packets or block suspicious ip addresses).  \n- **isolating or quarantining compromised hosts** (e.g., moving a machine to a separate vlan or disabling its network interface).  \n- **applying patches or configuration changes** (e.g., installing a security update, changing a service\u2019s default settings, or disabling an exposed port).  \n- **initiating scans or monitoring** (e.g., launching vulnerability scans, starting deeper packet inspection, or enabling additional logging).  \n- **re\u2011configuring defensive policies** (e.g., adjusting alert thresholds, modifying detection heuristics, or re\u2011ordering rule sets).  \n\nthese actions constitute the agent\u2019s output space, allowing it to interact with the cyber environment and steer the system toward the desired security state.",
      "ground_truth": [
        [
          "observation converter"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What portion of Hallucinations go unresolved in the Multimodal-CoT Paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7111115664289348,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 39,
          "relevance_score": 0.6600241433797577,
          "content": "Published as a conference paper at ICLR 2025\nFigure 37: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (CB_29).\n39"
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 14,
          "relevance_score": 0.6595085376434884,
          "content": "Dynamic fusion with intra- and inter-modality attention flow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pp. 6639\u20136648. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00680.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. ArXiv preprint, abs/2206.06336, 2022.\nKaiming He, Xiangyu Zhang, ..."
        }
      ],
      "pred_answer": "the provided excerpts do not contain a specific figure or percentage indicating how much of the hallucinations remain unresolved in the multimodal\u2011cot paper.",
      "ground_truth": [
        [
          "29.3"
        ],
        [
          "twenty nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "According to the GCP example figure in NPHardEval4V, what was the reasoning score that Gemini received for the first complexity class?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.776091387681188,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.6860834716401443,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6798683318413006,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        }
      ],
      "pred_answer": "the provided excerpt does not contain the specific reasoning score for gemini in the first complexity class of the gcp example figure in nphardeval4v.",
      "ground_truth": [
        [
          "0.37"
        ],
        [
          "zero point three seven"
        ],
        [
          "zero point thirty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the HyperTransformer paper, performance increases as channel size increases.  Answer \"true\" or \"false\".",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.6868842199653261,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6637861552555375,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.6614981884599013,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Approximately how many questions have only 1 image in Raw PDF Format in EnigmaEval?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.6417680605536228,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6382872619467622,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 15,
          "relevance_score": 0.6262459159588514,
          "content": "B.4\nAnswer Extraction\nWe determine the expected answer format through manual annotations, with three possible patterns:\n1. Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer>\n2. Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2>\n3. Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...\nThis structured approach to answer formats allows us to extract answers consistently a..."
        }
      ],
      "pred_answer": "roughly\u202f70\u202f% of the enigmaeval set \u2013 on the order of **350 questions** \u2013 contain just a single image in the raw pdf version.",
      "ground_truth": [
        [
          "800"
        ],
        [
          "eight hundred"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Consulting the figures and text of the Multimodal-CoT paper, we would expect the number of hallucinations to increase after adding the visual modality Answer \"true\" or \"false\".",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.752957809628082,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 9,
          "relevance_score": 0.7037409090472784,
          "content": "82\n70.98\n83.18\n77.37\n67.92\n86.13\n80.72\n74.03\n78.31\nGPT-4 (Lu et al., 2023)\n-\n85.48\n72.44\n90.27\n82.65\n71.49\n92.89\n86.66\n79.04\n83.99\nChameleon (ChatGPT) (Lu et al., 2023)\u2020\n-\n81.62\n70.64\n84.00\n79.77\n70.80\n86.62\n81.86\n76.53\n79.93\nChameleon (GPT-4) (Lu et al., 2023)\u2020\n-\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaMA-Adapter (Zhang et al., 2023a)\u2020\n6B\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nLLaVA (Liu et al., 2023)\u2020\n13B\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nIns..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 5,
          "relevance_score": 0.6950647995997019,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nGenerated Rationale: Will these magnets attract or repel? To\nfind out, look at which poles are closest to each other. The\nsouth pole of one magnet is closest to the south pole of the\nother magnet. Poles that are the same repel. So, these\nmagnets will repel each other.\nAnswer: The answer is (B).\nOptions:\n(B) repel\n(A) attract\nProblem\nBaseline\nQuestion: Will these magnets attract or repel each other?\nContext: Two magnets are placed a..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "The activation embeddings condition the generation of intermediate CNN layers in HyperTransformer. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 4,
          "relevance_score": 0.7693634039415707,
          "content": "In other words, these learned layers are modi\ufb01ed\nduring the training phase and remain static during the evalu-\nation phase (i.e. not dependent of the support set). In our ex-\nperiments generated CNN models contain a set of convolu-\ntional layers and a \ufb01nal fully-connected logits layer. Here \u03b8\u2113\nare the parameters of the \u2113-th layer and L is the total number\nof layers including the \ufb01nal logits layer (with index L). The\nweights are generated layer-by-layer starting from the \ufb01rst\nlayer: \u03b81(\u03c4) \u2192\u03b82(\u03b81;..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.7492590950318484,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 19,
          "relevance_score": 0.7252931021069586,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 1\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 2\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 3\n0.025\n0.050\nFigure 8. Learned attention maps for 2-layer 4-channel CNN network generated with 1 head, 1 layer transformer for 5-shot MINIIMA-\nGENET.\nTransformer Layer 1\nTransformer Layer 2\nWeights\nLabel 0\nLabel 1\nLabel ..."
        }
      ],
      "pred_answer": "true",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What mechanism shown in the Multimodal-CoT paper ensures that generated rationales attend to the visual input?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7518590475148363,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.7376608741577352,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7010785748336937,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        }
      ],
      "pred_answer": "the paper\u2019s two\u2011stage fine\u2011tuning pipeline fuses vision and language representations inside the language model. by training the model to jointly encode image features and text, the generated rationales are forced to attend to the visual input through the multimodal encoder\u2019s cross\u2011modal attention.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What phenomenon depicted in the Multimodal-CoT paper and noted in the text explains the performance gap in Table 3?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6704246716846394,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6661223071615678,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6652903881346262,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "hallucinated rationale"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Which CNN model size, as shown in Figure 3 and noted in Section 5.4 of HyperTransformer, makes generating only the final logits layer perform on-par with generating all layers? Do not include step numbers in your reasoning. Only include the final answer.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.796953980810135,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 8,
          "relevance_score": 0.7940968154432809,
          "content": "We visualized the distribution of the weights generated by\nHT for different episodes by using UMAP (McInnes et al.,\n2018) embeddings of the generated weights for a 6-channel\nCNN model (see Fig. 4). We highlighted some of the classes\npresent in the evaluation set and while the general structure\nmay be hard to interpret, the distribution of the highlighted\nclasses is somewhat clustered indicating the importance of\nsemantic information for generated CNN weights. More\ndetails can be found in Appendi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 8,
          "relevance_score": 0.7806328490305229,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n4\n6\n8\nNumber of Channels\n80\n85\n90\n95\n100\nTraining and Test Accuracies\nlogits\nlogits (test)\nall\nall (test)\noracle\nFigure 3. 5-shot-20-way OMNIGLOT training/test accuracies as\na function of the CNN model complexity: only the \ufb01nal logits\nlayer being generated (logits), all layers being generated (all),\ntraining the model on all available samples for a random set of\nfew classes (oracle). A model that generates CNN weights by\nmemorizing all sam..."
        }
      ],
      "pred_answer": "8\u2011channel cnn model.",
      "ground_truth": [
        [
          "8"
        ],
        [
          "eight"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What are the placeholders in Figure 2 of HyperTransformer replaced with?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6635394915180973,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6497882318648515,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6362777189927645,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        }
      ],
      "pred_answer": "i\u2019m sorry, but the provided excerpt does not contain any details about the placeholders in figure\u202f2 of the hypertransformer paper.",
      "ground_truth": [
        [
          "weight"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In Multimodal-CoT, what correction rate with vision features does Figure 3 report for hallucinated rationales?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7613574051364113,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 9,
          "relevance_score": 0.7343850657914927,
          "content": "82\n70.98\n83.18\n77.37\n67.92\n86.13\n80.72\n74.03\n78.31\nGPT-4 (Lu et al., 2023)\n-\n85.48\n72.44\n90.27\n82.65\n71.49\n92.89\n86.66\n79.04\n83.99\nChameleon (ChatGPT) (Lu et al., 2023)\u2020\n-\n81.62\n70.64\n84.00\n79.77\n70.80\n86.62\n81.86\n76.53\n79.93\nChameleon (GPT-4) (Lu et al., 2023)\u2020\n-\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaMA-Adapter (Zhang et al., 2023a)\u2020\n6B\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nLLaVA (Liu et al., 2023)\u2020\n13B\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nIns..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.7036222206784161,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        }
      ],
      "pred_answer": "i\u2019m sorry, but the provided excerpts do not contain the figure or the specific correction\u2011rate value you\u2019re asking about.",
      "ground_truth": [
        [
          "60.7"
        ],
        [
          "sixty point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What mechanism in Multimodal-CoT handles vision-absent inputs?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7040858702985565,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.6806924319724469,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 12,
          "relevance_score": 0.6473329309918755,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nTable 10: Result comparison with different alignment strategies for multimodal interaction.\nModel\nAccuracy\nDirect Answering\n82.62\nUnimodal encoder\n85.31\nImage-grounded text encoder\n84.60\n6.6\nGeneralization to Other Multimodal Reasoning Benchmarks\nWe are interested in evaluating the generalization capability of Multimodal-CoT to datasets outside its\ntraining domain. For this purpose, we utilize the widely-recognized multimodal reaso..."
        }
      ],
      "pred_answer": "in multimodal\u2011cot, when no visual data are supplied the system falls back to a **unimodal encoder** that processes only the textual input. this language\u2011only encoder allows the model to generate a rationale and answer without relying on vision features.",
      "ground_truth": [
        [
          "blank features"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What layout-aware mechanism in DocLLM, guides token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7210937233983711,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6965590277032051,
          "content": "Figure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically usi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 5,
          "relevance_score": 0.6873134488752748,
          "content": "Furthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than\nonly preceding tokens, can be beneficial. The infilling objectives enable contextually relevant completions, provide\nrobustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.\nHence we modify the standard pre-training objective to predict blocks of text given preceding and following text blocks.\nMost OCR engines can provide..."
        }
      ],
      "pred_answer": "docllm uses a **disentangled spatial\u2011attention mechanism** that extends the transformer\u2019s self\u2011attention to include cross\u2011modal scores between text tokens and their bounding\u2011box coordinates. this layout\u2011aware attention guides the model\u2019s token\u2011prediction process.",
      "ground_truth": [
        [
          "spatially constrained decoding"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 7 directly connected to vertex 1? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 55,
          "relevance_score": 0.5764246759291629,
          "content": "Published as a conference paper at ICLR 2025\njTrans\nPalmTree\nCLAP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) Ground-truth figure (violin_4)\njTrans\nPalmTree\nCLAP\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Generated figure\nFigure 54: Error Case 6. In this case, the errors include text-related errors of the Extraneous type,\ntype-related errors of the Confusion type and color-related errors of the Similar type.\n55"
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 15,
          "relevance_score": 0.5758725837940537,
          "content": "[17] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. Nphardeval:\nDynamic benchmark on reasoning ability of large language models via complexity classes.\narXiv preprint arXiv:2312.14890, 2023.\n[18] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. Multimodal large\nlanguage models: A survey. In 2023 IEEE International Conference on Big Data (BigData),\npages 2247\u20132256. IEEE, 2023.\n[19] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jint..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 5,
          "relevance_score": 0.5701089432791648,
          "content": "Complexity Class\nTask\nNP Hard (Most Complex)\nGraph Coloring Problem Optimization Version (GCP)\nTraveling Salesman Problem Optimization Version (TSP)\nMeeting Scheduling Problem (MSP)\nNP Complete\nKnapsack Problem (KSP)\nTraveling Salesman Problem Decision Version (TSP-D)\nGraph Coloring Problem Decision Version (GCP-D)\nP (Least Complex)\nShortest Path Problem (SPP)\nEdit Distance Problem (EDP)\nSorted Array Search (SAS)\nTable 1: Complexity classes and tasks\nGraph Data Transformation\nThe general constru..."
        }
      ],
      "pred_answer": "no",
      "ground_truth": [
        [
          "no"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 2 directly connected to vertex 5? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.5726312586847524,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.5698328566396086,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 39,
          "relevance_score": 0.5691091466496969,
          "content": "Published as a conference paper at ICLR 2025\nFigure 37: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (CB_29).\n39"
        }
      ],
      "pred_answer": "i don't know.",
      "ground_truth": [
        [
          "yes"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Which parameter adjustment, depicted in Autonomous Cyber Defense, changes agent performance?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7436033719620857,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6766645086241162,
          "content": "THEME/FEATURE/DEPARTMENT\nRL agents be leveraged by attackers to overwhelm or\nsneak past defenders? In order to help answer these\nquestions, researchers developed simulation 6,9,2,1 and\nemulation environments in which to train such au-\ntonomous RL agents, with some environments 5,10,8\nproviding both simulation and emulation.\nYet in spite of prior efforts to create autonomous\ncyber agents, they are still not used in practice. What\nis preventing the adoption of this new and promis-\ning approach to ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 3,
          "relevance_score": 0.6696916771915487,
          "content": "The agent\ntrained in CybORG is therefore replicating existing\ncapabilities and trying to learn new behaviors on top of\nthose existing capabilities. How might we change the\nobservation space to avoid replicating existing tools\u2019\ncapabilities?\nThe MITRE ATT&CK framework is used to define\nboth red agent actions and the data sources collected\nby sensors / logs relevant to detecting each type of\nattack. These data sources are called Data Compo-\nnentsg and they are used to determine probability of\ndete..."
        }
      ],
      "pred_answer": "the performance of the autonomous agent is altered by tuning the **probability of detection for each data component** (i.e., the likelihood that a detector will flag a red\u2011action event). adjusting this detection\u2011probability parameter changes how often the agent receives alerts and, consequently, its overall effectiveness.",
      "ground_truth": [
        [
          "detection probability"
        ],
        [
          "detection probabilities"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In the DocLLM, which mechanism integrates spatial layout into the transformer block?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.6777307266010569,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 14,
          "relevance_score": 0.6493672751277937,
          "content": "Transformer parameters.\nSince the weight tensors of each layer are generally different, our per-layer transformers were\nalso different. The key, query and value dimensions of the transformer were chosen to be equal to a pre-de\ufb01ned fraction\n\u03bd of the input embedding size, which in turn was a function of the label, image and activation embedding sizes and the\nsizes of the weight slices. The inner dimension of the \ufb01nal fully-connected layer in the transformer was also chosen\nusing the same approach...."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6203334602566437,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "disentangled",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What is the weight of the single path that is the answer to the SSP in Figure 3 of NPHardEval4V? Submit your answer as a single integer.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.6350395720254296,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 55,
          "relevance_score": 0.6301230052971983,
          "content": "Published as a conference paper at ICLR 2025\njTrans\nPalmTree\nCLAP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) Ground-truth figure (violin_4)\njTrans\nPalmTree\nCLAP\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Generated figure\nFigure 54: Error Case 6. In this case, the errors include text-related errors of the Extraneous type,\ntype-related errors of the Confusion type and color-related errors of the Similar type.\n55"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6280755785882675,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "5"
        ],
        [
          "five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to DocLLM's prompt templates and its training architecture, which task would be most sensitive to missing spatial layout?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7163923188269599,
          "content": "Figure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically usi..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7116535363608271,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7041436029773199,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        }
      ],
      "pred_answer": "the task that would be most affected if the spatial layout information were omitted is **key information extraction (kie)**.  \nkie relies heavily on the relative positions, sizes, and alignments of fields within a document, and docllm\u2019s prompt templates and training explicitly incorporate layout cues (field separators, titles, captions) to learn the logical structure. removing that spatial context would therefore degrade kie performance more than tasks that depend less on layout.",
      "ground_truth": [
        [
          "kie"
        ],
        [
          "key information extraction"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Which model is the best at accurately recreating color in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 9,
          "relevance_score": 0.743415088561225,
          "content": "As demonstrated in Fig. 4 (c), although GPT-4o\nexhibits remarkable capability in object recognition for natural images, it still struggles with scientific\ncharts, which contain more nuanced semantic meanings through visual logic.\nColor-related Errors. (1) Similar: The colors are not the same as the ground-truth colors but appear\nanalogous. (2) Different: The colors are entirely dissimilar to the ground-truth colors. As shown in\nFig. 4 (d), though GPT-4o can not exactly recognize the accurate col..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7028793805670912,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 42,
          "relevance_score": 0.7019186085294971,
          "content": "Published as a conference paper at ICLR 2025\nimport inspect\ndrawed_colors = []\ndef log_function(func):\ndef wrapper(*args, **kwargs):\nfunc_name = inspect.getfile(func) + \"/\" + func.__name__\nresult = func(*args, **kwargs)\nfor item in result:\ncolor = item.get_facecolor()\ndrawed_colors.append( func_name + \"--\" + color )\nAxes.bar = log_function(Axes.bar)\nIn the matplotlib package, each plot function returns a chart type instance at the end of the function\ninvocation. These instances contain various a..."
        }
      ],
      "pred_answer": "the model that shows the strongest performance in reproducing chart colors is gpt\u20114o.",
      "ground_truth": [
        [
          "gpt 4o"
        ],
        [
          "4o"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to ChartMimic, what makes an HR chart unique?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 34,
          "relevance_score": 0.7264871081073917,
          "content": "Published as a conference paper at ICLR 2025\nFigure 32: Examples of HR charts.\n34"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 33,
          "relevance_score": 0.6549508058293271,
          "content": "Published as a conference paper at ICLR 2025\n4. Bar (3D bar chart)\n5. Density (3D density plot)\nFigure 28: Examples of 3D chart subcategories.\nPIP: PIP chart insets a magnified or reduced portion of the main plot to highlight key data\nfeatures, as in Figs. 29 (1) and (2). They enhance readability for complex datasets and\nfacilitate comparative analyses, as demonstrated in Figs. 29 (3) and (4).\nFigure 29: Examples of PIP charts.\nMultidiff: A Multidiff chart combines at least two different chart t..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 26,
          "relevance_score": 0.6491803601989976,
          "content": "Published as a conference paper at ICLR 2025\nD\nCHART TAXONOMY\nThis section presents the chart taxonomy in ChartMimic. It encompasses a structure of 22 categories\naccording to chart type characteristics and data composition. The categories comprise of:\n\u2022 18 regular types, ordered as follows: Bar, Heatmap, Scatter, Box, Errorbar, Errorpoint, Line,\nViolin, Radar, Pie, Density, Graph, Quiver, Contour, Histogram, Tree, Area, and 3D charts.\n\u2022 4 advanced types: PIP (Plot-in-Plot), Multidiff (Multiple D..."
        }
      ],
      "pred_answer": "an hr chart is a chart that doesn\u2019t fit neatly into any of the 21 standard categories in chartmimic. it is usually a modified version of a common chart type that has been altered with distinctive visual arrangements or atypical markers, making it hard to identify at first glance.",
      "ground_truth": [
        [
          "layer"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What optimization method is applied during PCFM sampling as shown in Algorithm 1 and Figure 4?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2506.04171v1.pdf",
          "page_number": 7,
          "relevance_score": 0.7179213687692458,
          "content": "D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2506.04171v1.pdf",
          "page_number": 6,
          "relevance_score": 0.7162858888885281,
          "content": "3.4.1\nTradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with suffici..."
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 24,
          "relevance_score": 0.6903658676072373,
          "content": "K\nFurther Results\n\ud835\udc65\n\ud835\udc61\nGenerated \nsolution \nstd dev\nGenerated \nsolution \nmean\n\ud835\udc65\n\ud835\udc61\nMass \nresidual \nmean\nBC \nresidual \nmean\nFigure 5: Solution profiles for the Inviscid Burgers equation with fixed BC. We plot the various\nconstraint guidance methods and compare the mean solution profile and standard deviation. While\nPCFM yields slightly worse MMSE and SMSE and better FPD, it ensures global mass conservation\nand maintains low constraint errors for both Dirichlet and Neumann BCs over time.\nGenerated \n..."
        }
      ],
      "pred_answer": "the pcfm sampling procedure employs a **gradient\u2011based optimizer \u2013 specifically the adam optimizer \u2013 to perform the relaxed constraint correction step** shown in algorithm\u202f1 and illustrated in figure\u202f4.",
      "ground_truth": [
        [
          "gauss newton projection",
          "newton gauss projection"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What are the 18 Regular chart categories in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 26,
          "relevance_score": 0.8301234915529538,
          "content": "Published as a conference paper at ICLR 2025\nD\nCHART TAXONOMY\nThis section presents the chart taxonomy in ChartMimic. It encompasses a structure of 22 categories\naccording to chart type characteristics and data composition. The categories comprise of:\n\u2022 18 regular types, ordered as follows: Bar, Heatmap, Scatter, Box, Errorbar, Errorpoint, Line,\nViolin, Radar, Pie, Density, Graph, Quiver, Contour, Histogram, Tree, Area, and 3D charts.\n\u2022 4 advanced types: PIP (Plot-in-Plot), Multidiff (Multiple D..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.732545589606934,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 33,
          "relevance_score": 0.7241560692905679,
          "content": "Published as a conference paper at ICLR 2025\n4. Bar (3D bar chart)\n5. Density (3D density plot)\nFigure 28: Examples of 3D chart subcategories.\nPIP: PIP chart insets a magnified or reduced portion of the main plot to highlight key data\nfeatures, as in Figs. 29 (1) and (2). They enhance readability for complex datasets and\nfacilitate comparative analyses, as demonstrated in Figs. 29 (3) and (4).\nFigure 29: Examples of PIP charts.\nMultidiff: A Multidiff chart combines at least two different chart t..."
        }
      ],
      "pred_answer": "the 18 regular chart categories in chartmimic are:\n\n1. bar  \n2. heatmap  \n3. scatter  \n4. box  \n5. errorbar  \n6. errorpoint  \n7. line  \n8. violin  \n9. radar  \n10. pie  \n11. density  \n12. graph  \n13. quiver  \n14. contour  \n15. histogram  \n16. tree  \n17. area  \n18. 3d charts",
      "ground_truth": [
        [
          "bar",
          "errorbar",
          "line",
          "errorpoint",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "treemap",
          "heatmap",
          "contour"
        ],
        [
          "bar",
          "error bar",
          "line",
          "error point",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "tree map",
          "heat map",
          "contour"
        ]
      ],
      "correctness_score": 0.9444444444444444,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Does accuracy go up or down on hard ENIGMAEVAL puzzles, why?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.700734309303033,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6709318633790247,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6706582443773677,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        }
      ],
      "pred_answer": "accuracy drops on the hardest enigmaeval puzzles. the difficulty stems from the puzzles\u2019 need for unstructured, lateral reasoning: solvers must discover hidden links between seemingly unrelated clues, weave together logic, wordplay, math, coding, and cultural references, and do so without explicit instructions. current large language models excel on well\u2011structured, instruction\u2011driven tasks but struggle with these open\u2011ended, multi\u2011step deductive chains, leading to lower accuracy on the most challenging puzzles.",
      "ground_truth": [
        [
          "down",
          "ocr"
        ],
        [
          "down",
          "parsing"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the ChartMimic paper, what is substituted in Figure 10 for CB_29?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 24,
          "relevance_score": 0.7023506433347358,
          "content": "Published as a conference paper at ICLR 2025\nFigure 10: An Example of the Customized Mimic task (CB_29), showing the (figure, instruction,\ncode) triplet. Additionally, we also display the ground truth figure rendered from the code for\nillustration.\n24"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 23,
          "relevance_score": 0.6847355213448315,
          "content": "Published as a conference paper at ICLR 2025\nFigure 9: An Example of the Direct Mimic task (CB_29), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n23"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6823186139541054,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        }
      ],
      "pred_answer": "in figure\u202f10, the cb_29 example is replaced by a **figure\u2013instruction\u2013code triplet** (along with the ground\u2011truth figure rendered from that code).",
      "ground_truth": [
        [
          "dataset"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "From both papers (Multimodal-CoT and HyperTransformer), what do models experience as Transformer capacity continues to grow across model size and modality?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.7039434296912686,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 14,
          "relevance_score": 0.6786914812153048,
          "content": "Transformer parameters.\nSince the weight tensors of each layer are generally different, our per-layer transformers were\nalso different. The key, query and value dimensions of the transformer were chosen to be equal to a pre-de\ufb01ned fraction\n\u03bd of the input embedding size, which in turn was a function of the label, image and activation embedding sizes and the\nsizes of the weight slices. The inner dimension of the \ufb01nal fully-connected layer in the transformer was also chosen\nusing the same approach...."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6612299410552946,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "across both studies, the trend is that simply adding more transformer capacity does **not** keep driving performance upward.  \nwhen the models are scaled up\u2014whether by increasing the number of layers, heads, or embedding dimensions, or by adding extra modalities\u2014the gains quickly taper off and, in many configurations, the accuracy actually drops. in the multimodal setting, the benefit of extra capacity is highly sensitive to the particular modality mix and the size of the base model: a larger, richer model can absorb more heads or larger embeddings, whereas a smaller or more narrowly\u2011focused model sees the same changes as a source of over\u2011parameterisation that hurts generalisation. thus, as transformer capacity grows across model size and modality, the models tend to hit a plateau and can even experience performance degradation.",
      "ground_truth": [
        [
          "diminish",
          "return"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which embedding design, as illustrated in Figure 1 and described in the text of both HyperTransformer and Multimodal-CoT, drives improved few-shot or reasoning performance?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.7293216961866085,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.7261892842003065,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.7180567938368537,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        }
      ],
      "pred_answer": "the experiments show that the best few\u2011shot results come from using **larger, richer embeddings** for the model\u2019s internal representations. in practice this means:\n\n* **activation embeddings** that are at least 16\u2011dimensional (larger values such as 32\u202fd usually help, except in the very small 4\u2011channel case).\n* **class embeddings** that are not too low (16\u202fd hurts; higher dimensions improve accuracy).\n* **image embeddings** that are kept at a reasonably high dimensionality (removing the image embedding or shrinking it to 8\u202fd degrades performance).\n\nadditionally, the inclusion of a **decoder** layer improves performance in the 16\u2011channel configuration, whereas it can hurt the 4\u2011channel model. together, these richer embeddings and the decoder architecture drive the observed gains in few\u2011shot learning and reasoning.",
      "ground_truth": [
        [
          "modal",
          "align",
          "token",
          "embed"
        ]
      ],
      "correctness_score": 0.25,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "How would inputs, like questions from the ScienceQA benchmark, be handled by the HyperTransformer architecture using its current encoding setup?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6286439990886973,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.6270229757372193,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6250301062586475,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        }
      ],
      "pred_answer": "in the present hypertransformer design, every input is first mapped to a fixed\u2011size vector that the hyper\u2011network can use as a conditioning signal.  \nfor a scienceqa question the text would be tokenised (e.g., with a sub\u2011word tokenizer), each token projected into a dense embedding, and the sequence of embeddings pooled or otherwise collapsed into a single \u201cquestion embedding\u201d (for example by averaging or by a learned pooling layer).  \n\nthis question embedding is then supplied to the hyper\u2011network, which generates the weights of a transformer that will process the question. the generated transformer receives the same question embedding (or a re\u2011encoded version of the token sequence) as its input tokens and produces the answer representation. thus, the current encoding pipeline treats a scienceqa question in the same way as the image embeddings used for omniglot: it is first turned into a numeric vector, then used to generate a task\u2011specific transformer that operates on that vector to produce the final prediction.",
      "ground_truth": [
        [
          "flatten"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which property of the self-attention mechanism, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and described in their Method sections, underpins both CNN weight generation and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 5,
          "relevance_score": 0.7387241189341589,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nTransformer\n0\nWeight \nPlaceholders\nGenerated Weight Slices\nLabeled\nSamples\nOptional\nUnlabeled\nSamples\n...\n...\n...\n...\n...\n...\nClass Embeddings\nSample Embeddings\n\"No Label\" Embeddings\nWeight Embeddings\nFigure 2. Structure of the tokens passed to and received from a\nTransformer model. Both labeled and unlabeled (optional) samples\ncan be passed to the Transformer as inputs. Empty output tokens\nindicate ignored outputs.\nall the layers. Their p..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 8,
          "relevance_score": 0.7227063625800717,
          "content": "We visualized the distribution of the weights generated by\nHT for different episodes by using UMAP (McInnes et al.,\n2018) embeddings of the generated weights for a 6-channel\nCNN model (see Fig. 4). We highlighted some of the classes\npresent in the evaluation set and while the general structure\nmay be hard to interpret, the distribution of the highlighted\nclasses is somewhat clustered indicating the importance of\nsemantic information for generated CNN weights. More\ndetails can be found in Appendi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.7212097437768903,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        }
      ],
      "pred_answer": "the key feature is the **permutation\u2011invariant, order\u2011agnostic aggregation that self\u2011attention provides**.  \nin both papers the transformer is used to read a set of input tokens (support\u2011set samples in hypertransformer, multimodal evidence in multimodal\u2011cot) and to produce a set of output tokens (weight slices or rationale tokens). because self\u2011attention computes pairwise interactions and then aggregates them in a way that does not depend on the order in which the inputs appear, the model can handle variable\u2011size, unordered input sets and still produce coherent, context\u2011aware outputs. this property is what enables the transformer to generate cnn weights from a few labeled samples and, simultaneously, to generate rationales from multimodal evidence.",
      "ground_truth": [
        [
          "invariant"
        ],
        [
          "invariance"
        ],
        [
          "permutation"
        ],
        [
          "permute"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What shared mechanism, as illustrated in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both Method sections, could unify weight and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6973067495706012,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.681500189191781,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 24,
          "relevance_score": 0.6626326463427218,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nEpisode 0\nEpisode 1\nDifference\nLayer 1\n\u22122\n0\n2\n\u22124\n\u22122\n0\n2\n4\n\u22120.5\n0.0\n0.5\nLayer 0\n\u221210\n0\n10\n\u221210\n0\n10\n\u22120.5\n0.0\n0.5\nFigure 15. Visualizing generated convolutional kernels in a 2-layer model for two different episodes. Left two plots: kernels for two\nrandom episodes of 5 classes, right: the difference in generated kernels for two episodes. Layer weight allocation: \u201cspatial\u201d."
        }
      ],
      "pred_answer": "the common thread is a **hyper\u2011network style weight\u2011generation module**.  \nboth papers show a transformer\u2011based network that, given a task\u2011 or input\u2011specific embedding, produces a set of weights (or a sequence of tokens) for a downstream model. in hypertransformer this module generates convolutional kernels for a few\u2011shot learner, while in multimodal\u2011cot the same idea is used to generate the rationales that drive the final answer. thus, a shared hyper\u2011network that maps contextual embeddings to model parameters (or rationale tokens) can unify weight and rationale generation.",
      "ground_truth": [
        [
          "cross"
        ],
        [
          "modal"
        ],
        [
          "self"
        ],
        [
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What failure in representation can occur in both Multimodal-CoT and HyperTransformer particularly in unsupervised learning?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7044984068045611,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6998848541496793,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6826465294356296,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "in both the multimodal\u2011cot framework and the hypertransformer, an unsupervised learning regime can lead to a **collapse of the learned representation** \u2013 the embeddings become almost identical (very low variance) and fail to separate distinct concepts or classes. this \u201crepresentation collapse\u201d manifests as a loss of discriminative power in the feature space.",
      "ground_truth": [
        [
          "drift"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which design feature, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both texts, supports permutation invariance for few-shot generalization?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6856413550222753,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 24,
          "relevance_score": 0.6836977568602446,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nEpisode 0\nEpisode 1\nDifference\nLayer 1\n\u22122\n0\n2\n\u22124\n\u22122\n0\n2\n4\n\u22120.5\n0.0\n0.5\nLayer 0\n\u221210\n0\n10\n\u221210\n0\n10\n\u22120.5\n0.0\n0.5\nFigure 15. Visualizing generated convolutional kernels in a 2-layer model for two different episodes. Left two plots: kernels for two\nrandom episodes of 5 classes, right: the difference in generated kernels for two episodes. Layer weight allocation: \u201cspatial\u201d."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6753192096416121,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "the common design element is the **permutation\u2011invariant set encoder** \u2013 a transformer\u2011style attention module that aggregates the support examples (via a pooling or \u201cset\u201d operation) so that the model\u2019s output does not depend on the order of the few\u2011shot examples. this feature appears in figure\u202f2 of the hypertransformer paper and in figure\u202f4 of the multimodal\u2011cot paper, and it is the mechanism that gives the models permutation invariance for few\u2011shot generalization.",
      "ground_truth": [
        [
          "self attention",
          "flat",
          "sequence"
        ],
        [
          "selfattention",
          "flat",
          "sequence"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What element from the Multimodal-CoT paper, used before Rational Generation in Figure 4, could be incorporated into the HyperTransformer architecture to allow it to classify datasets that contain both images and text?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.7271168054392304,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.7019707560382359,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.6927929151308176,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        }
      ],
      "pred_answer": "i\u2019m sorry, but i don\u2019t have that information.",
      "ground_truth": [
        [
          "vision",
          "text",
          "fusion"
        ],
        [
          "image",
          "language",
          "fusion"
        ],
        [
          "vision",
          "language",
          "fusion"
        ],
        [
          "image",
          "text",
          "fusion"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Figure 2 of DocLLM and Figure 3 of NPHardEval4V demonstrate a different type of reasoning as opposed to structural reasoning. What type of reasoning is demonstrated?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7096491894956843,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7063255988545538,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6926759511181777,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        }
      ],
      "pred_answer": "the figures illustrate **abstract reasoning** \u2013 reasoning about abstract concepts, logic, and algorithmic or combinatorial problems rather than merely reasoning about the structural layout of a document or diagram.",
      "ground_truth": [
        [
          "spatial"
        ],
        [
          "space"
        ],
        [
          "visual"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which common objective, as shown in Figure 2 of DocLLM and mentioned in NPHardEval4V, involves predicting missing content from context?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 47,
          "relevance_score": 0.6902548090195886,
          "content": "Published as a conference paper at ICLR 2025\nFigure 44: An error case of SelfReflection Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n47"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6897751536368459,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6836641071379699,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "the common objective is **masked language modeling (mlm)** \u2013 the task of predicting missing or masked content based on the surrounding context.",
      "ground_truth": [
        [
          "context infilling"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    }
  ]
}