{
  "model": "llama3.3:70b_ LlamaIndex RAG",
  "api_model": "llama3.3:70b",
  "scores": {
    "Text-Only": 0.6236559139784947,
    "Tables": 0.15384615384615385,
    "Images": 0.21428571428571427,
    "Multimodal": 0.3377777777777778,
    "Cross-Document Multimodal": 0.2916666666666667,
    "Text-Only Hallucination Rate": 0.25806451612903225,
    "Tables Hallucination Rate": 0.38461538461538464,
    "Images Hallucination Rate": 0.6428571428571429,
    "Multimodal Hallucination Rate": 0.6,
    "Cross-Document Multimodal Hallucination Rate": 0.8
  },
  "overall_score": 0.3242464453109615,
  "total_hallucination_rate": 0.5371074087203119,
  "responses": [
    {
      "category": "Text-Only",
      "question": "What type of neural network does the HyperTransformer model generate? Do not use acronyms.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 19,
          "relevance_score": 0.7419531272984543,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 1\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 2\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 3\n0.025\n0.050\nFigure 8. Learned attention maps for 2-layer 4-channel CNN network generated with 1 head, 1 layer transformer for 5-shot MINIIMA-\nGENET.\nTransformer Layer 1\nTransformer Layer 2\nWeights\nLabel 0\nLabel 1\nLabel ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.730424219633416,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7285477764961411,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        }
      ],
      "pred_answer": "the hypertransformer model generates a convolutional neural network.",
      "ground_truth": [
        [
          "convolution"
        ],
        [
          "cnn"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What modality other than text does Multimodal-CoT incorporate?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7476094627812999,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nOutput\nRationale: Will these magnets attract or repel? To find out, look at which poles are\nclosest to each other. The north pole of one magnet is closest to the south pole of the\nother magnet. Poles that are different attract. So, these magnets will attract each other.\nAnswer: The answer is (A).\nVision\nLanguage\nInput\nQuestion: Will these magnets attract or repel each\nother?\nContext: Two magnets are placed as shown. Hint:\nMagnets t..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 12,
          "relevance_score": 0.6574353346291462,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nTable 10: Result comparison with different alignment strategies for multimodal interaction.\nModel\nAccuracy\nDirect Answering\n82.62\nUnimodal encoder\n85.31\nImage-grounded text encoder\n84.60\n6.6\nGeneralization to Other Multimodal Reasoning Benchmarks\nWe are interested in evaluating the generalization capability of Multimodal-CoT to datasets outside its\ntraining domain. For this purpose, we utilize the widely-recognized multimodal reaso..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6482962442996708,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        }
      ],
      "pred_answer": "vision (images)",
      "ground_truth": [
        [
          "vision"
        ],
        [
          "image"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one question and answer benchmark that the Multimodal-CoT paper mentions in its abstract.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.7388527665026804,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7315586655646096,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 17,
          "relevance_score": 0.7212655059624421,
          "content": "Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal language modeling. Proceedings of\nthe 40th International Conference on Machine Learning, PMLR, pp. 39755\u201339769, 2022.\nZhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual\nquestion answering. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long\nBeach, CA, USA,..."
        }
      ],
      "pred_answer": "one question and answer benchmark mentioned is mmmu.",
      "ground_truth": [
        [
          "scienceqa"
        ],
        [
          "a okvqa"
        ],
        [
          "aokvqa"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "In the Multimodal-CoT paper, what are the two stages of the proposed framework? Answer as a comma separated list.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6792781253768915,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6769671156705755,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6663782496225272,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        }
      ],
      "pred_answer": "there is no information about the file \"2302.00923v5.pdf\" or the \"multimodal-cot paper\", so it's not possible to provide the two stages of the proposed framework.",
      "ground_truth": [
        [
          "rationale generation",
          "answer inference"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Which datasets are used to evaluate HyperTransformer? Answer as a comma separated list.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.7150066731605126,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7011066177015296,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6990107807641305,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        }
      ],
      "pred_answer": "tieredimagenet",
      "ground_truth": [
        [
          "omniglot, miniimagenet, tieredimagenet"
        ],
        [
          "omniglot, mini imagenet, tiered imagenet"
        ],
        [
          "omniglot, mini image net, tiered image net"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What optimization-based algorithm mentioned in the HyperTransformer paper can fine-tune the embedding by performing additional SGD updates on all parameters of the model producing it?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 2,
          "relevance_score": 0.7136465965688713,
          "content": "This family of methods includes, for\nexample, such methods as Siamese networks (Koch et al.,\n2015), Matching Networks (Vinyals et al., 2016), Proto-\ntypical Networks (Snell et al., 2017), Relation Networks\n(Sung et al., 2018) and TADAM (Oreshkin et al., 2018).\nIt has recently been argued (Tian et al., 2020) that meth-\nods based on building a powerful sample representation\ncan frequently outperform numerous other approaches in-\ncluding many optimization-based methods. However, such\napproaches ess..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6911545327639604,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6858792223964905,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        }
      ],
      "pred_answer": "maml (model-agnostic meta-learning) is an optimization-based algorithm that can adapt the embedding to a new task by incorporating optimization within the learning process, and it does so by learning initial model parameters that produce good models for each episode after being adjusted with one or more gradient descent updates minimizing the corresponding episode classification loss.",
      "ground_truth": [
        [
          "maml"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What function (and input to the function) is used to generate H_language in Multimodal_CoT? Omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6794757811340596,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6566545344495281,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 14,
          "relevance_score": 0.6394692213122383,
          "content": "Dynamic fusion with intra- and inter-modality attention flow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pp. 6639\u20136648. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00680.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. ArXiv preprint, abs/2206.06336, 2022.\nKaiming He, Xiangyu Zhang, ..."
        }
      ],
      "pred_answer": "there is no information provided about the function used to generate h_language in multimodal_cot.",
      "ground_truth": [
        [
          "languageencoder",
          "x",
          "language"
        ],
        [
          "language encoder",
          "x",
          "language"
        ],
        [
          "language",
          "encoder",
          "x"
        ]
      ],
      "correctness_score": 0.3333333333333333,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does Multimodal-CoT claim to be the main benefit of separating rationale generation from answer inference?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.7838094372816017,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7685112266957838,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 4,
          "relevance_score": 0.7398870226582446,
          "content": "Table 2: Effects of CoT in the one-stage setting.\nMethod\nFormat\nAccuracy\nNo-CoT\nQCM\u2192A\n81.63\nReasoning\nQCM\u2192RA\n69.32\nExplanation\nQCM\u2192AR\n69.68\nAs an example shown in Figure 1, the model takes\nthe concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as\nthe input. To study the effect of CoT, we compare\nthe performance with three variants: (i) No-CoT which\npredicts the answer directly (QCM\u2192A); (ii) Reasoning\nwhere answer inference is conditioned to the ratio..."
        }
      ],
      "pred_answer": "multimodal-cot claims that separating rationale generation from answer inference mitigates hallucination and enhances convergence, resulting in superior performance on benchmark datasets.",
      "ground_truth": [
        [
          "hallucinat"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "NPHardEval4V says reasoning abilities increase as problem size increases. Answer \"true\" or \"false\"",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.732449250431618,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7275086233614273,
          "content": "2.2\nBenchmarks of Multimodal Large Language Models (MLLMs)\nAs the reasoning ability of MLLMs continues to advance, benchmarks have become instrumental in\nevaluating their performance and identifying areas that require improvement. Wang et al. (2024) sug-\ngest that a robust multimodal reasoning benchmark must fulfill three key criteria: (1) the integration\nof multimodal information, (2) the categorization of reasoning, and (3) in-depth annotations of the\nreasoning steps [4].\nPrevious research has..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7206326718274495,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "There is a typo in the version of The Path To Autonomous Cyber Defense that you have. What is 'cyber' misspelled as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6752293027023976,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6512204366325942,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6164719363277363,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        }
      ],
      "pred_answer": "there is no indication of a typo in the word \"cyber\" in the provided text. the word \"cyber\" appears multiple times and is consistently spelled correctly as \"cyber\". therefore, it is not possible to determine what \"cyber\" might be misspelled as.",
      "ground_truth": [
        [
          "cuber"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the Oak Ridge Center for AI Security Research abbreviated as?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7522109436252645,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7255197448575132,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 34,
          "relevance_score": 0.6219144630171698,
          "content": "Published as a conference paper at ICLR 2025\nFigure 32: Examples of HR charts.\n34"
        }
      ],
      "pred_answer": "caiser",
      "ground_truth": [
        [
          "caiser"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does DocLLM model that traditional LLMs don't?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7487414314780718,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7060669800030915,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6908543255595933,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        }
      ],
      "pred_answer": "docllm demonstrates superior performance in layout-intensive tasks such as kie and cls, which is not typically seen in traditional llms. additionally, it outperforms most multimodal language models in vqa and nli tasks, although with some exceptions. the model also shows strong performance in the stdd setting, surpassing other models like llama2 and mplug-docowl on certain datasets. this suggests that docllm has capabilities that traditional llms may not possess, particularly in handling layout-intensive and multimodal tasks.",
      "ground_truth": [
        [
          "spatial layouts",
          "text semantics"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Which pre-training objective does DocLLM use instead of standard next-token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7379496322229407,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6961907231170723,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 5,
          "relevance_score": 0.6958583276814789,
          "content": "Furthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than\nonly preceding tokens, can be beneficial. The infilling objectives enable contextually relevant completions, provide\nrobustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.\nHence we modify the standard pre-training objective to predict blocks of text given preceding and following text blocks.\nMost OCR engines can provide..."
        }
      ],
      "pred_answer": "docllm uses an autoregressive block infilling objective as its pre-training objective, where text blocks are randomly masked, and the masked blocks are shuffled and reconstructed in a sequential left-to-right fashion. this objective involves predicting blocks of text given preceding and following text blocks, rather than just predicting the next token.",
      "ground_truth": [
        [
          "text infilling"
        ],
        [
          "block infilling"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What type of problems does the NPHardEval4V benchmark aim to test in MLLMs?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7277517044945809,
          "content": "2.2\nBenchmarks of Multimodal Large Language Models (MLLMs)\nAs the reasoning ability of MLLMs continues to advance, benchmarks have become instrumental in\nevaluating their performance and identifying areas that require improvement. Wang et al. (2024) sug-\ngest that a robust multimodal reasoning benchmark must fulfill three key criteria: (1) the integration\nof multimodal information, (2) the categorization of reasoning, and (3) in-depth annotations of the\nreasoning steps [4].\nPrevious research has..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7216200647752443,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.7212416017097468,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        }
      ],
      "pred_answer": "the nphardeval4v benchmark aims to test problems that are categorized into three primary computational complexity classes: p (polynomial time), np-complete (nondeterministic polynomial-time complete), and np-hard, with tasks further divided into 10 progressive difficulty levels. this includes visual representations of problems, enabling the assessment of model performance across a spectrum of computational challenges, particularly focusing on the pure reasoning ability of multimodal large language models (mllms).",
      "ground_truth": [
        [
          "reasoning"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name the two NP computational complexity classes used in NPHardEval4V.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7464335593924729,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 15,
          "relevance_score": 0.7285338961250334,
          "content": "[17] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. Nphardeval:\nDynamic benchmark on reasoning ability of large language models via complexity classes.\narXiv preprint arXiv:2312.14890, 2023.\n[18] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. Multimodal large\nlanguage models: A survey. In 2023 IEEE International Conference on Big Data (BigData),\npages 2247\u20132256. IEEE, 2023.\n[19] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jint..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.7086237264375325,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        }
      ],
      "pred_answer": "np-complete and np-hard are the two np computational complexity classes used in nphardeval4v.",
      "ground_truth": [
        [
          "np hard",
          "np complete"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What are attackers called in the strategic game modeled in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7441370368814224,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7114152065926035,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6570732967261991,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        }
      ],
      "pred_answer": "the provided text does not explicitly mention what attackers are called in the strategic game modeled in the autonomous cyber defense paper. it discusses autonomous cyber defense agents, their goals, and action spaces but does not provide specific terminology for attackers within this context.",
      "ground_truth": [
        [
          "red team"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Who does the blue team represent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.704949032457147,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6947066823698324,
          "content": "THEME/FEATURE/DEPARTMENT\nAmir Sadovnik is a research scientist at Oak Ridge\nNational Laboratory. He is the current Research Lead\nfor the Center for AI Security (CAISER) at the lab\ninvestigating threats to and from AI. Amir received his\nPh.D. from Cornell University, where his focus was on\ncomputer vision and machine learning. He is a member\nof IEEE.\nCory Watson is a Cyber Security Systems Engineer at\nOak Ridge National Laboratory. He specializes in Linux\nsystem administration, network engineerin..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.667631302532137,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        }
      ],
      "pred_answer": "the provided text does not explicitly mention the \"blue team\" or their representation in the autonomous cyber defense paper. however, it discusses autonomous cyber defense agents, their creation, and the importance of playing the \"right game\" with well-defined goals and boundaries, but it does not specify who the blue team represents.",
      "ground_truth": [
        [
          "defens"
        ],
        [
          "defender"
        ],
        [
          "defen"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does NPHardEval4V use in place of static benchmarks?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6758999610485175,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.675301808825663,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.668870476788268,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "a dynamic reasoning benchmark",
      "ground_truth": [
        [
          "dynamic",
          "benchmark"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What is the attention mechanism in DocLLM that allows for selective focus called?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6603098848955539,
          "content": "Figure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically usi..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6546677846612283,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 47,
          "relevance_score": 0.6469459370646718,
          "content": "Published as a conference paper at ICLR 2025\nFigure 44: An error case of SelfReflection Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n47"
        }
      ],
      "pred_answer": "the attention mechanism in docllm is extended to capture dependencies between text semantics and spatial layouts, allowing it to selectively focus on certain parts of the input. this is referred to as a \"disentangled spatial attention mechanism\" that captures cross-alignment between text and layout modalities.",
      "ground_truth": [
        [
          "disentangle",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-source LLM was adapted to build DocLLM-7B?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6909151178076519,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 12,
          "relevance_score": 0.6840691019035336,
          "content": "Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Si..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6682820118659288,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        }
      ],
      "pred_answer": "there is no mention of an open-source llm being adapted to build docllm-7b in the provided context. the text only mentions that docllm demonstrates superior performance compared to other models, including llama2, but it does not provide information about the specific model used to build docllm-7b.",
      "ground_truth": [
        [
          "llama2",
          "7b"
        ],
        [
          "llama 2",
          "7 b"
        ],
        [
          "llama 2",
          "7b"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What medium does the ChartMimic benchmark evaluate a model's ability to turn to code?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.8004534992256572,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.7722958130970448,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7261134194686671,
          "content": "Published as a conference paper at ICLR 2025\n5\nRELATED WORK\nLarge Multimodal Models.\nThe proprietary LMMs such as GPT-4o (OpenAI, 2024), Gem-\nini (Google, 2023), and Claude-3 (Anthropic, 2024) have enabled complex multimodal interac-\ntions. Similarly, emerging open-weight LMMs such as LLaVA (Xu et al., 2024; Li et al., 2024a),\nInternVL (Chen et al., 2023), Qwen-VL (Bai et al., 2023), DeepSeek-VL (Lu et al., 2024) have\ncontributed to the community. Despite these advancements, the effective evalua..."
        }
      ],
      "pred_answer": "charts",
      "ground_truth": [
        [
          "chart"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "How many total (figure, instruction, code) triplets were annotated for ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6981641740649852,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.6917231503115083,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 4,
          "relevance_score": 0.6911449113707079,
          "content": "Published as a conference paper at ICLR 2025\nTable 1: Statistics of ChartMimic. We measure code length in terms of tokens, utilizing the Llama3\ntokenizer. In the level count, \u201cA/B/C\u201d denotes the number of \u201ceasy/medium/hard\u201d level, respectively.\nType\nBar\nLine\nErrorBar\nHeatmap\nBox\nScatters\nHist\nRadar\n3D\nPie\nErrorPoint\nViolin\nCount\n320\n280\n120\n120\n100\n100\n80\n80\n80\n80\n80\n80\nSubcategories\n16\n8\n12\n4\n6\n4\n3\n6\n5\n8\n5\n3\nCode Length (AVG.)\n689.6\n794.0\n681.2\n685.8\n689.0\n655.0\n529.6\n779.8\n655.4\n418.4\n624.3\n97..."
        }
      ],
      "pred_answer": "1200 high-quality seed data triplets were initially established for chartmimic, comprising 600 triplets for the direct mimic task and 600 triplets for the customized mimic task. following manual data augmentation, this number was increased to a total of 4800 triplets.",
      "ground_truth": [
        [
          "4,800"
        ],
        [
          "4800"
        ],
        [
          "four thousand, eight hundred"
        ],
        [
          "four thousand eight hundred"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What are the two tasks defined in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7336765125644148,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.7082085401154046,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 2,
          "relevance_score": 0.6814017147656098,
          "content": "Published as a conference paper at ICLR 2025\nFigure 1: The real-world example. LMMs assist scientists and researchers in understanding, inter-\npreting and creating charts during the reading and writing of academic papers. These models serve as\nassistants that enhance the comprehension and presentation of data in scholarly communications.\nTo this end, we present ChartMimic (Fig. 2), a multimodal code generation benchmark. ChartMimic\nis characterized by its (1) information-intensive visual inputs,..."
        }
      ],
      "pred_answer": "the two tasks defined in chartmimic are direct mimic and customized mimic. the direct mimic task requires lmms to directly generate code that can reproduce a provided chart, assessing their visual comprehension and reasoning capabilities. the customized mimic task involves generating code that incorporates new data specified in instructions, challenging lmms to adapt and apply their understanding of charts and coding skills.",
      "ground_truth": [
        [
          "direct",
          "custom"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Which open-weights model achieved the highest GPT-4o score on the Direct Mimic and Customized Mimic tasks?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 36,
          "relevance_score": 0.8264304683695549,
          "content": "Published as a conference paper at ICLR 2025\nFigure 34: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (bar_28).\nStability of Evaluation with GPT-4o Score. We conduct the high-level evaluation for GPT-4o on\nthe Direct Mimic task for 5 times to assess the stability of GPT-4o Score. The result indicate a mean\nGPT-4o Score of 83.4 with a standard deviation of 0.08, demonstrating the stability of GPT-4o Score.\nCost of Evaluation with GPT-4o Score. A single-round evaluation with GPT-4..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 6,
          "relevance_score": 0.8171194332751512,
          "content": "Recently, MMCode (Li et al., 2024b) attempted to create a benchmark for\nmultimodal code generation, but the vision inputs for their task are still overly simple and only have a\nsingle pass rate evaluation metric. Design2Code (Si et al., 2024) and Plot2Code (Wu et al., 2024a)\nare the most similar ones to ours. Although they use multi-level evaluation metrics like ours, their\ntest is crawled directly from the internet or existing datasets, which may pose a risk of data leakage.\nOur ChartMimic benc..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.7875147839999173,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        }
      ],
      "pred_answer": "internvl2-llama3-76b achieved the highest overall score among open-weight models, with a score of 58.5 in direct mimic and 64.7 in customized mimic.",
      "ground_truth": [
        [
          "internvl2",
          "llama3",
          "76"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "Name one of the four low-level evaluation metrics used in ChartMimic.",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7771840312589479,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7538327651934523,
          "content": "They are then required to modify the corresponding code and instructions, leading to the\n600 (figure, code, instruction) triplets for the Customized Mimic task. Consequently, we establish the\nChartMimic benchmark, comprising 1, 200 high-quality seed data.\nData Augmentation. Following the development of seed triplets, we initiate a process of manual data\naugmentation. Annotators are tasked with altering various elements of each seed triplet, including\ndata, color schemes, mark styles, etc., to pr..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.7487110036277125,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        }
      ],
      "pred_answer": "one of the low-level evaluation metrics used in chartmimic is text.",
      "ground_truth": [
        [
          "text"
        ],
        [
          "layout"
        ],
        [
          "type"
        ],
        [
          "color"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What maximum accuracy did state-of-the-art models achieve on normal ENIGMAEVAL puzzles? Answer as a single value.",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.736779830362694,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 5,
          "relevance_score": 0.7194737435504415,
          "content": "intervention. The standardization addressed several key challenges: (a) removing source-identifying headers and\nfooters to prevent shortcut solutions, (b) preserving complex layouts (particularly in grid-based puzzles where\nstructural elements like lines and dots are separate from textual content), and (c) ensuring accurate text extraction\nfrom non-standard formatting. A similar curation process was applied to puzzle solutions: after initial automated\nextraction from PDF and HTML pages, each sol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 15,
          "relevance_score": 0.7174667730347097,
          "content": "B.4\nAnswer Extraction\nWe determine the expected answer format through manual annotations, with three possible patterns:\n1. Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer>\n2. Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2>\n3. Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...\nThis structured approach to answer formats allows us to extract answers consistently a..."
        }
      ],
      "pred_answer": "7.0",
      "ground_truth": [
        [
          "7.0%"
        ],
        [
          "7%"
        ],
        [
          "7.0 %"
        ],
        [
          "7 %"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "How many puzzles are in the ENIGMAEVAL normal split?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.713612456759779,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 15,
          "relevance_score": 0.7113666459909379,
          "content": "B.4\nAnswer Extraction\nWe determine the expected answer format through manual annotations, with three possible patterns:\n1. Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer>\n2. Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2>\n3. Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...\nThis structured approach to answer formats allows us to extract answers consistently a..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.6978338948749748,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        }
      ],
      "pred_answer": "there is no mention of the number of puzzles in the enigmaeval normal split.",
      "ground_truth": [
        [
          "949"
        ],
        [
          "nine hundred forty nine"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "What does ENIGMAEVAL call a puzzle that combines answers or elements from multiple previous puzzles to reveal a final solution?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7379104959811863,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.73509160291395,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.7153848097332415,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        }
      ],
      "pred_answer": "a metapuzzle.",
      "ground_truth": [
        [
          "meta puzzle"
        ],
        [
          "metapuzzle"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Text-Only",
      "question": "What does PCFM stand for?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6315290144758492,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6255160082802426,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 6,
          "relevance_score": 0.6212768866677881,
          "content": "3.4.1\nTradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with suffici..."
        }
      ],
      "pred_answer": "pcfm is not explicitly stated to stand for anything in the given text snippet from 2506.04171v1.pdf. however, it appears to be related to a method or approach, possibly an acronym, but its full form or meaning is not provided.",
      "ground_truth": [
        [
          "physics",
          "constrained",
          "flow",
          "matching"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "Which methods propose gradient-based constraint enforcement during sampling, but often require backpropagation through expensive PDE operators?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2506.04171v1.pdf",
          "page_number": 3,
          "relevance_score": 0.764394812698855,
          "content": "Table 1: Comparison of generation methods motivated by constraint guidance or enforcement.\nZero-shot\nContinuous Guidance\nHard Constraint\nComplex Constraints\nConditional FFM [11]\n\u2717\n\u2713\n\u2713\n\u2717\nDiffusionPDE [10]\n\u2713\n\u2717\n\u2717\n\u2713\nD-Flow [35]\n\u2713\n\u2713\n\u2717\n\u2713\nECI [12]\n\u2713\n\u2713\n\u2713\n\u2717\nPCFM (Ours)\n\u2713\n\u2713\n\u2713\n\u2713\n4. We evaluate our method on challenging PDEs exhibiting shocks, discontinuities, and sharp\nspikes\u2014settings in which standard flow matching models typically fail. Our approach\nimproves the accuracy of such models at inference time ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2506.04171v1.pdf",
          "page_number": 4,
          "relevance_score": 0.7459339427625014,
          "content": "The vector field v\u03b8 is parameterized by a Neural\nOperator [23, 24] which is forward compatible to operate on functional spaces. The resulting\ngenerative model approximates the target measure \u03c01 \u2248\u03bd over the support UF through sufficient\ntraining (see Appendix G.2 for further details). In this work, we focus on augmenting such pre-trained\nmodels to enforce additional physical constraints at inference time, without requiring retraining.\n3.2.1\nConstraint Types in PDE Systems\nConstraints in PDE-gover..."
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 7,
          "relevance_score": 0.7377992636125158,
          "content": "D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of ..."
        }
      ],
      "pred_answer": "diffusionpde and d-flow propose gradient-based constraint enforcement during sampling, but often require backpropagation through expensive pde operators.",
      "ground_truth": [
        [
          "diffusionpde, d flow"
        ],
        [
          "diffusion pde, d flow"
        ],
        [
          "diffusionpde, dflow"
        ],
        [
          "diffusion pde, dflow"
        ],
        [
          "diffusionpde, d-flow"
        ],
        [
          "diffusion pde, d-flow"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Text-Only",
      "question": "In ENIGMAEVAL, how do the authors justify excluding audio/video puzzles?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 4,
          "relevance_score": 0.7062937603740322,
          "content": "Table 1: Overview of puzzle sources in ENIGMAEVAL, organized by difficulty split. N is the number of puzzles.\nSplit (N)\nPuzzle Source (N)\nDescription\nNormal\n(949)\nPuzzledPint (838) [1]\nMonthly beginner-friendly puzzle-solving event typically consisting of\naround seven puzzles including a meta-puzzle. Together, these puzzles\nshould \u201cbe solvable in under two hours by a team of inexperienced puz-\nzlers who are socializing, drinking, and eating in a pub or restaurant\u201d1.\nCS50x Puzzle Day (41)\n[2]\nAnn..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6958200464162476,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6746035535654956,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        }
      ],
      "pred_answer": "the authors excluded puzzles that rely on audio or video clues because, at the time of writing, only a few frontier models can process audio and video clues effectively.",
      "ground_truth": [
        [
          "parsing limitations"
        ],
        [
          "scope"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "In Table 11 of the Multimodal-CoT Paper, what size of Multimodal-CoT was used?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6512941121427619,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6455530292766115,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 24,
          "relevance_score": 0.6417120311877401,
          "content": "Published as a conference paper at ICLR 2025\nFigure 10: An Example of the Customized Mimic task (CB_29), showing the (figure, instruction,\ncode) triplet. Additionally, we also display the ground truth figure rendered from the code for\nillustration.\n24"
        }
      ],
      "pred_answer": "there is no information provided about the file \"2302.00923v5.pdf\" or table 11 of the multimodal-cot paper, so it's not possible to determine the size of multimodal-cot used. the available information only mentions a different file, \"2406.09961v2.pdf\", and its contents.",
      "ground_truth": [
        [
          "738M"
        ],
        [
          "738"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What template does DocLLM use for CLS Internal Classification?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.7070829237739452,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6680855185974732,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.653047059380375,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "the document doesn't explicitly mention the template used by docllm for cls internal classification. it only mentions that three instruction templates are leveraged for key information extraction (kie): extraction, internal classification, and mcq, as shown in table 1, but it does not provide information about the specific template used for cls internal classification.",
      "ground_truth": [
        [
          "{document} What type of document is this?"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In DocLLM, how many total documents were used in pre-training?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.697322679097778,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 7,
          "relevance_score": 0.6749640352157906,
          "content": "Table 2: Pre-training dataset statistics.\nNo. of Docs\nNo. of Pages\nNo. of Total Tokens\nCDIP\n5,092,636\n16,293,353\n3,637,551,478\nDocBank\n499,609\n499,609\n228,362,274\nTotal\n5,592,245\n16,792,962\n3,865,913,752\nTable 3: Instruction-tuning dataset statistics.\nTasks\nNo. of Training\nNo. of Testing\nVQA\n145,090\n24,347\nNLI\n104,360\n12,720\nKIE\n236,806\n38,039\nCLS\n149,627\n21,813\nTotal\n635,883\n96,919\nextraction template, we add the \u201cNone\u201d answer if the key does not exist in the given document. To increase diversi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6571049972639218,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "5,592,245",
      "ground_truth": [
        [
          "5,592,245"
        ],
        [
          "5592245"
        ],
        [
          "five million five hundred ninety two thousand two hundred forty five"
        ],
        [
          "five million, five hundred ninety two thousand, and forty five"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, what is the Testmini test-set execution rate of InternVL2-26B on the Direct Mimic task?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.8063732701265287,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.723961477173818,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.6863052812678508,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        }
      ],
      "pred_answer": "the provided context does not contain specific information about the testmini test-set execution rate of internvl2-26b on the direct mimic task. however, it mentions that reports 7 and 8 discuss the performance of gpt-4o and internvl2 series lmms (including 26b) on direct mimic and customized mimic tasks, but the actual performance values are not provided in the given context.",
      "ground_truth": [
        [
          "69.3"
        ],
        [
          "sixty nine point three"
        ],
        [
          "sixty-nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggests 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot MiniImageNet with 8 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.7434720029251222,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.7387828229855788,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 14,
          "relevance_score": 0.6888246499426696,
          "content": "Transformer parameters.\nSince the weight tensors of each layer are generally different, our per-layer transformers were\nalso different. The key, query and value dimensions of the transformer were chosen to be equal to a pre-de\ufb01ned fraction\n\u03bd of the input embedding size, which in turn was a function of the label, image and activation embedding sizes and the\nsizes of the weight slices. The inner dimension of the \ufb01nal fully-connected layer in the transformer was also chosen\nusing the same approach...."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "The HyperTransformer paper suggest 5-shot OMNIGLOT with 8 channels has better accuracy than 1-shot OMNIGLOT with 64 channels. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.7347848592660252,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.7304969195845664,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.6732724868220402,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nD. Additional Supervised Experiments\nWhile the advantage of decoupling parameters of the weight generator and the generated CNN model is expected to vanish\nwith the growing CNN model size, we compared our approach to two other methods, LGM-Net (Li et al., 2019b) and LEO\n(Rusu et al., 2019), to verify that our approach can match their performance on suf\ufb01ciently large models.\nFor our comparison with the LGM-Net method, we used the same image..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the PCFM paper, what is the MMSE/10^-2 for D-Flow?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2506.04171v1.pdf",
          "page_number": 27,
          "relevance_score": 0.7268814846583977,
          "content": "10\n4\n10\n3\n10\n2\n10\n1\n100\nPenalty weight \n0.005\n0.006\n0.007\n0.008\n0.009\n0.010\n0.011\n0.012\nError\nMMSE and SMSE vs \nMMSE\nSMSE\n10\n4\n10\n3\n10\n2\nPenalty weight \n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\nError\nMMSE and SMSE vs \nMMSE\nSMSE\nFigure 9: Effect of penalty weight \u03bb on MMSE and SMSE for the Reaction-Diffusion dataset. Left:\n10 flow matching steps. Right: 100 flow matching steps.\nnumber of steps\u2014for instance, in scenarios where evaluating the vector field v\u03b8 is computationally\nexpensive.\nHowever, ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2506.04171v1.pdf",
          "page_number": 7,
          "relevance_score": 0.6794449585051984,
          "content": "D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 6,
          "relevance_score": 0.6486462781742366,
          "content": "3.4.1\nTradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with suffici..."
        }
      ],
      "pred_answer": "the results for d-flow are omitted due to numerical instabilities, so the mmse value for d-flow is not provided.",
      "ground_truth": [
        [
          "1.97"
        ],
        [
          "one point ninety seven"
        ],
        [
          "one point nine seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "In ChartMimic, which chart type has the largest average code token length?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7158328449870294,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 52,
          "relevance_score": 0.6938095061481103,
          "content": "Published as a conference paper at ICLR 2025\nH\nCORRELATION WITH CHART UNDERSTANDING AND CODE GENERATION\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n50\n60\n70\n80\n90\nChartQA\nr = 0.6877\np-value = 6.56e-03\n(a) Correlation with Chart Understanding\n20\n30\n40\n50\n60\n70\n80\nChartMimic\n40\n60\n80\n100\nHumanEval\nr = 0.8714\np-value = 4.92e-05\n(b) Correlation with Code Generation\nFigure 48: Performance correlation of ChartMimic with benchmarks assessing chart understanding\nand code generation capabilities.\nIn an effort to she..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 20,
          "relevance_score": 0.6905889926662062,
          "content": "Published as a conference paper at ICLR 2025\nbased on both its frequency and sophistication level (designated as 1-3 points for low/medium/high\ncomplexity, respectively). This multi-dimensional scoring approach ensures a thorough evaluation of\nthe visual complexity inherent in each chart.\nImplementation Complexity We incorporate code complexity as a quantitative metric, measured\nprimarily through code length and structural intricacy. This parameter effectively captures the\ntechnical sophisticati..."
        }
      ],
      "pred_answer": "the chart type with the largest average code token length in chartmimic is not explicitly stated in the provided context. however, based on the information given about the complexity levels of charts, it can be inferred that charts categorized as \"hard\" might have more complex visual elements and advanced implementation requirements, which could potentially result in longer code lengths. examples of such charts include complex dual-subplot bar charts with extensive color schemes and substantial code complexity.",
      "ground_truth": [
        [
          "plot in plot"
        ],
        [
          "pip"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Tables",
      "question": "What accuracy did Multimodal-CoT get on the MMMU benchmark?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6894435880763968,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6882783744691446,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 9,
          "relevance_score": 0.6618091723285339,
          "content": "82\n70.98\n83.18\n77.37\n67.92\n86.13\n80.72\n74.03\n78.31\nGPT-4 (Lu et al., 2023)\n-\n85.48\n72.44\n90.27\n82.65\n71.49\n92.89\n86.66\n79.04\n83.99\nChameleon (ChatGPT) (Lu et al., 2023)\u2020\n-\n81.62\n70.64\n84.00\n79.77\n70.80\n86.62\n81.86\n76.53\n79.93\nChameleon (GPT-4) (Lu et al., 2023)\u2020\n-\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaMA-Adapter (Zhang et al., 2023a)\u2020\n6B\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nLLaVA (Liu et al., 2023)\u2020\n13B\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nIns..."
        }
      ],
      "pred_answer": "there is no mention of the mmmu benchmark in the provided text from 2302.00923v5.pdf. the text only mentions results on the scienceqa and a-okvqa benchmarks.",
      "ground_truth": [
        [
          "28.7"
        ],
        [
          "twenty eight point seven"
        ],
        [
          "twenty-eight point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "In the NPHardEval4V paper, what is LLaVa's instruction following performance?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6887023552200779,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 12,
          "relevance_score": 0.6641080682164906,
          "content": "Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[5] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli,\nBaptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming\ncurated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023.\n[6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Si..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 25,
          "relevance_score": 0.6565329720289612,
          "content": "7 and 8 reports the performance of GPT-4o and InternVL2 series LMMs (2B, 8B, 26B, Llama3-\n76B) on Direct Mimic and Customized Mimic task. The minor differences between scores on the test\nsubset and the testmini subset suggest that testmini effectively mirrors the test subset, serving as a\nvaluable evaluation subset for model development, especially for those who have limited computing\nresources.\n25"
        }
      ],
      "pred_answer": "the provided text does not mention \"nphardeval4v\" or \"llava\". however, it does discuss instruction-following effective rate (er) for mllm and llm models. for llm, the instruction-following effective rate is mentioned in figure 7, but the actual performance value is not provided in the given text.",
      "ground_truth": [
        [
          "0.75"
        ],
        [
          "zero point seven five"
        ],
        [
          "zero point seventy five"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "What was the highest TSP reasoning score across all complexity levels in NPHardEval4V? Answer as a single number only. Omit Reasoning Steps.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.7300783862735991,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.7206303147537536,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7119013842914124,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        }
      ],
      "pred_answer": "no specific tsp reasoning score is mentioned in the provided context.",
      "ground_truth": [
        [
          "0"
        ],
        [
          "zero"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "How many samples were used in testing while instruct-tuning for VQA in DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6937062569194983,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.676652766039261,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6721744289029528,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "the specific number of samples used in testing while instruction-tuning for vqa in docllm is not directly stated. however, it is mentioned that the vqa instruction-tuning data mix includes several datasets such as docvqa, wikitablequestions (wtq), visualmrc, dude, and bizdocs2. the details about the exact number of samples from these datasets used for testing are not provided in the given context.",
      "ground_truth": [
        [
          "24,347"
        ],
        [
          "24347"
        ],
        [
          "twenty four thousand three hundred forty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Tables",
      "question": "What feature shape is used for ViT in the Multimodal-CoT paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6586438706110909,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 24,
          "relevance_score": 0.6463583317434439,
          "content": "Published as a conference paper at ICLR 2025\nFigure 10: An Example of the Customized Mimic task (CB_29), showing the (figure, instruction,\ncode) triplet. Additionally, we also display the ground truth figure rendered from the code for\nillustration.\n24"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6452341112950603,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        }
      ],
      "pred_answer": "there's no mention of \"2302.00923v5.pdf\" or the specific details about the feature shape used for vit in the multimodal-cot paper. the provided information only discusses a different pdf file, \"2406.09961v2.pdf\", and its contents related to figures and examples from a conference paper published at iclr 2025.",
      "ground_truth": [
        [
          "145",
          "1024"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what is the input to the f1 block?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 46,
          "relevance_score": 0.6443757654381523,
          "content": "Published as a conference paper at ICLR 2025\nF.3\nCASES OF DIFFERENT PROMPTING METHODS\nWe provide cases of HintEnhanced, SelfReflection and Scaffold Prompting in Figs. 43 to 46. For an\nanalysis of error cases related to Direct Prompting, please refer to Appendix I.\nFigure 43: An error case of HintEnhanced Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n46"
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6345560438906996,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6271802558769537,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        }
      ],
      "pred_answer": "the input to the first layer (layer 1) has an original dimensionality of 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6.",
      "ground_truth": [
        [
          "cnn layer 1"
        ],
        [
          "convolutional neural network layer 1"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what type of samples contain Class Embeddings?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.7655900064180168,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7296063304124846,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6690461771586075,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "episodes contain class embeddings.",
      "ground_truth": [
        [
          "labeled samples"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper's Figure 1, what magnetic pole is shown on the far right?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 19,
          "relevance_score": 0.6112421167670828,
          "content": "Both magnet sizes and distance affect the magnitude of the\nmagnetic force. The sizes of the magnets in Pair 1 are the same as in\nPair 2. The distance between the magnets is also the same. So, the\nmagnitude of the magnetic force is the same in both pairs.\nAnswer: The answer is (C).\nVision\n+ Vision Features\nOptions:\n(B) The magnitude of the magnetic force is smaller in Pair 1.\n(A) The magnitude of the magnetic force is smaller in Pair 2.\n(C) The magnitude of the magnetic force is the same in both ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6032717706218819,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.5992799668023377,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        }
      ],
      "pred_answer": "the provided information does not mention figure 1 from the multimodal-cot paper or describe a magnetic pole on the far right. it discusses the magnitude of magnetic force in pairs of magnets and mentions figures related to a two-stage framework for generating rationales and predicting answers, but it does not provide details about specific magnetic poles in any figure.",
      "ground_truth": [
        [
          "north"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "In figures in the Multimodal-CoT paper, what step creates the Rationale? Please omit reasoning steps.",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 4,
          "relevance_score": 0.7106324097341903,
          "content": "Table 2: Effects of CoT in the one-stage setting.\nMethod\nFormat\nAccuracy\nNo-CoT\nQCM\u2192A\n81.63\nReasoning\nQCM\u2192RA\n69.32\nExplanation\nQCM\u2192AR\n69.68\nAs an example shown in Figure 1, the model takes\nthe concatenation of tokens of the question text (Q),\nthe context text (C), and multiple options (M) as\nthe input. To study the effect of CoT, we compare\nthe performance with three variants: (i) No-CoT which\npredicts the answer directly (QCM\u2192A); (ii) Reasoning\nwhere answer inference is conditioned to the ratio..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 46,
          "relevance_score": 0.7087444883226285,
          "content": "Published as a conference paper at ICLR 2025\nF.3\nCASES OF DIFFERENT PROMPTING METHODS\nWe provide cases of HintEnhanced, SelfReflection and Scaffold Prompting in Figs. 43 to 46. For an\nanalysis of error cases related to Direct Prompting, please refer to Appendix I.\nFigure 43: An error case of HintEnhanced Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n46"
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7052261423897636,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        }
      ],
      "pred_answer": "qcm\u2192ra and qcm\u2192ar create the rationale.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the Multimodal-CoT paper, what is used to extract vision components?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6955843614124654,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6704581962208215,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6679206164679674,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        }
      ],
      "pred_answer": "the method fine-tunes language models to fuse vision and language representations, but it does not explicitly state what is used to extract vision components. however, it mentions that incorporating vision features alleviates the problem of hallucination and boosts convergence, suggesting that some form of vision feature extraction is involved, but the specifics are not provided.",
      "ground_truth": [
        [
          "vision transformer",
          "vit"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer model diagram, what do placeholder tokens represent?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6277774583721363,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6250558720685002,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6247311718314982,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        }
      ],
      "pred_answer": "the provided information does not mention placeholder tokens in the hypertransformer model diagram. it discusses umap embeddings of weights generated by the hypertransformer and highlights episodes containing specific classes from tieredimagenet, but it does not provide details about placeholder tokens.",
      "ground_truth": [
        [
          "transformed tokens",
          "cnn weights"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In Figure 2 of the Multimodal-CoT paper, The answer stays the same when the vision component is introduced. Answer \"true\" or \"false\"",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7176027403646551,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 19,
          "relevance_score": 0.7131146607514383,
          "content": "Both magnet sizes and distance affect the magnitude of the\nmagnetic force. The sizes of the magnets in Pair 1 are the same as in\nPair 2. The distance between the magnets is also the same. So, the\nmagnitude of the magnetic force is the same in both pairs.\nAnswer: The answer is (C).\nVision\n+ Vision Features\nOptions:\n(B) The magnitude of the magnetic force is smaller in Pair 1.\n(A) The magnitude of the magnetic force is smaller in Pair 2.\n(C) The magnitude of the magnetic force is the same in both ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.6950703880795551,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        }
      ],
      "pred_answer": "true",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "In the HyperTransformer paper, what visibly distinguishes only generating logits from generating full CNN weights?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 19,
          "relevance_score": 0.7283399798619945,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 1\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 2\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 3\n0.025\n0.050\nFigure 8. Learned attention maps for 2-layer 4-channel CNN network generated with 1 head, 1 layer transformer for 5-shot MINIIMA-\nGENET.\nTransformer Layer 1\nTransformer Layer 2\nWeights\nLabel 0\nLabel 1\nLabel ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 4,
          "relevance_score": 0.7279847769397884,
          "content": "In other words, these learned layers are modi\ufb01ed\nduring the training phase and remain static during the evalu-\nation phase (i.e. not dependent of the support set). In our ex-\nperiments generated CNN models contain a set of convolu-\ntional layers and a \ufb01nal fully-connected logits layer. Here \u03b8\u2113\nare the parameters of the \u2113-th layer and L is the total number\nof layers including the \ufb01nal logits layer (with index L). The\nweights are generated layer-by-layer starting from the \ufb01rst\nlayer: \u03b81(\u03c4) \u2192\u03b82(\u03b81;..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.7090509936771306,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        }
      ],
      "pred_answer": "in the hypertransformer paper, what visibly distinguishes only generating logits from generating full cnn weights is the visualization of convolutional kernels. when only the logits are generated, the convolutional kernels appear to be more random and less structured, whereas when the full cnn weights are generated, the kernels exhibit a more organized and spatially coherent pattern. this distinction can be observed in figures such as figure 13, which shows the visualized convolutional kernels for different layer weight allocation scenarios, including generated and trained layers.",
      "ground_truth": [
        [
          "varied convolution patterns"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "What are the 4 key elements of DocLLM?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6956075781773822,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.683532805583473,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.6499816391741164,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        }
      ],
      "pred_answer": "the four key elements of docllm are: \n1. disentangled spatial features, \n2. the block infilling pre-training objective, \n3. the masking strategy used for decoding, \nand its architecture that allows it to derive significant benefits even in smaller models like docllm-1b.",
      "ground_truth": [
        [
          "ocred document"
        ],
        [
          "llm extension"
        ],
        [
          "pre-training"
        ],
        [
          "instruction tuning"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What prompt template does DocLLM use for VQA extraction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 6,
          "relevance_score": 0.8249634136968789,
          "content": "document\ntype names) to classify extracted values (resp. entire documents). We introduce the templates in detail as follows.\nVisual Question Answering. We collect DocVQA [43], WikiTableQuestions (WTQ) [44], VisualMRC [45], DUDE\n[46], and BizDocs2, to compose the VQA instruction-tuning data mix. We use one instruction template to build our\nSFT inputs for VQA, as shown in table 1. An example prompt derived from DocVQA would read: \"{document} What\nis the deadline for scientific abstract submission ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6881602264632694,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.6803988062616123,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        }
      ],
      "pred_answer": "one instruction template is used to build sft inputs for vqa, with an example prompt derived from docvqa reading: \"{document} what is the deadline for scientific abstract submission for acog - 51st annual clinical meeting?\"",
      "ground_truth": [
        [
          "{document} {question}"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Images",
      "question": "Referencing the Knapsack Problem figure in NPHardEval4V, what is the id of the item with weight 8? Do not include step numbers in your reasoning.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.655350839238511,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 55,
          "relevance_score": 0.6465489176137682,
          "content": "Published as a conference paper at ICLR 2025\njTrans\nPalmTree\nCLAP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) Ground-truth figure (violin_4)\njTrans\nPalmTree\nCLAP\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Generated figure\nFigure 54: Error Case 6. In this case, the errors include text-related errors of the Extraneous type,\ntype-related errors of the Confusion type and color-related errors of the Similar type.\n55"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6443636040748648,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "there is no information provided about the knapsack problem figure in nphardeval4v or an item with weight 8 from 2403.01777v2.pdf, as the given context only mentions a different file name, 2406.09961v2.pdf.",
      "ground_truth": [
        [
          "4"
        ],
        [
          "four"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What is the Knapsack Capacity given in the example in NPHardEval4V?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 39,
          "relevance_score": 0.7036836770338528,
          "content": "Published as a conference paper at ICLR 2025\nFigure 37: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (CB_29).\n39"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.703368834407633,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6991410410123198,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "there is no information provided about the file \"2403.01777v2.pdf\" or \"nphardeval4v\", so it's not possible to determine the knapsack capacity. the available information only mentions a different file, \"2406.09961v2.pdf\", which discusses gpt-4o\u2019s scoring results for customized mimic and direct mimic examples.",
      "ground_truth": [
        [
          "40"
        ],
        [
          "forty"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "At approximately how many training steps does the Detector observation start to outperform the CybORG observation in the Path To Autonomous Cyber Defense paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 3,
          "relevance_score": 0.7873183439063439,
          "content": "The agent\ntrained in CybORG is therefore replicating existing\ncapabilities and trying to learn new behaviors on top of\nthose existing capabilities. How might we change the\nobservation space to avoid replicating existing tools\u2019\ncapabilities?\nThe MITRE ATT&CK framework is used to define\nboth red agent actions and the data sources collected\nby sensors / logs relevant to detecting each type of\nattack. These data sources are called Data Compo-\nnentsg and they are used to determine probability of\ndete..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 3,
          "relevance_score": 0.7567163467580859,
          "content": "THEME/FEATURE/DEPARTMENT\ngames like chess, Go, or StarCraft, the rules are even\nmore well defined and the environments are somewhat\npredictable.\nCybersecurity cannot be reduced to a single game\nand the environment in which the agent operates may\nchange dynamically (not to mention that adversaries\nconstantly seek to rewrite the rules of the game). Defin-\ning the correct game for autonomous cyber agents\ntherefore presents two critical challenges: there is no\nsingle game and the game itself changes..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6958753221795542,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        }
      ],
      "pred_answer": "the provided context does not contain specific information about the number of training steps at which the detector observation starts to outperform the cyborg observation. the text discusses the concept and implementation of detectors as a new observation space but does not provide a detailed comparison or specific metrics such as training steps where one method outperforms the other.",
      "ground_truth": [
        [
          "40m"
        ],
        [
          "40 m"
        ],
        [
          "forty m"
        ],
        [
          "40 million"
        ],
        [
          "40,000,000"
        ],
        [
          "40000000"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Images",
      "question": "What outputs to the RL Agent in the Autonomous Cyber Defense Paper?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7013945059965007,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 8,
          "relevance_score": 0.6965166247869755,
          "content": "arXiv preprint arXiv:2108.09118,\n2021.\n13. Sanyam Vyas, John Hannay, Andrew Bolton, and\nProfessor Pete Burnap. Automated cyber defence:\nA review. arXiv preprint arXiv:2303.04926, 2023.\n14. Erich Walter, Kimberly Ferguson-Walter, and Ah-\nmad Ridley.\nIncorporating deception into cyber-\nbattlesim for autonomous defense. arXiv preprint\narXiv:2108.13980, 2021.\n15. Yunqiu Xu, Ling Chen, Meng Fang, Yang Wang,\nand Chengqi Zhang.\nDeep reinforcement learn-\ning with transformers for text adventure games.\nI..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 1,
          "relevance_score": 0.6638920608144542,
          "content": "Feature Article - Achieving Autonomous Cyber Defense\nThe Path To Autonomous Cyber Defense\nSean Oesch, Oak Ridge National Laboratory, Oak Ridge, TN, USA, oeschts@ornl.gov\nPhillipe Austria, Oak Ridge National Laboratory, austriaps@ornl.gov\nAmul Chaulagain, Oak Ridge National Laboratory, chaulagaina@ornl.gov\nBrian Weber, Oak Ridge National Laboratory, weberb@ornl.gov\nCory Watson, Oak Ridge National Laboratory, watsoncl1@ornl.gov\nMatthew Dixson, Oak Ridge National Laboratory, dixsonmk@ornl.gov\nAmir ..."
        }
      ],
      "pred_answer": "the output of the rl agent is a \"policy\" function that maps situations (a state) to actions in order to maximize a numerical reward signal. rewards may vary in frequency and complexity. the agent learns from interaction with an environment to achieve long-term goals, such as protecting a network against cyber attacks.",
      "ground_truth": [
        [
          "observation converter"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What portion of Hallucinations go unresolved in the Multimodal-CoT Paper?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7111115664289348,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 39,
          "relevance_score": 0.6600241433797577,
          "content": "Published as a conference paper at ICLR 2025\nFigure 37: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (CB_29).\n39"
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 14,
          "relevance_score": 0.6595085376434884,
          "content": "Dynamic fusion with intra- and inter-modality attention flow for visual question answering. In IEEE\nConference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20,\n2019, pp. 6639\u20136648. Computer Vision Foundation / IEEE, 2019. doi: 10.1109/CVPR.2019.00680.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. ArXiv preprint, abs/2206.06336, 2022.\nKaiming He, Xiangyu Zhang, ..."
        }
      ],
      "pred_answer": "the text does not provide a specific percentage or portion of hallucinations that go unresolved in the multimodal-cot paper. it only mentions that multimodal-cot is beneficial in mitigating hallucination and boosting convergence, but it does not quantify the extent to which hallucinations are resolved.",
      "ground_truth": [
        [
          "29.3"
        ],
        [
          "twenty nine point three"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "According to the GCP example figure in NPHardEval4V, what was the reasoning score that Gemini received for the first complexity class?",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.776091387681188,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.6860834716401443,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 8,
          "relevance_score": 0.6798683318413006,
          "content": "Mmmu: A massive multi-\ndiscipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of CVPR,\n2024.\n[15] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei\nChang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models\nin visual contexts. In International Conference on Learning Representations (ICLR), 2024.\n[16] Cristina Menghini, Diego Mares, Ernesto Hernandez, Dean Lee, Mike..."
        }
      ],
      "pred_answer": "the provided context does not contain information about a gcp example figure in nphardeval4v or the specific reasoning score that gemini received for the first complexity class. however, it is mentioned that gemini consistently outperforms its counterparts, including gpt-4v, across the board, and maintains a higher aggregated accuracy and a more effective instruction-following rate than gpt-4v.",
      "ground_truth": [
        [
          "0.37"
        ],
        [
          "zero point three seven"
        ],
        [
          "zero point thirty seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the HyperTransformer paper, performance increases as channel size increases.  Answer \"true\" or \"false\".",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.6868842199653261,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6637861552555375,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.6614981884599013,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Approximately how many questions have only 1 image in Raw PDF Format in EnigmaEval?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.6417680605536228,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6382872619467622,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 15,
          "relevance_score": 0.6262459159588514,
          "content": "B.4\nAnswer Extraction\nWe determine the expected answer format through manual annotations, with three possible patterns:\n1. Single answer: For puzzles requiring one word or short phrase, Format: Answer: <answer>\n2. Answer pairs: For solutions with exactly two elements, Format: Answer: <answer1>, <answer2>\n3. Comma-separated lists: For puzzles requiring multiple answers, Format: Answer: <answer1>, <answer2>, ...\nThis structured approach to answer formats allows us to extract answers consistently a..."
        }
      ],
      "pred_answer": "approximately most puzzles have a single key visual component.",
      "ground_truth": [
        [
          "800"
        ],
        [
          "eight hundred"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Consulting the figures and text of the Multimodal-CoT paper, we would expect the number of hallucinations to increase after adding the visual modality Answer \"true\" or \"false\".",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.752957809628082,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 9,
          "relevance_score": 0.7037409090472784,
          "content": "82\n70.98\n83.18\n77.37\n67.92\n86.13\n80.72\n74.03\n78.31\nGPT-4 (Lu et al., 2023)\n-\n85.48\n72.44\n90.27\n82.65\n71.49\n92.89\n86.66\n79.04\n83.99\nChameleon (ChatGPT) (Lu et al., 2023)\u2020\n-\n81.62\n70.64\n84.00\n79.77\n70.80\n86.62\n81.86\n76.53\n79.93\nChameleon (GPT-4) (Lu et al., 2023)\u2020\n-\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaMA-Adapter (Zhang et al., 2023a)\u2020\n6B\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nLLaVA (Liu et al., 2023)\u2020\n13B\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nIns..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 5,
          "relevance_score": 0.6950647995997019,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nGenerated Rationale: Will these magnets attract or repel? To\nfind out, look at which poles are closest to each other. The\nsouth pole of one magnet is closest to the south pole of the\nother magnet. Poles that are the same repel. So, these\nmagnets will repel each other.\nAnswer: The answer is (B).\nOptions:\n(B) repel\n(A) attract\nProblem\nBaseline\nQuestion: Will these magnets attract or repel each other?\nContext: Two magnets are placed a..."
        }
      ],
      "pred_answer": "false",
      "ground_truth": [
        [
          "false"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "The activation embeddings condition the generation of intermediate CNN layers in HyperTransformer. Answer \"true\" or \"false\"",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 4,
          "relevance_score": 0.7693634039415707,
          "content": "In other words, these learned layers are modi\ufb01ed\nduring the training phase and remain static during the evalu-\nation phase (i.e. not dependent of the support set). In our ex-\nperiments generated CNN models contain a set of convolu-\ntional layers and a \ufb01nal fully-connected logits layer. Here \u03b8\u2113\nare the parameters of the \u2113-th layer and L is the total number\nof layers including the \ufb01nal logits layer (with index L). The\nweights are generated layer-by-layer starting from the \ufb01rst\nlayer: \u03b81(\u03c4) \u2192\u03b82(\u03b81;..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.7492590950318484,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 19,
          "relevance_score": 0.7252931021069586,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 1\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 2\n0.05\n0.10\nWeights\nLabel 0\nLabel 1\nLabel 2\nLabel 3\nLabel 4\n0.0\n2.5\nCNN Layer 3\n0.025\n0.050\nFigure 8. Learned attention maps for 2-layer 4-channel CNN network generated with 1 head, 1 layer transformer for 5-shot MINIIMA-\nGENET.\nTransformer Layer 1\nTransformer Layer 2\nWeights\nLabel 0\nLabel 1\nLabel ..."
        }
      ],
      "pred_answer": "true.",
      "ground_truth": [
        [
          "true"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What mechanism shown in the Multimodal-CoT paper ensures that generated rationales attend to the visual input?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7518590475148363,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.7376608741577352,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7010785748336937,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        }
      ],
      "pred_answer": "the mechanism that ensures generated rationales attend to the visual input in the multimodal-cot paper is the two-stage framework, which fine-tunes language models to fuse vision and language representations. this allows the model to generate informative rationales that facilitate inferring final answers by incorporating well-founded rationales, leveraging feature-level interactions between vision and language inputs.",
      "ground_truth": [
        [
          "rationale generation"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What phenomenon depicted in the Multimodal-CoT paper and noted in the text explains the performance gap in Table 3?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6704246716846394,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6661223071615678,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6652903881346262,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        }
      ],
      "pred_answer": "there is no information provided about the file \"2302.00923v5.pdf\" or the \"multimodal-cot paper\", so it's not possible to determine the phenomenon that explains the performance gap in table 3. the available information only mentions two different pdf files, \"2406.09961v2.pdf\" and \"2403.01777v2.pdf\", but does not provide any details about \"2302.00923v5.pdf\".",
      "ground_truth": [
        [
          "hallucinated rationale"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Which CNN model size, as shown in Figure 3 and noted in Section 5.4 of HyperTransformer, makes generating only the final logits layer perform on-par with generating all layers? Do not include step numbers in your reasoning. Only include the final answer.",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.796953980810135,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 8,
          "relevance_score": 0.7940968154432809,
          "content": "We visualized the distribution of the weights generated by\nHT for different episodes by using UMAP (McInnes et al.,\n2018) embeddings of the generated weights for a 6-channel\nCNN model (see Fig. 4). We highlighted some of the classes\npresent in the evaluation set and while the general structure\nmay be hard to interpret, the distribution of the highlighted\nclasses is somewhat clustered indicating the importance of\nsemantic information for generated CNN weights. More\ndetails can be found in Appendi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 8,
          "relevance_score": 0.7806328490305229,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n4\n6\n8\nNumber of Channels\n80\n85\n90\n95\n100\nTraining and Test Accuracies\nlogits\nlogits (test)\nall\nall (test)\noracle\nFigure 3. 5-shot-20-way OMNIGLOT training/test accuracies as\na function of the CNN model complexity: only the \ufb01nal logits\nlayer being generated (logits), all layers being generated (all),\ntraining the model on all available samples for a random set of\nfew classes (oracle). A model that generates CNN weights by\nmemorizing all sam..."
        }
      ],
      "pred_answer": "8 channels",
      "ground_truth": [
        [
          "8"
        ],
        [
          "eight"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What are the placeholders in Figure 2 of HyperTransformer replaced with?",
      "required_files": [
        "2201.04182v3.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6635394915180973,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6497882318648515,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6362777189927645,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        }
      ],
      "pred_answer": "the provided information does not mention figure 2 of hypertransformer or any placeholders being replaced in it. it only mentions figure 11, which shows examples of classes from tieredimagenet used to highlight in umap embedding.",
      "ground_truth": [
        [
          "weight"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In Multimodal-CoT, what correction rate with vision features does Figure 3 report for hallucinated rationales?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7613574051364113,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 9,
          "relevance_score": 0.7343850657914927,
          "content": "82\n70.98\n83.18\n77.37\n67.92\n86.13\n80.72\n74.03\n78.31\nGPT-4 (Lu et al., 2023)\n-\n85.48\n72.44\n90.27\n82.65\n71.49\n92.89\n86.66\n79.04\n83.99\nChameleon (ChatGPT) (Lu et al., 2023)\u2020\n-\n81.62\n70.64\n84.00\n79.77\n70.80\n86.62\n81.86\n76.53\n79.93\nChameleon (GPT-4) (Lu et al., 2023)\u2020\n-\n89.83\n74.13\n89.82\n88.27\n77.64\n92.13\n88.03\n83.72\n86.54\nLLaMA-Adapter (Zhang et al., 2023a)\u2020\n6B\n84.37\n88.30\n84.36\n83.72\n80.32\n86.90\n85.83\n84.05\n85.19\nLLaVA (Liu et al., 2023)\u2020\n13B\n90.36\n95.95\n88.00\n89.49\n88.00\n90.66\n90.93\n90.90\n90.92\nIns..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.7036222206784161,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        }
      ],
      "pred_answer": "figure 3 is not mentioned in the provided context, and therefore, the correction rate with vision features for hallucinated rationales cannot be determined. however, it is mentioned that multimodal-cot demonstrates the ability to mitigate hallucination.",
      "ground_truth": [
        [
          "60.7"
        ],
        [
          "sixty point seven"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What mechanism in Multimodal-CoT handles vision-absent inputs?",
      "required_files": [
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.7040858702985565,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.6806924319724469,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2302.00923v5.pdf",
          "page_number": 12,
          "relevance_score": 0.6473329309918755,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nTable 10: Result comparison with different alignment strategies for multimodal interaction.\nModel\nAccuracy\nDirect Answering\n82.62\nUnimodal encoder\n85.31\nImage-grounded text encoder\n84.60\n6.6\nGeneralization to Other Multimodal Reasoning Benchmarks\nWe are interested in evaluating the generalization capability of Multimodal-CoT to datasets outside its\ntraining domain. For this purpose, we utilize the widely-recognized multimodal reaso..."
        }
      ],
      "pred_answer": "the mechanism that handles vision-absent inputs in multimodal-cot is not explicitly stated, but it can be inferred that the model is capable of processing language-only inputs. however, the effectiveness of this approach depends on the quality of the input and may be susceptible to errors. \n\nin cases where vision features are absent, the model may rely solely on language representations, which could lead to hallucination or incorrect rationales. the incorporation of vision features has been shown to alleviate these problems and improve performance. therefore, while multimodal-cot can handle vision-absent inputs, its effectiveness is enhanced when vision features are present.",
      "ground_truth": [
        [
          "blank features"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What layout-aware mechanism in DocLLM, guides token prediction?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7210937233983711,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6965590277032051,
          "content": "Figure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically usi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 5,
          "relevance_score": 0.6873134488752748,
          "content": "Furthermore, learning to infill text, where the prediction is conditioned on both prefix and suffix tokens rather than\nonly preceding tokens, can be beneficial. The infilling objectives enable contextually relevant completions, provide\nrobustness to OCR noise or misaligned tokens, and can better handle relationships between various document fields.\nHence we modify the standard pre-training objective to predict blocks of text given preceding and following text blocks.\nMost OCR engines can provide..."
        }
      ],
      "pred_answer": "the disentangled spatial attention mechanism in docllm guides token prediction by capturing cross-modal relationships between text semantics and spatial layouts. this mechanism extends the self-attention mechanism of standard transformers to include new attention scores that represent the alignments of fields in a form at various abstraction levels across the transformer layers, thus enhancing document understanding.",
      "ground_truth": [
        [
          "spatially constrained decoding"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 7 directly connected to vertex 1? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 55,
          "relevance_score": 0.5764246759291629,
          "content": "Published as a conference paper at ICLR 2025\njTrans\nPalmTree\nCLAP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) Ground-truth figure (violin_4)\njTrans\nPalmTree\nCLAP\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Generated figure\nFigure 54: Error Case 6. In this case, the errors include text-related errors of the Extraneous type,\ntype-related errors of the Confusion type and color-related errors of the Similar type.\n55"
        },
        {
          "chunk_id": 2,
          "file_name": "2403.01777v2.pdf",
          "page_number": 15,
          "relevance_score": 0.5758725837940537,
          "content": "[17] Lizhou Fan, Wenyue Hua, Lingyao Li, Haoyang Ling, and Yongfeng Zhang. Nphardeval:\nDynamic benchmark on reasoning ability of large language models via complexity classes.\narXiv preprint arXiv:2312.14890, 2023.\n[18] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and S Yu Philip. Multimodal large\nlanguage models: A survey. In 2023 IEEE International Conference on Big Data (BigData),\npages 2247\u20132256. IEEE, 2023.\n[19] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jint..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 5,
          "relevance_score": 0.5701089432791648,
          "content": "Complexity Class\nTask\nNP Hard (Most Complex)\nGraph Coloring Problem Optimization Version (GCP)\nTraveling Salesman Problem Optimization Version (TSP)\nMeeting Scheduling Problem (MSP)\nNP Complete\nKnapsack Problem (KSP)\nTraveling Salesman Problem Decision Version (TSP-D)\nGraph Coloring Problem Decision Version (GCP-D)\nP (Least Complex)\nShortest Path Problem (SPP)\nEdit Distance Problem (EDP)\nSorted Array Search (SAS)\nTable 1: Complexity classes and tasks\nGraph Data Transformation\nThe general constru..."
        }
      ],
      "pred_answer": "no.",
      "ground_truth": [
        [
          "no"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "In NPHardEval4V, is vertex 2 directly connected to vertex 5? Answer only yes or no.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.5726312586847524,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.5698328566396086,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 39,
          "relevance_score": 0.5691091466496969,
          "content": "Published as a conference paper at ICLR 2025\nFigure 37: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (CB_29).\n39"
        }
      ],
      "pred_answer": "no",
      "ground_truth": [
        [
          "yes"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Which parameter adjustment, depicted in Autonomous Cyber Defense, changes agent performance?",
      "required_files": [
        "2404.10788v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7436033719620857,
          "content": "An autonomous\nagent with a more specific goal and smaller action\nspace is easier to create, test, and deploy. Such\nagents make the overall system more modular, so\nthat individual agents can be swapped out or removed\nwith minimal impact to the overall performance of the\nsystem. It is also easier for a SOC to adopt because\nit will require less effort to integrate into their existing\ntoolset and it will require less input data which a given\nSOC may or may not currently be collecting.\nPLAYING THE RI..."
        },
        {
          "chunk_id": 2,
          "file_name": "2404.10788v1.pdf",
          "page_number": 2,
          "relevance_score": 0.6766645086241162,
          "content": "THEME/FEATURE/DEPARTMENT\nRL agents be leveraged by attackers to overwhelm or\nsneak past defenders? In order to help answer these\nquestions, researchers developed simulation 6,9,2,1 and\nemulation environments in which to train such au-\ntonomous RL agents, with some environments 5,10,8\nproviding both simulation and emulation.\nYet in spite of prior efforts to create autonomous\ncyber agents, they are still not used in practice. What\nis preventing the adoption of this new and promis-\ning approach to ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2404.10788v1.pdf",
          "page_number": 3,
          "relevance_score": 0.6696916771915487,
          "content": "The agent\ntrained in CybORG is therefore replicating existing\ncapabilities and trying to learn new behaviors on top of\nthose existing capabilities. How might we change the\nobservation space to avoid replicating existing tools\u2019\ncapabilities?\nThe MITRE ATT&CK framework is used to define\nboth red agent actions and the data sources collected\nby sensors / logs relevant to detecting each type of\nattack. These data sources are called Data Compo-\nnentsg and they are used to determine probability of\ndete..."
        }
      ],
      "pred_answer": "the observation space is a parameter that can be adjusted to change agent performance in autonomous cyber defense. specifically, changing the observation space to use detectors as the signal rather than network or host logs can provide the agent with a slightly noisier yet more realistic signal, and improve flexibility and simplify deployment. this adjustment can help avoid replicating the capabilities of existing detection tools and allow the agent to learn new behaviors on top of those existing capabilities.",
      "ground_truth": [
        [
          "detection probability"
        ],
        [
          "detection probabilities"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the DocLLM, which mechanism integrates spatial layout into the transformer block?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.6777307266010569,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 14,
          "relevance_score": 0.6493672751277937,
          "content": "Transformer parameters.\nSince the weight tensors of each layer are generally different, our per-layer transformers were\nalso different. The key, query and value dimensions of the transformer were chosen to be equal to a pre-de\ufb01ned fraction\n\u03bd of the input embedding size, which in turn was a function of the label, image and activation embedding sizes and the\nsizes of the weight slices. The inner dimension of the \ufb01nal fully-connected layer in the transformer was also chosen\nusing the same approach...."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.6203334602566437,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        }
      ],
      "pred_answer": "there is no mention of \"2401.00908v1.pdf\" or \"docllm\" in the provided context, and therefore it's not possible to provide an answer based on that information. the context only mentions files \"2201.04182v3.pdf\" and \"2406.09961v2.pdf\", but does not contain any relevant details about a \"docllm\" or its mechanisms.",
      "ground_truth": [
        [
          "disentangled",
          "spatial",
          "attention"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "What is the weight of the single path that is the answer to the SSP in Figure 3 of NPHardEval4V? Submit your answer as a single integer.",
      "required_files": [
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2502.08859v2.pdf",
          "page_number": 3,
          "relevance_score": 0.6350395720254296,
          "content": "(a) PuzzledPint \nHED LOST ALL HIS MARBLES\nAnswer\n(b) Labor Day Extravaganza \nBLU TACK\nAnswer\n(c) MIT Mystery Hunt \nOVER\nAnswer\n(d) CRUMS \nTOMATOES\nAnswer\nFigure 2: A sample of puzzles and solutions from (a) PuzzledPint (link, CC BY-NC-SA Intl. 4.0), (b) Labor Day\nExtravaganza (link, \u00a9Mark Halpin, all rights reserved), (c) MIT Mystery Hunt (link, \u00a9Jeck Lim, all rights reserved),\nand (d) CRUMS (link, CC BY-NC 3.0).\n3"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 55,
          "relevance_score": 0.6301230052971983,
          "content": "Published as a conference paper at ICLR 2025\njTrans\nPalmTree\nCLAP\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAccuracy\n(a) Ground-truth figure (violin_4)\njTrans\nPalmTree\nCLAP\nMethod\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAccuracy\n(b) Generated figure\nFigure 54: Error Case 6. In this case, the errors include text-related errors of the Extraneous type,\ntype-related errors of the Confusion type and color-related errors of the Similar type.\n55"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 37,
          "relevance_score": 0.6280755785882675,
          "content": "Published as a conference paper at ICLR 2025\nFigure 35: Example of GPT-4o\u2019s scoring results for a Customized Mimic example (bar_28).\n37"
        }
      ],
      "pred_answer": "there is no information provided about the file \"2403.01777v2.pdf\" or its contents, including figure 3 of nphardeval4v. the available information pertains to different files (2502.08859v2.pdf and 2406.09961v2.pdf) and does not mention the specific query details. therefore, it's impossible to determine the weight of the single path that is the answer to the ssp in figure 3 of nphardeval4v based on the given context.",
      "ground_truth": [
        [
          "5"
        ],
        [
          "five"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to DocLLM's prompt templates and its training architecture, which task would be most sensitive to missing spatial layout?",
      "required_files": [
        "2401.00908v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2401.00908v1.pdf",
          "page_number": 2,
          "relevance_score": 0.7163923188269599,
          "content": "Figure 1: Key elements of DocLLM. (1) Input documents contain text tokens and their bounding boxes. (2) Attention\nmechanism of LLMs are extended to capture dependencies between text semantics and spatial layouts. (3) Infilling text\nblocks is used as pre-training objective. (4) Task adaptation is performed on a newly collated dataset of instructions.\nintrinsically multi-modal. The spatial layout information is incorporated through bounding box coordinates of the\ntext tokens obtained typically usi..."
        },
        {
          "chunk_id": 2,
          "file_name": "2401.00908v1.pdf",
          "page_number": 9,
          "relevance_score": 0.7116535363608271,
          "content": "Among\nequivalent models (excluding GPT4), our model outperforms in 14 out of 16 datasets. Specifically, DocLLM demonstrates\nsuperior performance in layout-intensive tasks such as KIE and CLS. In VQA and NLI, its performance surpasses that\nof most multimodal language models, although it underperforms compared to GPT-4. GPT-4 outperforms DocLLM in\nVQA, possibly due to the higher complexity of reasoning and abstraction involved in VQA datasets compared to tasks\nlike KIE or CLS. DocLLM-1B demonstrat..."
        },
        {
          "chunk_id": 3,
          "file_name": "2401.00908v1.pdf",
          "page_number": 8,
          "relevance_score": 0.7041436029773199,
          "content": "The rationale behind this evaluation setting is to assess the performance of DocLLM when tasks are unchanged\nbut domains and layouts differ from train to test. We believe examining this setting in the DocAI field is\nrelevant because industry use cases usually encountered in practice revolve around VQA, KIE, and CLS,\nwhile document characteristics tend to change more often in production. We specifically isolate DocVQA,\nKLC, and BizDocs for STDD evaluation in order to (1) exclude at least one data..."
        }
      ],
      "pred_answer": "key information extraction (kie) would be the task most sensitive to missing spatial layout, as it relies heavily on understanding the structure and organization of documents, which is often conveyed through spatial layout. docllm's ability to capture cross-modal relationships between text and spatial layout makes it particularly well-suited for tasks like kie, where the position and size of fields in a form are crucial for accurate extraction.",
      "ground_truth": [
        [
          "kie"
        ],
        [
          "key information extraction"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "Which model is the best at accurately recreating color in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 9,
          "relevance_score": 0.743415088561225,
          "content": "As demonstrated in Fig. 4 (c), although GPT-4o\nexhibits remarkable capability in object recognition for natural images, it still struggles with scientific\ncharts, which contain more nuanced semantic meanings through visual logic.\nColor-related Errors. (1) Similar: The colors are not the same as the ground-truth colors but appear\nanalogous. (2) Different: The colors are entirely dissimilar to the ground-truth colors. As shown in\nFig. 4 (d), though GPT-4o can not exactly recognize the accurate col..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7028793805670912,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 42,
          "relevance_score": 0.7019186085294971,
          "content": "Published as a conference paper at ICLR 2025\nimport inspect\ndrawed_colors = []\ndef log_function(func):\ndef wrapper(*args, **kwargs):\nfunc_name = inspect.getfile(func) + \"/\" + func.__name__\nresult = func(*args, **kwargs)\nfor item in result:\ncolor = item.get_facecolor()\ndrawed_colors.append( func_name + \"--\" + color )\nAxes.bar = log_function(Axes.bar)\nIn the matplotlib package, each plot function returns a chart type instance at the end of the function\ninvocation. These instances contain various a..."
        }
      ],
      "pred_answer": "gpt-4o can identify similar colors, but it still struggles with accurately recognizing the accurate colors, and may have difficulty with complex data analysis, including color-related errors. the model's ability to recreate color in chartmimic is not perfect, as it can only identify similar colors, not exact matches.",
      "ground_truth": [
        [
          "gpt 4o"
        ],
        [
          "4o"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "According to ChartMimic, what makes an HR chart unique?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 34,
          "relevance_score": 0.7264871081073917,
          "content": "Published as a conference paper at ICLR 2025\nFigure 32: Examples of HR charts.\n34"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 33,
          "relevance_score": 0.6549508058293271,
          "content": "Published as a conference paper at ICLR 2025\n4. Bar (3D bar chart)\n5. Density (3D density plot)\nFigure 28: Examples of 3D chart subcategories.\nPIP: PIP chart insets a magnified or reduced portion of the main plot to highlight key data\nfeatures, as in Figs. 29 (1) and (2). They enhance readability for complex datasets and\nfacilitate comparative analyses, as demonstrated in Figs. 29 (3) and (4).\nFigure 29: Examples of PIP charts.\nMultidiff: A Multidiff chart combines at least two different chart t..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 26,
          "relevance_score": 0.6491803601989976,
          "content": "Published as a conference paper at ICLR 2025\nD\nCHART TAXONOMY\nThis section presents the chart taxonomy in ChartMimic. It encompasses a structure of 22 categories\naccording to chart type characteristics and data composition. The categories comprise of:\n\u2022 18 regular types, ordered as follows: Bar, Heatmap, Scatter, Box, Errorbar, Errorpoint, Line,\nViolin, Radar, Pie, Density, Graph, Quiver, Contour, Histogram, Tree, Area, and 3D charts.\n\u2022 4 advanced types: PIP (Plot-in-Plot), Multidiff (Multiple D..."
        }
      ],
      "pred_answer": "an hr chart is unique because it defies the other 21 categories or is challenging to identify, typically being modified from common charts with distinctive features like custom visual arrangement or atypical markers.",
      "ground_truth": [
        [
          "layer"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What optimization method is applied during PCFM sampling as shown in Algorithm 1 and Figure 4?",
      "required_files": [
        "2506.04171v1.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2506.04171v1.pdf",
          "page_number": 7,
          "relevance_score": 0.7179213687692458,
          "content": "D-Flow results are omitted due to numerical instabilities.\nsatisfaction, we focus on generating solution subsets constrained on a selected held-out IC or BC,\naiming to guide the pretrained model toward the corresponding solution subset. Furthermore, we\nincorporate physical constraints, specifically global mass conservation, via PCFM and adapt other\nbaseline methods, where possible, through their sampling frameworks to evaluate their performance\non constraint satisfaction tasks.\nFor a variety of ..."
        },
        {
          "chunk_id": 2,
          "file_name": "2506.04171v1.pdf",
          "page_number": 6,
          "relevance_score": 0.7162858888885281,
          "content": "3.4.1\nTradeoff Between Flow Steps and Relaxed Constraint Correction\nIn PCFM, the number of flow steps and the strength of the relaxed constraint correction jointly\ndetermine the quality of the final solution. Using fewer steps speeds up inference but increases\nnumerical error, potentially violating constraints at intermediate states. In such regimes, the relaxed\ncorrection term in Equation (6) provides a mechanism to compensate for deviation from the constraint\nmanifold. Conversely, with suffici..."
        },
        {
          "chunk_id": 3,
          "file_name": "2506.04171v1.pdf",
          "page_number": 24,
          "relevance_score": 0.6903658676072373,
          "content": "K\nFurther Results\n\ud835\udc65\n\ud835\udc61\nGenerated \nsolution \nstd dev\nGenerated \nsolution \nmean\n\ud835\udc65\n\ud835\udc61\nMass \nresidual \nmean\nBC \nresidual \nmean\nFigure 5: Solution profiles for the Inviscid Burgers equation with fixed BC. We plot the various\nconstraint guidance methods and compare the mean solution profile and standard deviation. While\nPCFM yields slightly worse MMSE and SMSE and better FPD, it ensures global mass conservation\nand maintains low constraint errors for both Dirichlet and Neumann BCs over time.\nGenerated \n..."
        }
      ],
      "pred_answer": "the specific optimization method used during pcfm sampling is not explicitly stated in the provided context. however, it is mentioned that pcfm incorporates physical constraints, such as global mass conservation, via a relaxed constraint correction term, which provides a mechanism to compensate for deviation from the constraint manifold. additionally, the tradeoff between flow steps and relaxed constraint correction is discussed, indicating that the number of flow steps and the strength of the relaxed constraint correction jointly determine the quality of the final solution.",
      "ground_truth": [
        [
          "gauss newton projection",
          "newton gauss projection"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": false
    },
    {
      "category": "Multimodal",
      "question": "What are the 18 Regular chart categories in ChartMimic?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 26,
          "relevance_score": 0.8301234915529538,
          "content": "Published as a conference paper at ICLR 2025\nD\nCHART TAXONOMY\nThis section presents the chart taxonomy in ChartMimic. It encompasses a structure of 22 categories\naccording to chart type characteristics and data composition. The categories comprise of:\n\u2022 18 regular types, ordered as follows: Bar, Heatmap, Scatter, Box, Errorbar, Errorpoint, Line,\nViolin, Radar, Pie, Density, Graph, Quiver, Contour, Histogram, Tree, Area, and 3D charts.\n\u2022 4 advanced types: PIP (Plot-in-Plot), Multidiff (Multiple D..."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 10,
          "relevance_score": 0.732545589606934,
          "content": "They assess the LMMs\u2019 ability to understand specific data characteristics or summarize key in-\nformation into text. ChartMimic advances the field by introducing a chart-to-code task, transforming\nthe LMMs\u2019 understanding of charts into code, which is neglected before but a realistic scenario for\npractical, real-world usage. This approach enables a comprehensive evaluation of the LMMs\u2019 overall\ncomprehension of charts and their ability to express this understanding in code form. Leveraging the\nling..."
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 33,
          "relevance_score": 0.7241560692905679,
          "content": "Published as a conference paper at ICLR 2025\n4. Bar (3D bar chart)\n5. Density (3D density plot)\nFigure 28: Examples of 3D chart subcategories.\nPIP: PIP chart insets a magnified or reduced portion of the main plot to highlight key data\nfeatures, as in Figs. 29 (1) and (2). They enhance readability for complex datasets and\nfacilitate comparative analyses, as demonstrated in Figs. 29 (3) and (4).\nFigure 29: Examples of PIP charts.\nMultidiff: A Multidiff chart combines at least two different chart t..."
        }
      ],
      "pred_answer": "the 18 regular chart categories in chartmimic are: bar, heatmap, scatter, box, errorbar, errorpoint, line, violin, radar, pie, density, graph, quiver, contour, histogram, tree, area, and 3d charts.",
      "ground_truth": [
        [
          "bar",
          "errorbar",
          "line",
          "errorpoint",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "treemap",
          "heatmap",
          "contour"
        ],
        [
          "bar",
          "error bar",
          "line",
          "error point",
          "box",
          "violin",
          "hist",
          "density",
          "area",
          "scatter",
          "graph",
          "quiver",
          "3d",
          "pie",
          "radar",
          "tree map",
          "heat map",
          "contour"
        ]
      ],
      "correctness_score": 0.9444444444444444,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "Does accuracy go up or down on hard ENIGMAEVAL puzzles, why?",
      "required_files": [
        "2502.08859v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 11,
          "relevance_score": 0.700734309303033,
          "content": "This metric is a weighted measure that integrates the Recognition Accuracy and the inverse of the\nFailure Rate:\nAA =\nPN\ni=1(wi \u00d7 A\n\u2032\ni \u00d7 RAi \u00d7 (ERi))\nPN\ni=1 wi\nwhere wi represents the difficulty weight for the ith level, A\n\u2032\ni represents the accuracy in recognizable\nand parsable questions at ith level, RAi is the Recognition Accuracy for the ith level, and ERi is the\nInstruction-following Effectiveness Rate for the ith level. The Aggregated Accuracy metric will\nbe used as the main evaluation met..."
        },
        {
          "chunk_id": 2,
          "file_name": "2502.08859v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6709318633790247,
          "content": "ENIGMAEVAL:\nA Benchmark of Long Multimodal\nReasoning Challenges\nClinton J. Wang1, Dean Lee1, Cristina Menghini1, Johannes Mols1, Jack Doughty1, Adam\nKhoja2, Jayson Lynch3, Sean Hendryx1, Summer Yue1, Dan Hendrycks2\n1Scale AI, 2Center for AI Safety, 3MIT\n# {clinton.wang, summer.yue}@scale.com, dan@safe.ai\n\u0080 https://scale.com/leaderboard/enigma_eval\nAbstract\nAs language models master existing reasoning benchmarks, we need new challenges to evaluate their cognitive\nfrontiers. Puzzle-solving events ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2502.08859v2.pdf",
          "page_number": 10,
          "relevance_score": 0.6706582443773677,
          "content": "Appendix\nA. Dataset Details\nA.1\nNumber of Images per Puzzle\nText-only puzzles comprise a small minority of the dataset. While most puzzles have a single key visual component,\nthere is a significant number of puzzles with many graphics.\nFigure A.1: Image distribution across puzzles. Left shows number of images in human transcriptions of the\npuzzles. Right shows number of pages in the PDF version of the puzzle.\nA.2\nAnnotator Instructions\nAnnotators were presented with instructions based on the fol..."
        }
      ],
      "pred_answer": "accuracy goes down on hard enigmaeval puzzles. this is because state-of-the-art language models achieve extremely low accuracy on these puzzles, which require unstructured and lateral reasoning, and the ability to discover hidden connections between seemingly unrelated pieces of information. the benchmark comprises highly difficult problems with diverse unstructured formats, and the models' performance declines as the complexity of the tasks increases.",
      "ground_truth": [
        [
          "down",
          "ocr"
        ],
        [
          "down",
          "parsing"
        ]
      ],
      "correctness_score": 0.5,
      "suspected_hallucination": true
    },
    {
      "category": "Multimodal",
      "question": "In the ChartMimic paper, what is substituted in Figure 10 for CB_29?",
      "required_files": [
        "2406.09961v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 24,
          "relevance_score": 0.7023506433347358,
          "content": "Published as a conference paper at ICLR 2025\nFigure 10: An Example of the Customized Mimic task (CB_29), showing the (figure, instruction,\ncode) triplet. Additionally, we also display the ground truth figure rendered from the code for\nillustration.\n24"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 23,
          "relevance_score": 0.6847355213448315,
          "content": "Published as a conference paper at ICLR 2025\nFigure 9: An Example of the Direct Mimic task (CB_29), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n23"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6823186139541054,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        }
      ],
      "pred_answer": "in figure 10, the customized mimic task is shown for cb_29, displaying a (figure, instruction, code) triplet, along with the ground truth figure rendered from the code.",
      "ground_truth": [
        [
          "dataset"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "From both papers (Multimodal-CoT and HyperTransformer), what do models experience as Transformer capacity continues to grow across model size and modality?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.7039434296912686,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 14,
          "relevance_score": 0.6786914812153048,
          "content": "Transformer parameters.\nSince the weight tensors of each layer are generally different, our per-layer transformers were\nalso different. The key, query and value dimensions of the transformer were chosen to be equal to a pre-de\ufb01ned fraction\n\u03bd of the input embedding size, which in turn was a function of the label, image and activation embedding sizes and the\nsizes of the weight slices. The inner dimension of the \ufb01nal fully-connected layer in the transformer was also chosen\nusing the same approach...."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6612299410552946,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "as transformer capacity continues to grow across model size and modality, models experience performance degradation in some cases, while in others, it leads to increased model performance. specifically, increasing the number of transformer heads can lead to performance degradation, whereas using larger activation embeddings appears to be advantageous in most cases. additionally, the presence of a decoder can have different effects on model performance, increasing it in some models while leading to accuracy degradation in others.",
      "ground_truth": [
        [
          "diminish",
          "return"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which embedding design, as illustrated in Figure 1 and described in the text of both HyperTransformer and Multimodal-CoT, drives improved few-shot or reasoning performance?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.7293216961866085,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.7261892842003065,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 15,
          "relevance_score": 0.7180567938368537,
          "content": "Activation embedding dimension.\nParticularly, small activation embeddings can be seen to hurt the performance in both\nmodels, while using larger activation embeddings appears to be advantageous in most cases except for the 32-dimensional\nactivation embeddings in the 4-channel model.\nClass embedding dimension.\nParticularly low embedding dimension of 16 can be seen to hurt the performance of both\nmodels.\nNumber of transformer heads.\nIncreasing the number of transformer heads leads to performance d..."
        }
      ],
      "pred_answer": "larger activation embeddings appear to be advantageous in most cases for driving improved few-shot performance. additionally, using a higher-dimensional embedding, such as 32 or 64, tends to have a positive effect on accuracy. in contrast, small activation embeddings and low embedding dimensions, like 16, can hurt performance. the image embedding dimension also plays a role, with removing the image embedding or using an 8-dimensional embedding leading to decreased performance.",
      "ground_truth": [
        [
          "modal",
          "align",
          "token",
          "embed"
        ]
      ],
      "correctness_score": 0.25,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "How would inputs, like questions from the ScienceQA benchmark, be handled by the HyperTransformer architecture using its current encoding setup?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6286439990886973,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 17,
          "relevance_score": 0.6270229757372193,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n0.0\n0.5\nAccuracy delta\n1\n2\n3\nLayers\n\u22121\n0\nAccuracy delta\n8\n16\n32\n\u22120.5\n0.0\n0.5\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22120.5\n0.0\n0.5\nAccuracy delta\nnone\n8\n16\n32\n0.0\n0.5\nAccuracy delta\nno\nyes\nDecoder\n\u22121.0\n\u22120.5\n0.0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n0.0\n0.5\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22120.50\n\u22120.25\n0.00\n0.25\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\n..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6250301062586475,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        }
      ],
      "pred_answer": "the hypertransformer architecture is designed to handle few-shot learning tasks, and its encoding setup is tailored for image embedding. to handle text-based inputs like questions from the scienceqa benchmark, the architecture would likely require modifications to its encoding setup.\n\ngiven the current encoding setup, which includes activation embedding size and image embedding size, it's unclear how text-based inputs would be directly handled. the hypertransformer's current configuration seems to focus on visual data, with parameters such as embedding dimension d, number of heads, and \u03bd for value, inner, and key/query dimensions.\n\nto accommodate text-based inputs, the architecture might need to incorporate additional encoding mechanisms, such as tokenization, word embeddings, or other natural language processing techniques. this could involve integrating external modules or modifying the existing encoding setup to accommodate sequential data like text.\n\nwithout further modifications or adaptations, it's uncertain how the hypertransformer would handle questions from the scienceqa benchmark using its current encoding setup. the architecture's primary focus on image embedding and few-shot learning tasks suggests that text-based inputs might not be directly supported in its current configuration.",
      "ground_truth": [
        [
          "flatten"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which property of the self-attention mechanism, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and described in their Method sections, underpins both CNN weight generation and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 5,
          "relevance_score": 0.7387241189341589,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nTransformer\n0\nWeight \nPlaceholders\nGenerated Weight Slices\nLabeled\nSamples\nOptional\nUnlabeled\nSamples\n...\n...\n...\n...\n...\n...\nClass Embeddings\nSample Embeddings\n\"No Label\" Embeddings\nWeight Embeddings\nFigure 2. Structure of the tokens passed to and received from a\nTransformer model. Both labeled and unlabeled (optional) samples\ncan be passed to the Transformer as inputs. Empty output tokens\nindicate ignored outputs.\nall the layers. Their p..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 8,
          "relevance_score": 0.7227063625800717,
          "content": "We visualized the distribution of the weights generated by\nHT for different episodes by using UMAP (McInnes et al.,\n2018) embeddings of the generated weights for a 6-channel\nCNN model (see Fig. 4). We highlighted some of the classes\npresent in the evaluation set and while the general structure\nmay be hard to interpret, the distribution of the highlighted\nclasses is somewhat clustered indicating the importance of\nsemantic information for generated CNN weights. More\ndetails can be found in Appendi..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 23,
          "relevance_score": 0.7212097437768903,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nGenerated/Generated\nTrained/Generated\nTrained/Trained\nCNN Layer 1\n\u22122\n0\n2\n\u22122\n0\n2\n\u22120.4\n\u22120.2\n0.0\n0.2\n0.4\nCNN Layer 0\n\u221210\n0\n10\n\u22121\n0\n1\n\u22121\n0\n1\nFigure 13. Visualizing convolutional kernels for a 2-layer network with a 9\u00d79 CNN kernel size and stride of 4 trained on MINIIMAGENET.\nEach column shows 1 model with two layers and fully-connected head that is always generated by the transformer. Left: both CNN\nlayers are generated, center: \ufb01rst CNN layer..."
        }
      ],
      "pred_answer": "the property of the self-attention mechanism that underpins both cnn weight generation and rationale generation is its ability to be invariant to input permutations. this means that the output produced by the generator with the basic self-attention is independent of the order of the samples in the training dataset, making it suitable for processing unbalanced batches and batches with a variable number of samples. additionally, the self-attention mechanism can implement a method similar to cosine-similarity-based sample weighting, which can also be viewed as a result of applying a single gradient descent step on the cross-entropy loss. this property allows the self-attention mechanism to effectively generate cnn weights and rationales in few-shot learning scenarios.",
      "ground_truth": [
        [
          "invariant"
        ],
        [
          "invariance"
        ],
        [
          "permutation"
        ],
        [
          "permute"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What shared mechanism, as illustrated in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both Method sections, could unify weight and rationale generation?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2302.00923v5.pdf",
          "page_number": 2,
          "relevance_score": 0.6973067495706012,
          "content": "Our experiments were conducted on the ScienceQA (Lu et al., 2022a) and A-OKVQA (Schwenk\net al., 2022) datasets, which are the latest multimodal reasoning benchmarks with annotated reasoning chains.\nOur method achieves state-of-the-art performance on the ScienceQA benchmark upon the release. We find\nthat Multimodal-CoT is beneficial in mitigating hallucination and boosting convergence. Our contributions\nare summarized as follows:\n(i) To the best of our knowledge, this work is the first to study C..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 20,
          "relevance_score": 0.681500189191781,
          "content": "Published in Transactions on Machine Learning Research (05/2024)\nand without the integration of vision features. Notably, when employing a larger LM, the baseline accuracy\n(without vision features) experiences a significant enhancement. This finding suggests that scaling the LM\nsize could potentially alleviate the problem of incorrect rationales. However, it is crucial to acknowledge that\nthe performance still falls considerably short of utilizing vision features. This outcome further validates ..."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 24,
          "relevance_score": 0.6626326463427218,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nEpisode 0\nEpisode 1\nDifference\nLayer 1\n\u22122\n0\n2\n\u22124\n\u22122\n0\n2\n4\n\u22120.5\n0.0\n0.5\nLayer 0\n\u221210\n0\n10\n\u221210\n0\n10\n\u22120.5\n0.0\n0.5\nFigure 15. Visualizing generated convolutional kernels in a 2-layer model for two different episodes. Left two plots: kernels for two\nrandom episodes of 5 classes, right: the difference in generated kernels for two episodes. Layer weight allocation: \u201cspatial\u201d."
        }
      ],
      "pred_answer": "the shared mechanism that could unify weight and rationale generation is the ability to decompose complex problems into a series of simpler problems and solve them step by step, as illustrated in figure 2 of hypertransformer and figure 4 of multimodal-cot. this transformation leads to a modification of the standard format <question \u2192answer> into <question \u2192rationale \u2192answer>, where rationales play a crucial role in reflecting the reasoning processes leading to the answer. in the context of weight generation, this mechanism can be applied by generating convolutional kernels that capture feature-level interactions between different inputs, as shown in figure 15 of hypertransformer. similarly, in multimodal-cot, this mechanism is used to generate informative rationales that facilitate inferring final answers. the shared idea is to break down complex problems into simpler sub-problems and solve them sequentially, which can be applied to both weight generation and rationale generation.",
      "ground_truth": [
        [
          "cross"
        ],
        [
          "modal"
        ],
        [
          "self"
        ],
        [
          "attention"
        ]
      ],
      "correctness_score": 1.0,
      "suspected_hallucination": false
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What failure in representation can occur in both Multimodal-CoT and HyperTransformer particularly in unsupervised learning?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.7044984068045611,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.6998848541496793,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6826465294356296,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "mode collapse can be a failure in representation that occurs in both multimodal-cot and hypertransformer, particularly in unsupervised learning. this happens when the model generates limited variations of outputs, failing to capture the full range of possibilities in the data.",
      "ground_truth": [
        [
          "drift"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which design feature, shown in Figure 2 of HyperTransformer and Figure 4 of Multimodal-CoT and discussed in both texts, supports permutation invariance for few-shot generalization?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 20,
          "relevance_score": 0.6856413550222753,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nAll layers\nClass 10\nClass 85\nClass 39\nClass 60\nClass 84\nClass 8\nFigure 10. UMAP embedding of weights generated by the HYPERTRANSFORMER. Each point corresponds to the embedding of the\nweights of a given layer for a given episode. The original dimensionality for the \ufb01rst layer weights is 162 = 3 \u00d7 3 \u00d7 3 \u00d7 6, for each\nsubsequent weights is 324 = 3 \u00d7 3 \u00d7 6 \u00d7 6 and for all layers combined is 1 134. We further sel..."
        },
        {
          "chunk_id": 2,
          "file_name": "2201.04182v3.pdf",
          "page_number": 24,
          "relevance_score": 0.6836977568602446,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nEpisode 0\nEpisode 1\nDifference\nLayer 1\n\u22122\n0\n2\n\u22124\n\u22122\n0\n2\n4\n\u22120.5\n0.0\n0.5\nLayer 0\n\u221210\n0\n10\n\u221210\n0\n10\n\u22120.5\n0.0\n0.5\nFigure 15. Visualizing generated convolutional kernels in a 2-layer model for two different episodes. Left two plots: kernels for two\nrandom episodes of 5 classes, right: the difference in generated kernels for two episodes. Layer weight allocation: \u201cspatial\u201d."
        },
        {
          "chunk_id": 3,
          "file_name": "2201.04182v3.pdf",
          "page_number": 18,
          "relevance_score": 0.6753192096416121,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\n\u22124\n\u22122\n0\nAccuracy delta\n1\n2\n3\nLayers\n\u22122\n0\nAccuracy delta\n8\n16\n32\n\u22122\n\u22121\n0\nAccuracy delta\n16\n32\n64\nEmbedding dimension d\n\u22121\n0\n1\nAccuracy delta\n1\n2\n4\n8\nHeads\n\u22122\n0\nAccuracy delta\nnone\n8\n16\n32\n\u22123\n\u22122\n\u22121\n0\nAccuracy delta\nno\nyes\nDecoder\n\u22122\n\u22121\n0\n1\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for value dimension\n\u22124\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for inner dimension\n\u22122\n0\nAccuracy delta\n0.2\n0.5\n1.0\n1.5\n\u03bd for key/query dimension\nActivation embedding size\nIma..."
        }
      ],
      "pred_answer": "the design feature that supports permutation invariance for few-shot generalization is the use of self-attention mechanisms, which allow the model to attend to different parts of the input data regardless of their order or position. this is evident from the discussion of hypertransformer, where the model's ability to generate convolutional kernels and perform embedding of weights is highlighted, suggesting that the self-attention mechanism plays a key role in achieving permutation invariance. however, since figure 2 of hypertransformer and figure 4 of multimodal-cot are not provided, it can be inferred that the design feature supporting permutation invariance is related to the model's architecture, specifically the use of attention mechanisms or other techniques that enable the model to handle input data in a order-agnostic manner.",
      "ground_truth": [
        [
          "self attention",
          "flat",
          "sequence"
        ],
        [
          "selfattention",
          "flat",
          "sequence"
        ]
      ],
      "correctness_score": 0.3333333333333333,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "What element from the Multimodal-CoT paper, used before Rational Generation in Figure 4, could be incorporated into the HyperTransformer architecture to allow it to classify datasets that contain both images and text?",
      "required_files": [
        "2201.04182v3.pdf",
        "2302.00923v5.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2201.04182v3.pdf",
          "page_number": 21,
          "relevance_score": 0.7271168054392304,
          "content": "HyperTransformer: Model Generation for Few-Shot Learning\nClass 8\nClass 84\nClass 60\nClass 39\nClass 85\nClass 10\nFigure 11. Examples of classes from TIEREDIMAGENET that we used to highlight in UMAP embedding."
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 22,
          "relevance_score": 0.7019707560382359,
          "content": "Published as a conference paper at ICLR 2025\nFigure 8: An Example of the Customized Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n22"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 21,
          "relevance_score": 0.6927929151308176,
          "content": "Published as a conference paper at ICLR 2025\nFigure 7: An Example of the Direct Mimic task (bar_28), showing the (figure, instruction, code)\ntriplet. Additionally, we also display the ground truth figure rendered from the code for illustration.\n21"
        }
      ],
      "pred_answer": "there is no information provided about the multimodal-cot paper or its contents, including figure 4, in relation to the given context. the context only mentions two pdf files: 2201.04182v3.pdf and 2406.09961v2.pdf, but not 2302.00923v5.pdf. therefore, it's impossible to determine what element could be incorporated into the hypertransformer architecture based on the provided information.",
      "ground_truth": [
        [
          "vision",
          "text",
          "fusion"
        ],
        [
          "image",
          "language",
          "fusion"
        ],
        [
          "vision",
          "language",
          "fusion"
        ],
        [
          "image",
          "text",
          "fusion"
        ]
      ],
      "correctness_score": 0.3333333333333333,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Figure 2 of DocLLM and Figure 3 of NPHardEval4V demonstrate a different type of reasoning as opposed to structural reasoning. What type of reasoning is demonstrated?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2403.01777v2.pdf",
          "page_number": 10,
          "relevance_score": 0.7096491894956843,
          "content": "Figure 6: MLLM: a. Reasoning abilities performance excluding the effects of recognition and\ninstruction following on figure+limited_text representation of questions. b. Instruction-following\neffective rate\nFigure 7: LLM: a. Reasoning abilities performance excluding the effect of instruction following on\npure textual description of questions. b. Instruction-following effective rate\nfoundation for any subsequent reasoning or decision-making processes the model undertakes. It is\nquantified by the r..."
        },
        {
          "chunk_id": 2,
          "file_name": "2302.00923v5.pdf",
          "page_number": 15,
          "relevance_score": 0.7063255988545538,
          "content": "Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning. In The\n35th Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks,\n2021.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question\nanswering. Advances in Neural Information Processing Systems, 35:2507\u20132521, 2022a.\nPan Lu, Lia..."
        },
        {
          "chunk_id": 3,
          "file_name": "2403.01777v2.pdf",
          "page_number": 1,
          "relevance_score": 0.6926759511181777,
          "content": "NPHardEval4V: A Dynamic Reasoning Benchmark of\nMultimodal Large Language Models\nLizhou Fan\u2217\nUniversity of Michigan\nWenyue Hua\u2217\nRutgers University\nXiang Li\u2217\nShandong University\nKaijie Zhu\nMicrosoft Research Asia\nMingyu Jin\nRutgers University\nLingyao Li\nUniversity of Michigan\nHaoyang Ling\nUniversity of Michigan\nJinkui Chi\nUniversity of Michigan\nJindong Wang\nMicrosoft Research Asia\nXin Ma\nShandong University\nYongfeng Zhang\nRutgers University\nFigure 1: Multimodal Large Language Models\u2019s performance ..."
        }
      ],
      "pred_answer": "multimodal reasoning.",
      "ground_truth": [
        [
          "spatial"
        ],
        [
          "space"
        ],
        [
          "visual"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    },
    {
      "category": "Cross-Document Multimodal",
      "question": "Which common objective, as shown in Figure 2 of DocLLM and mentioned in NPHardEval4V, involves predicting missing content from context?",
      "required_files": [
        "2401.00908v1.pdf",
        "2403.01777v2.pdf"
      ],
      "rag_context": [
        {
          "chunk_id": 1,
          "file_name": "2406.09961v2.pdf",
          "page_number": 47,
          "relevance_score": 0.6902548090195886,
          "content": "Published as a conference paper at ICLR 2025\nFigure 44: An error case of SelfReflection Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n47"
        },
        {
          "chunk_id": 2,
          "file_name": "2406.09961v2.pdf",
          "page_number": 49,
          "relevance_score": 0.6897751536368459,
          "content": "Published as a conference paper at ICLR 2025\nFigure 46: An error case of Scaffold Prompting. The text highlighted in red is LMM\u2019s incorrect\nunderstanding regarding provided figures. The remaining part of response is omitted with \u201c......\u201d.\n49"
        },
        {
          "chunk_id": 3,
          "file_name": "2406.09961v2.pdf",
          "page_number": 38,
          "relevance_score": 0.6836641071379699,
          "content": "Published as a conference paper at ICLR 2025\nFigure 36: Example of GPT-4o\u2019s scoring results for a Direct Mimic example (CB_29).\n38"
        }
      ],
      "pred_answer": "that information is not available here, the provided details only mention figures from a specific document, such as figure 44, figure 46, and figure 36, but do not reference figure 2 or the specific documents 2401.00908v1.pdf and 2403.01777v2.pdf.",
      "ground_truth": [
        [
          "context infilling"
        ]
      ],
      "correctness_score": 0.0,
      "suspected_hallucination": true
    }
  ]
}